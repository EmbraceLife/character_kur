{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "heading_collapsed": true
   },
   "source": [
    "## Tasks\n",
    "\n",
    "- make a fluid/defaults splits as needed for each modification\n",
    "- Try different ways to reduce overfitting and divergence\n",
    "    - add dropout to each gru layer?\n",
    "    - learning rate?\n",
    "    - L2 ...\n",
    "- implement dlnd_character_example model in kur (make sure the model structure first)\n",
    "- How can I change directories inside `fluid.yml` rather than in `defaults.yml`?\n",
    "- it says people spend 30-40% time on gathering, cleaning and feature engineering dataset. Kur is taking care of them by defaults. Then the tasks to complete is to add more feature engineering features, right?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Natsume/Downloads/kur_road/character_rnn'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure you are at a level of directory with all the py files\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m__pycache__\u001b[m\u001b[m/        cleaned.txt         \u001b[34mt1\u001b[m\u001b[m/                 vocab.py\r\n",
      "\u001b[34mbooks\u001b[m\u001b[m/              \u001b[34mdata\u001b[m\u001b[m/               view_data.py\r\n",
      "char_rnn_demo.yaml  make_data.py        view_logs.py\r\n",
      "char_rnn_kur.ipynb  steps.sh            view_outputs.py\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "heading_collapsed": true
   },
   "source": [
    "## **Create a small dataset for speed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "source": [
    "- inside `make_data.py` set `dev=True` to reduce data size by x10\n",
    "- make_data create a data file named cleaned.txt\n",
    "- It is nice we can control size of dataset, so I wonder is it possible to **use `provider` inside kurfile.yaml to control how much data to use**? \n",
    "\n",
    "**Effect of doing the above**\n",
    "- make_data.py only takes a few seconds\n",
    "- `kur -v train kurfile.yaml` only takes less than 4 mins, compared to default setting's estimated 5 hours training \n",
    "- Also the previous 30 minutes loading time is gone too\n",
    "- **What made it to take 30 mins to load previously**? was it the large dataset? with smaller dataset, loading time reduced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13300\n",
      "dims:\n",
      "(13300, 30, 30)\n",
      "(13300, 30)\n",
      "13300 14131\n",
      "dims:\n",
      "(831, 30, 30)\n",
      "(831, 30)\n",
      "14131 14962\n",
      "dims:\n",
      "(831, 30, 30)\n",
      "(831, 30)\n",
      "14962 15793\n",
      "dims:\n",
      "(831, 30, 30)\n",
      "(831, 30)\n",
      "CPU times: user 260 ms, sys: 307 ms, total: 567 ms\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python make_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "heading_collapsed": true
   },
   "source": [
    "## Let's view the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "peek at train:\n",
      "------\n",
      "\"the project gutenberg ebook of\" --> \" \"\n",
      "\"he project gutenberg ebook of \" --> \"p\"\n",
      "\"e project gutenberg ebook of p\" --> \"r\"\n",
      "------\n",
      "\"o particular resentment by his\" --> \" \"\n",
      "\" particular resentment by his \" --> \"h\"\n",
      "\"particular resentment by his h\" --> \"a\"\n",
      "\n",
      "\n",
      "peek at validate:\n",
      "------\n",
      "\"articular resentment by his ha\" --> \"v\"\n",
      "\"rticular resentment by his hav\" --> \"i\"\n",
      "\"ticular resentment by his havi\" --> \"n\"\n",
      "------\n",
      "\"ingley for a kingdom upon my h\" --> \"o\"\n",
      "\"ngley for a kingdom upon my ho\" --> \"n\"\n",
      "\"gley for a kingdom upon my hon\" --> \"o\"\n",
      "\n",
      "\n",
      "peek at test:\n",
      "------\n",
      "\"ley for a kingdom upon my hono\" --> \"u\"\n",
      "\"ey for a kingdom upon my honou\" --> \"r\"\n",
      "\"y for a kingdom upon my honour\" --> \" \"\n",
      "------\n",
      "\"sting your time with me. mr. b\" --> \"i\"\n",
      "\"ting your time with me. mr. bi\" --> \"n\"\n",
      "\"ing your time with me. mr. bin\" --> \"g\"\n",
      "\n",
      "\n",
      "peek at evaluate:\n",
      "------\n",
      "\"ng your time with me. mr. bing\" --> \"l\"\n",
      "\"g your time with me. mr. bingl\" --> \"e\"\n",
      "\" your time with me. mr. bingle\" --> \"y\"\n",
      "------\n",
      "\"they had yet learnt to care fo\" --> \"r\"\n",
      "\"hey had yet learnt to care for\" --> \" \"\n",
      "\"ey had yet learnt to care for \" --> \"a\"\n",
      "CPU times: user 74.6 ms, sys: 94.9 ms, total: 170 ms\n",
      "Wall time: 4.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python view_data.py\n",
    "# below is last bit of data from trainingset, validation set, test set, evaluation set\n",
    "# left part of --> is X or input\n",
    "# right part of --> is y or output\n",
    "\n",
    "## If needed, I shall dig into it to see more of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "heading_collapsed": true
   },
   "source": [
    "## Let's see the original kurfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%pycat char_rnn_demo.yaml\n",
    "# copy and paste kurfile.yaml before to see the kurfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting char_rnn_demo.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile char_rnn_demo.yaml\n",
    "\n",
    "---\n",
    "\n",
    "settings:\n",
    "\n",
    "  vocab:\n",
    "    size: 30\n",
    "\n",
    "  rnn:\n",
    "    size: 128\n",
    "    depth: 3\n",
    "\n",
    "model:\n",
    "  - input: in_seq\n",
    "\n",
    "  - for:\n",
    "      range: \"{{ rnn.depth - 1 }}\"\n",
    "      iterate:\n",
    "        - recurrent:\n",
    "            size: \"{{ rnn.size }}\"\n",
    "            type: gru\n",
    "            sequence: yes\n",
    "            bidirectional: no\n",
    "        - batch_normalization\n",
    "\n",
    "  - recurrent:\n",
    "      size: \"{{ rnn.size }}\"\n",
    "      type: gru\n",
    "      sequence: no\n",
    "      bidirectional: no\n",
    "\n",
    "  - dense: \"{{ vocab.size }}\"\n",
    "\n",
    "  - activation: softmax\n",
    "\n",
    "  - output: out_char\n",
    "\n",
    "loss:\n",
    "  - target: out_char\n",
    "    name: categorical_crossentropy\n",
    "\n",
    "train:\n",
    "  data:\n",
    "    - jsonl: data/train.jsonl\n",
    "  epochs: 5                                \n",
    "  weights:\n",
    "    initial: t1/best.w.kur\n",
    "    best: t1/best.w.kur\n",
    "    last: t1/last.w.kur\n",
    "  log: t1/log\n",
    "  hooks:                                   # Let plot loss\n",
    "    - plot: t1/loss.png\n",
    "\n",
    "\n",
    "validate:\n",
    "  data:\n",
    "    - jsonl: data/validate.jsonl\n",
    "  weights: t1/best.w.kur\n",
    "\n",
    "\n",
    "test:\n",
    "  data:\n",
    "    - jsonl: data/test.jsonl\n",
    "  weights: t1/best.w.kur\n",
    "\n",
    "\n",
    "evaluate:\n",
    "  data:\n",
    "    - jsonl: data/evaluate.jsonl\n",
    "  weights: t1/best.w.kur\n",
    "\n",
    "  destination: t1/output.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the original model\n",
    "- data is created and stored inside data/ in the same level of model/\n",
    "- from the loss plot below, we see **overfitting and divergence**, what is the cause and how to reduce overfit and make better convergence? \n",
    "    - add drop out\n",
    "    - use LSTM instead of GRU?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m__pycache__\u001b[m\u001b[m/                      \u001b[34mdata\u001b[m\u001b[m/\r\n",
      "\u001b[34mbooks\u001b[m\u001b[m/                            \u001b[34mdata_supplier_char_rnn\u001b[m\u001b[m/\r\n",
      "char_rnn_demo.yaml                make_data.py\r\n",
      "char_rnn_demo_defaults.yaml       steps.sh\r\n",
      "char_rnn_demo_dlnd_defaults.yaml  \u001b[34mt1\u001b[m\u001b[m/\r\n",
      "char_rnn_demo_dp_defaults.yaml    \u001b[34mt2_dp\u001b[m\u001b[m/\r\n",
      "char_rnn_kur.ipynb                \u001b[34mt3_dlnd\u001b[m\u001b[m/\r\n",
      "char_rrn_demo_dlnd_fluid.yaml     view_data.py\r\n",
      "char_rrn_demo_dp_fluid.yaml       view_logs.py\r\n",
      "char_rrn_demo_fluid.yaml          view_outputs.py\r\n",
      "cleaned.txt                       vocab.py\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-03-05 21:25:22,495 kur.kurfile:699]\u001b[0m Parsing source: char_rnn_demo.yaml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:25:22,506 kur.kurfile:82]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:25:22,525 kur.loggers.binary_logger:71]\u001b[0m Loading log data: t1/log\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:25:26,980 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:25:26,980 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:25:26,980 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:25:28,086 kur.backend.keras_backend:195]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:25:28,086 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:25:28,086 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:25:28,086 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:25:29,110 kur.model.model:284]\u001b[0m Model inputs:  in_seq\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:25:29,111 kur.model.model:285]\u001b[0m Model outputs: out_char\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:25:29,139 kur.model.executor:313]\u001b[0m Best historical training loss: 0.389\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:25:29,139 kur.model.executor:320]\u001b[0m Best historical validation loss: 1.658\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:25:29,139 kur.model.executor:331]\u001b[0m Restarting from epoch 22.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:25:47,100 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\n",
      "Epoch 22/26, loss=0.411: 100%|██████| 13300/13300 [00:36<00:00, 359.56samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-05 21:26:24,336 kur.model.executor:464]\u001b[0m Training loss: 0.411\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:26:26,174 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "Validating, loss=2.522: 100%|██████████| 831/831 [00:00<00:00, 1494.47samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-05 21:26:26,755 kur.model.executor:197]\u001b[0m Validation loss: 2.522\u001b[0m\n",
      "\n",
      "Epoch 23/26, loss=0.366: 100%|██████| 13300/13300 [00:37<00:00, 356.57samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-05 21:27:05,471 kur.model.executor:464]\u001b[0m Training loss: 0.366\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:27:05,471 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=2.711: 100%|██████████| 831/831 [00:00<00:00, 1348.72samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-05 21:27:06,113 kur.model.executor:197]\u001b[0m Validation loss: 2.711\u001b[0m\n",
      "\n",
      "Epoch 24/26, loss=0.348: 100%|██████| 13300/13300 [00:37<00:00, 354.80samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-05 21:27:44,710 kur.model.executor:464]\u001b[0m Training loss: 0.348\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:27:44,711 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=2.715: 100%|██████████| 831/831 [00:00<00:00, 1589.66samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-05 21:27:45,255 kur.model.executor:197]\u001b[0m Validation loss: 2.715\u001b[0m\n",
      "\n",
      "Epoch 25/26, loss=0.339: 100%|██████| 13300/13300 [00:37<00:00, 357.56samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-05 21:28:23,897 kur.model.executor:464]\u001b[0m Training loss: 0.339\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:28:23,897 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=2.722: 100%|██████████| 831/831 [00:00<00:00, 1470.95samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-05 21:28:24,488 kur.model.executor:197]\u001b[0m Validation loss: 2.722\u001b[0m\n",
      "\n",
      "Epoch 26/26, loss=0.340: 100%|██████| 13300/13300 [00:37<00:00, 353.40samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-05 21:29:03,731 kur.model.executor:464]\u001b[0m Training loss: 0.340\u001b[0m\n",
      "Validating, loss=2.746: 100%|██████████| 831/831 [00:00<00:00, 1479.60samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-05 21:29:04,300 kur.model.executor:197]\u001b[0m Validation loss: 2.746\u001b[0m\n",
      "Completed 26 epochs.\n",
      "\u001b[1;37m[INFO 2017-03-05 21:29:05,792 kur.model.executor:235]\u001b[0m Saving most recent weights: t1/last.w.kur\u001b[0m\n",
      "CPU times: user 4.51 s, sys: 4.08 s, total: 8.59 s\n",
      "Wall time: 3min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kur -v train char_rnn_demo.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAG0CAYAAAA7Go31AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='t1/loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## split original kurfile into 2 kurfiles for easy modify\n",
    "- give `epochs:` a default value and an overide value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting char_rnn_demo_defaults.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile char_rnn_demo_defaults.yaml\n",
    "\n",
    "---\n",
    "\n",
    "settings:\n",
    "\n",
    "  vocab:\n",
    "    size: 30\n",
    "\n",
    "  rnn:\n",
    "    size: 128\n",
    "    depth: 3\n",
    "\n",
    "model:\n",
    "  - input: in_seq\n",
    "\n",
    "  - for:\n",
    "      range: \"{{ rnn.depth - 1 }}\"\n",
    "      iterate:\n",
    "        - recurrent:\n",
    "            size: \"{{ rnn.size }}\"\n",
    "            type: gru\n",
    "            sequence: yes\n",
    "            bidirectional: no\n",
    "        - batch_normalization\n",
    "\n",
    "  - recurrent:\n",
    "      size: \"{{ rnn.size }}\"\n",
    "      type: gru\n",
    "      sequence: no\n",
    "      bidirectional: no\n",
    "\n",
    "  - dense: \"{{ vocab.size }}\"\n",
    "\n",
    "  - activation: softmax\n",
    "#       name: out_char                      # won't work!\n",
    "  - output: out_char                        # QUESTION: in mnist and cifar example, we have `name: labels` but \n",
    "                                            # no `- output: labels`, why we use `- output: out_char` not just use\n",
    "                                            # `name: out_char`\n",
    "\n",
    "loss:\n",
    "  - target: out_char\n",
    "    name: categorical_crossentropy\n",
    "\n",
    "train:\n",
    "  data:\n",
    "    - jsonl: data/train.jsonl\n",
    "  epochs: \"{{ num_epochs|default(5) }}\"     # add default and overide values\n",
    "  weights:\n",
    "    initial: t1/best.w.kur\n",
    "    best: t1/best.w.kur\n",
    "    last: t1/last.w.kur\n",
    "  log: t1/log\n",
    "  hooks:                                   # Let plot loss\n",
    "#     - plot: t1/loss.png\n",
    "    - plot:\n",
    "        loss_per_batch: t1/loss1.png\n",
    "        loss_per_time: t1/loss2.png\n",
    "        throughput_per_time: t1/loss3.png\n",
    "\n",
    "validate:\n",
    "  data:\n",
    "    - jsonl: data/validate.jsonl\n",
    "  weights: t1/best.w.kur\n",
    "\n",
    "\n",
    "test:\n",
    "  data:\n",
    "    - jsonl: data/test.jsonl\n",
    "  weights: t1/best.w.kur\n",
    "\n",
    "\n",
    "evaluate:\n",
    "  data:\n",
    "    - jsonl: data/evaluate.jsonl\n",
    "  weights: t1/best.w.kur\n",
    "\n",
    "  destination: t1/output.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting char_rrn_demo_fluid.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile char_rrn_demo_fluid.yaml\n",
    "\n",
    "---\n",
    "settings: \n",
    "  num_epochs: 20\n",
    "\n",
    "\n",
    "include: char_rnn_demo_defaults.yaml\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-03-06 08:47:09,892 kur.kurfile:699]\u001b[0m Parsing source: char_rrn_demo_fluid.yaml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:47:09,895 kur.kurfile:699]\u001b[0m Parsing source: char_rnn_demo_defaults.yaml, included by char_rrn_demo_fluid.yaml.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:47:09,906 kur.kurfile:82]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:47:09,925 kur.loggers.binary_logger:107]\u001b[0m Log does not exist. Creating path: t1/log\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:47:13,387 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:47:13,388 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:47:13,388 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:47:14,438 kur.backend.keras_backend:195]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:47:14,438 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:47:14,438 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:47:14,439 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:47:15,620 kur.model.model:284]\u001b[0m Model inputs:  in_seq\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:47:15,620 kur.model.model:285]\u001b[0m Model outputs: out_char\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:47:15,620 kur.kurfile:357]\u001b[0m Ignoring missing initial weights: t1/best.w.kur. If this is undesireable, set \"must_exist\" to \"yes\" in the approriate \"weights\" section.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:47:15,620 kur.model.executor:315]\u001b[0m No historical training loss available from logs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:47:15,621 kur.model.executor:323]\u001b[0m No historical validation loss available from logs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:47:15,621 kur.model.executor:329]\u001b[0m No previous epochs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:47:40,427 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\n",
      "Epoch 1/20, loss=2.290: 100%|███████| 13300/13300 [00:40<00:00, 327.35samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:48:21,380 kur.model.executor:464]\u001b[0m Training loss: 2.290\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:48:21,381 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:48:23,689 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "Validating, loss=1.925: 100%|██████████| 831/831 [00:00<00:00, 2056.32samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:48:24,106 kur.model.executor:197]\u001b[0m Validation loss: 1.925\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:48:24,107 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1/best.w.kur\u001b[0m\n",
      "\n",
      "Epoch 2/20, loss=1.765: 100%|███████| 13300/13300 [00:37<00:00, 306.67samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:49:02,372 kur.model.executor:464]\u001b[0m Training loss: 1.765\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:49:02,372 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=1.780: 100%|███████████| 831/831 [00:00<00:00, 979.39samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:49:03,267 kur.model.executor:197]\u001b[0m Validation loss: 1.780\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:49:03,267 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1/best.w.kur\u001b[0m\n",
      "\n",
      "Epoch 3/20, loss=1.542: 100%|███████| 13300/13300 [00:46<00:00, 288.45samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:49:50,664 kur.model.executor:464]\u001b[0m Training loss: 1.542\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:49:50,664 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=1.693: 100%|██████████| 831/831 [00:00<00:00, 1047.69samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:49:51,512 kur.model.executor:197]\u001b[0m Validation loss: 1.693\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:49:51,512 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1/best.w.kur\u001b[0m\n",
      "\n",
      "Epoch 4/20, loss=1.389: 100%|███████| 13300/13300 [00:49<00:00, 268.34samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:50:42,491 kur.model.executor:464]\u001b[0m Training loss: 1.389\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:50:42,492 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=1.665: 100%|██████████| 831/831 [00:00<00:00, 1683.61samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:50:43,014 kur.model.executor:197]\u001b[0m Validation loss: 1.665\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:50:43,014 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1/best.w.kur\u001b[0m\n",
      "\n",
      "Epoch 5/20, loss=1.252: 100%|███████| 13300/13300 [00:47<00:00, 282.60samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:51:31,338 kur.model.executor:464]\u001b[0m Training loss: 1.252\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:51:31,338 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=1.641: 100%|██████████| 831/831 [00:00<00:00, 1501.42samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:51:31,916 kur.model.executor:197]\u001b[0m Validation loss: 1.641\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:51:31,917 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1/best.w.kur\u001b[0m\n",
      "\n",
      "Epoch 6/20, loss=1.132: 100%|███████| 13300/13300 [00:45<00:00, 346.12samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:52:19,222 kur.model.executor:464]\u001b[0m Training loss: 1.132\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:52:19,222 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=1.661: 100%|██████████| 831/831 [00:00<00:00, 1730.98samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:52:19,728 kur.model.executor:197]\u001b[0m Validation loss: 1.661\u001b[0m\n",
      "\n",
      "Epoch 7/20, loss=1.021: 100%|███████| 13300/13300 [00:40<00:00, 394.82samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:53:01,731 kur.model.executor:464]\u001b[0m Training loss: 1.021\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:53:01,731 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=1.714: 100%|██████████| 831/831 [00:00<00:00, 1934.60samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:53:02,182 kur.model.executor:197]\u001b[0m Validation loss: 1.714\u001b[0m\n",
      "\n",
      "Epoch 8/20, loss=0.926: 100%|███████| 13300/13300 [00:34<00:00, 388.75samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:53:37,546 kur.model.executor:464]\u001b[0m Training loss: 0.926\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:53:37,546 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=1.745: 100%|██████████| 831/831 [00:00<00:00, 2018.81samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:53:37,983 kur.model.executor:197]\u001b[0m Validation loss: 1.745\u001b[0m\n",
      "\n",
      "Epoch 9/20, loss=0.841: 100%|███████| 13300/13300 [00:35<00:00, 370.58samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:54:15,148 kur.model.executor:464]\u001b[0m Training loss: 0.841\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:54:15,148 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=1.799: 100%|██████████| 831/831 [00:00<00:00, 1685.76samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:54:15,666 kur.model.executor:197]\u001b[0m Validation loss: 1.799\u001b[0m\n",
      "\n",
      "Epoch 10/20, loss=0.761: 100%|██████| 13300/13300 [00:35<00:00, 379.71samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:54:51,959 kur.model.executor:464]\u001b[0m Training loss: 0.761\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:54:51,960 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=1.868: 100%|██████████| 831/831 [00:00<00:00, 2052.61samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:54:52,385 kur.model.executor:197]\u001b[0m Validation loss: 1.868\u001b[0m\n",
      "\n",
      "Epoch 11/20, loss=0.700: 100%|██████| 13300/13300 [00:35<00:00, 378.28samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:55:28,733 kur.model.executor:464]\u001b[0m Training loss: 0.700\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:55:28,733 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=2.004: 100%|██████████| 831/831 [00:00<00:00, 1879.55samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:55:29,203 kur.model.executor:197]\u001b[0m Validation loss: 2.004\u001b[0m\n",
      "\n",
      "Epoch 12/20, loss=0.635: 100%|██████| 13300/13300 [00:36<00:00, 366.92samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:56:06,716 kur.model.executor:464]\u001b[0m Training loss: 0.635\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:56:06,716 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=2.122: 100%|██████████| 831/831 [00:00<00:00, 1948.45samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:56:07,165 kur.model.executor:197]\u001b[0m Validation loss: 2.122\u001b[0m\n",
      "\n",
      "Epoch 13/20, loss=0.598: 100%|██████| 13300/13300 [00:35<00:00, 354.29samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:56:44,424 kur.model.executor:464]\u001b[0m Training loss: 0.598\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:56:44,424 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=2.092: 100%|██████████| 831/831 [00:00<00:00, 1583.25samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:56:44,972 kur.model.executor:197]\u001b[0m Validation loss: 2.092\u001b[0m\n",
      "\n",
      "Epoch 14/20, loss=0.543: 100%|██████| 13300/13300 [00:38<00:00, 341.72samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:57:25,514 kur.model.executor:464]\u001b[0m Training loss: 0.543\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:57:25,515 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=2.213: 100%|██████████| 831/831 [00:00<00:00, 1945.94samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:57:25,965 kur.model.executor:197]\u001b[0m Validation loss: 2.213\u001b[0m\n",
      "\n",
      "Epoch 15/20, loss=0.516: 100%|██████| 13300/13300 [00:36<00:00, 363.59samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:58:04,509 kur.model.executor:464]\u001b[0m Training loss: 0.516\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:58:04,509 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=2.257: 100%|██████████| 831/831 [00:00<00:00, 1630.19samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:58:05,045 kur.model.executor:197]\u001b[0m Validation loss: 2.257\u001b[0m\n",
      "\n",
      "Epoch 16/20, loss=0.473: 100%|██████| 13300/13300 [00:40<00:00, 350.57samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:58:46,868 kur.model.executor:464]\u001b[0m Training loss: 0.473\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:58:46,869 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=2.345: 100%|██████████| 831/831 [00:00<00:00, 1971.32samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:58:47,323 kur.model.executor:197]\u001b[0m Validation loss: 2.345\u001b[0m\n",
      "\n",
      "Epoch 17/20, loss=0.438: 100%|██████| 13300/13300 [00:47<00:00, 277.12samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:59:36,770 kur.model.executor:464]\u001b[0m Training loss: 0.438\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 08:59:36,771 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=2.430: 100%|██████████| 831/831 [00:00<00:00, 1787.97samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 08:59:37,271 kur.model.executor:197]\u001b[0m Validation loss: 2.430\u001b[0m\n",
      "\n",
      "Epoch 18/20, loss=0.425: 100%|██████| 13300/13300 [00:35<00:00, 393.12samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 09:00:14,258 kur.model.executor:464]\u001b[0m Training loss: 0.425\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 09:00:14,259 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=2.394: 100%|██████████| 831/831 [00:00<00:00, 1969.84samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 09:00:14,701 kur.model.executor:197]\u001b[0m Validation loss: 2.394\u001b[0m\n",
      "\n",
      "Epoch 19/20, loss=0.409: 100%|██████| 13300/13300 [00:36<00:00, 381.74samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 09:00:53,110 kur.model.executor:464]\u001b[0m Training loss: 0.409\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 09:00:53,110 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=2.496: 100%|██████████| 831/831 [00:00<00:00, 1976.75samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 09:00:53,556 kur.model.executor:197]\u001b[0m Validation loss: 2.496\u001b[0m\n",
      "\n",
      "Epoch 20/20, loss=0.370: 100%|██████| 13300/13300 [00:40<00:00, 368.44samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 09:01:36,127 kur.model.executor:464]\u001b[0m Training loss: 0.370\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-06 09:01:36,128 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1/best.w.kur\u001b[0m\n",
      "Validating, loss=2.542: 100%|██████████| 831/831 [00:00<00:00, 1867.11samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-06 09:01:36,606 kur.model.executor:197]\u001b[0m Validation loss: 2.542\u001b[0m\n",
      "Completed 20 epochs.\n",
      "\u001b[1;37m[INFO 2017-03-06 09:01:38,138 kur.model.executor:235]\u001b[0m Saving most recent weights: t1/last.w.kur\u001b[0m\n",
      "CPU times: user 19.2 s, sys: 21 s, total: 40.3 s\n",
      "Wall time: 14min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kur -v train char_rrn_demo_fluid.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**In average, it takes 6 minutes to run 5 epochs, compared with first 5 epochs 5:40 min**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAG0CAYAAAA7Go31AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCQvhHE6NriYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='t1/loss1.png') # overfitting and divergence !!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAG0CAYAAAA7Go31AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRKGvyE0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename='t1/loss2.png') # overfitting and divergence !!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAAG0CAYAAAAxRiOnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXHFCASzqVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename='t1/loss3.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "heading_collapsed": true
   },
   "source": [
    "## Evaluate and display output and accuracy\n",
    "**QUESTION**\n",
    "- what exactly is the purpose of evaluate?\n",
    "- what is the output?\n",
    "- is slightly **over 50% accuracy** is considered good result? How so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-03-05 21:56:19,006 kur.kurfile:699]\u001b[0m Parsing source: char_rnn_demo.yaml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:19,017 kur.kurfile:82]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:19,254 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:19,255 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:19,255 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:20,281 kur.backend.keras_backend:195]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:20,282 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:20,282 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:20,282 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:21,171 kur.model.model:284]\u001b[0m Model inputs:  in_seq\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:21,171 kur.model.model:285]\u001b[0m Model outputs: out_char\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:24,031 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "Evaluating: 100%|██████████████████████| 831/831 [00:00<00:00, 1371.43samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:24,693 kur.model.hooks.output_hook:40]\u001b[0m Saving model output as pickle: t1/output.pkl\u001b[0m\n",
      "CPU times: user 116 ms, sys: 137 ms, total: 253 ms\n",
      "Wall time: 7.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kur -v evaluate char_rnn_demo.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%pycat view_outputs.py\n",
    "\n",
    "# we must get output.pkl into and from the correct directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting view_outputs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile view_outputs.py\n",
    "\n",
    "\"\"\"\n",
    "Copyright 2016 Deepgram\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import view_data\n",
    "from vocab import *\n",
    "\n",
    "\n",
    "if len(sys.argv) < 2:\n",
    "    pickle_fname = 't1/output.pkl'\n",
    "else:\n",
    "    pickle_fname = sys.argv[1]\n",
    "\n",
    "with open(pickle_fname, 'rb') as infile:\n",
    "    prediction_data = pickle.load(infile)\n",
    "\n",
    "data = view_data.get_data('evaluate')\n",
    "\n",
    "batch_size = len(prediction_data['truth']['out_char'])\n",
    "\n",
    "for j in range(10):\n",
    "    predicted_char = int_to_char[np.argmax(prediction_data['result']['out_char'][j])]\n",
    "    correct_char = int_to_char[np.argmax(data['out_char'][j])]\n",
    "    print(\n",
    "        '\"%s\" --> \"%s\"' % (\n",
    "            ''.join([\n",
    "                int_to_char[np.argmax(_)]\n",
    "                for _ in data['in_seq'][j]\n",
    "            ]),\n",
    "            predicted_char\n",
    "        )\n",
    "    )\n",
    "    if predicted_char == correct_char:\n",
    "        print((' ' * (seq_len + 5)) + 'CORRECT')\n",
    "    else:\n",
    "        print((' ' * (seq_len + 5)) + 'INCORRECT (%s)' % correct_char)\n",
    "\n",
    "accuracy = sum(\n",
    "    [\n",
    "        int(\n",
    "            np.argmax(prediction_data['result']['out_char'][i]) == np.argmax(prediction_data['truth']['out_char'][i])\n",
    "        )\n",
    "        for i in range(batch_size)\n",
    "    ]\n",
    ") / float(len(prediction_data['truth']['out_char']))\n",
    "\n",
    "print('accuracy = %s' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ng your time with me. mr. bing\" --> \"l\"\r\n",
      "                                   CORRECT\r\n",
      "\"g your time with me. mr. bingl\" --> \"e\"\r\n",
      "                                   CORRECT\r\n",
      "\" your time with me. mr. bingle\" --> \"y\"\r\n",
      "                                   CORRECT\r\n",
      "\"your time with me. mr. bingley\" --> \" \"\r\n",
      "                                   CORRECT\r\n",
      "\"our time with me. mr. bingley \" --> \"w\"\r\n",
      "                                   INCORRECT (f)\r\n",
      "\"ur time with me. mr. bingley f\" --> \"o\"\r\n",
      "                                   CORRECT\r\n",
      "\"r time with me. mr. bingley fo\" --> \"r\"\r\n",
      "                                   INCORRECT (l)\r\n",
      "\" time with me. mr. bingley fol\" --> \"l\"\r\n",
      "                                   CORRECT\r\n",
      "\"time with me. mr. bingley foll\" --> \"o\"\r\n",
      "                                   CORRECT\r\n",
      "\"ime with me. mr. bingley follo\" --> \"w\"\r\n",
      "                                   CORRECT\r\n",
      "accuracy = 0.5054151624548736\r\n"
     ]
    }
   ],
   "source": [
    "!python view_outputs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "heading_collapsed": true
   },
   "source": [
    "## Test \n",
    "- it only provides us with test loss\n",
    "- QUESTION: why test don't save, print output and accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-03-05 21:56:58,008 kur.kurfile:699]\u001b[0m Parsing source: char_rnn_demo.yaml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:58,020 kur.kurfile:82]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:58,280 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:58,281 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:58,281 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:59,369 kur.backend.keras_backend:195]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:59,370 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:59,370 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:59,370 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:59,948 kur.model.model:284]\u001b[0m Model inputs:  in_seq\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:56:59,949 kur.model.model:285]\u001b[0m Model outputs: out_char\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:57:03,459 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "Testing, loss=3.026: 100%|█████████████| 831/831 [00:00<00:00, 1220.91samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-05 21:57:04,211 kur.model.executor:197]\u001b[0m Test loss: 3.026\u001b[0m\n",
      "CPU times: user 138 ms, sys: 160 ms, total: 298 ms\n",
      "Wall time: 7.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kur -v test char_rnn_demo.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m__pycache__\u001b[m\u001b[m/                      \u001b[34mdata\u001b[m\u001b[m/\r\n",
      "\u001b[34mbooks\u001b[m\u001b[m/                            \u001b[34mdata_supplier_char_rnn\u001b[m\u001b[m/\r\n",
      "char_rnn_demo.yaml                make_data.py\r\n",
      "char_rnn_demo_defaults.yaml       steps.sh\r\n",
      "char_rnn_demo_dlnd_defaults.yaml  \u001b[34mt1\u001b[m\u001b[m/\r\n",
      "char_rnn_demo_dp_defaults.yaml    \u001b[34mt2_dp\u001b[m\u001b[m/\r\n",
      "char_rnn_kur.ipynb                \u001b[34mt3_dlnd\u001b[m\u001b[m/\r\n",
      "char_rrn_demo_dlnd_fluid.yaml     view_data.py\r\n",
      "char_rrn_demo_dp_fluid.yaml       view_logs.py\r\n",
      "char_rrn_demo_fluid.yaml          view_outputs.py\r\n",
      "cleaned.txt                       vocab.py\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Adding dropout to reduce overfitting and divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting char_rnn_demo_dp_defaults.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile char_rnn_demo_dp_defaults.yaml\n",
    "\n",
    "---\n",
    "\n",
    "settings:\n",
    "\n",
    "  vocab:\n",
    "    size: 30\n",
    "\n",
    "  rnn:\n",
    "    size: 128\n",
    "    depth: 3\n",
    "\n",
    "model:\n",
    "  - input: in_seq\n",
    "\n",
    "  - for:\n",
    "      range: \"{{ rnn.depth - 1 }}\"\n",
    "      iterate:\n",
    "        - recurrent:\n",
    "            size: \"{{ rnn.size }}\"\n",
    "            type: gru\n",
    "            sequence: yes\n",
    "            bidirectional: no\n",
    "        - batch_normalization\n",
    "        - dropout: \"{{drop_neurons}}\"              # only add dropout to first 2 gru layers\n",
    "\n",
    "  - recurrent:\n",
    "      size: \"{{ rnn.size }}\"\n",
    "      type: gru\n",
    "      sequence: no\n",
    "      bidirectional: no\n",
    "  - dropout: \"{{drop_neurons}}\"                    # add dropout\n",
    "\n",
    "  - dense: \"{{ vocab.size }}\"\n",
    "\n",
    "  - activation: softmax\n",
    "\n",
    "  - output: out_char                               # make a name of output layer\n",
    "           \n",
    "\n",
    "loss:\n",
    "  - target: out_char\n",
    "    name: categorical_crossentropy\n",
    "\n",
    "train:\n",
    "  data:\n",
    "    - jsonl: data/train.jsonl\n",
    "  epochs: \"{{ num_epochs|default(5) }}\"     \n",
    "  weights:\n",
    "    initial: t2_dp/best.w.kur\n",
    "    best: t2_dp/best.w.kur\n",
    "    last: t2_dp/last.w.kur\n",
    "  log: t2_dp/log\n",
    "  hooks:                                   \n",
    "    - plot: t2_dp/loss.png\n",
    "\n",
    "validate:\n",
    "  data:\n",
    "    - jsonl: data/validate.jsonl\n",
    "  weights: t2_dp/best.w.kur\n",
    "\n",
    "\n",
    "test:\n",
    "  data:\n",
    "    - jsonl: data/test.jsonl\n",
    "  weights: t2_dp/best.w.kur\n",
    "\n",
    "\n",
    "evaluate:\n",
    "  data:\n",
    "    - jsonl: data/evaluate.jsonl\n",
    "  weights: t2_dp/best.w.kur\n",
    "\n",
    "  destination: t2_dp/output.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting char_rrn_demo_dp_fluid.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile char_rrn_demo_dp_fluid.yaml\n",
    "\n",
    "---\n",
    "settings: \n",
    "  num_epochs: 1                    # leave it empty means inf number of epochs\n",
    "                                 # so to use default value, just comment this line out\n",
    "  drop_neurons: 0.25\n",
    "\n",
    "\n",
    "include: char_rnn_demo_dp_defaults.yaml\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### How to understand the details of debug info\n",
    "**Questions**\n",
    "- see questions in annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-03-05 21:59:17,123 kur.kurfile:699]\u001b[0m Parsing source: char_rrn_demo_dp_fluid.yaml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:59:17,127 kur.kurfile:699]\u001b[0m Parsing source: char_rnn_demo_dp_defaults.yaml, included by char_rrn_demo_dp_fluid.yaml.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:59:17,137 kur.kurfile:82]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:17,138 kur.kurfile:784]\u001b[0m Parsing Kurfile section: settings\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:17,138 kur.kurfile:784]\u001b[0m Parsing Kurfile section: train\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:17,142 kur.kurfile:784]\u001b[0m Parsing Kurfile section: validate\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:17,143 kur.kurfile:784]\u001b[0m Parsing Kurfile section: test\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:17,144 kur.kurfile:784]\u001b[0m Parsing Kurfile section: evaluate\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:17,147 kur.containers.layers.placeholder:63]\u001b[0m Using short-hand name for placeholder: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:17,147 kur.containers.layers.placeholder:97]\u001b[0m Placeholder \"in_seq\" has a deferred shape.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:17,157 kur.containers.layers.output:50]\u001b[0m Using short-hand name for output: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:17,158 kur.kurfile:784]\u001b[0m Parsing Kurfile section: loss\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:59:17,160 kur.loggers.binary_logger:71]\u001b[0m Loading log data: t2_dp/log\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:17,160 kur.loggers.binary_logger:158]\u001b[0m Reading logger summary.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:17,163 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: training_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:17,163 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: training_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:17,164 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: training_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:17,164 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:17,165 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:17,165 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:20,403 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 32\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:20,660 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 32\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:20,661 kur.backend.backend:187]\u001b[0m Using backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:59:20,662 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:59:20,662 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:59:20,662 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:20,662 kur.backend.keras_backend:124]\u001b[0m Using the system-default Keras backend.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:20,662 kur.backend.keras_backend:189]\u001b[0m Overriding environmental variables: {'TF_CPP_MIN_LOG_LEVEL': '1', 'THEANO_FLAGS': None, 'KERAS_BACKEND': None}\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:59:21,777 kur.backend.keras_backend:195]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:59:21,777 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:59:21,777 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:272]\u001b[0m Assembled Node: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:274]\u001b[0m   Uses: \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:276]\u001b[0m   Used by: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:277]\u001b[0m   Aliases: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:272]\u001b[0m Assembled Node: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:274]\u001b[0m   Uses: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:276]\u001b[0m   Used by: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:277]\u001b[0m   Aliases: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:272]\u001b[0m Assembled Node: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:274]\u001b[0m   Uses: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:276]\u001b[0m   Used by: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:277]\u001b[0m   Aliases: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:272]\u001b[0m Assembled Node: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:274]\u001b[0m   Uses: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:276]\u001b[0m   Used by: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:277]\u001b[0m   Aliases: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:272]\u001b[0m Assembled Node: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:274]\u001b[0m   Uses: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:276]\u001b[0m   Used by: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:277]\u001b[0m   Aliases: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,778 kur.model.model:272]\u001b[0m Assembled Node: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:274]\u001b[0m   Uses: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:276]\u001b[0m   Used by: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:277]\u001b[0m   Aliases: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:272]\u001b[0m Assembled Node: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:274]\u001b[0m   Uses: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:276]\u001b[0m   Used by: ..recurrent.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:277]\u001b[0m   Aliases: ..dropout.1, ..for.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:272]\u001b[0m Assembled Node: ..recurrent.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:274]\u001b[0m   Uses: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:276]\u001b[0m   Used by: ..dropout.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:277]\u001b[0m   Aliases: ..recurrent.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:272]\u001b[0m Assembled Node: ..dropout.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:274]\u001b[0m   Uses: ..recurrent.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:276]\u001b[0m   Used by: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:277]\u001b[0m   Aliases: ..dropout.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:272]\u001b[0m Assembled Node: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:274]\u001b[0m   Uses: ..dropout.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:276]\u001b[0m   Used by: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:277]\u001b[0m   Aliases: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:272]\u001b[0m Assembled Node: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:274]\u001b[0m   Uses: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,779 kur.model.model:276]\u001b[0m   Used by: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,780 kur.model.model:277]\u001b[0m   Aliases: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,780 kur.model.model:272]\u001b[0m Assembled Node: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,780 kur.model.model:274]\u001b[0m   Uses: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,780 kur.model.model:276]\u001b[0m   Used by: \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,780 kur.model.model:277]\u001b[0m   Aliases: out_char\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:59:21,780 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,780 kur.model.model:311]\u001b[0m Building node: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,780 kur.model.model:312]\u001b[0m   Aliases: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,780 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,780 kur.containers.layers.placeholder:117]\u001b[0m Creating placeholder for \"in_seq\" with data type \"float32\".\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,780 kur.model.model:125]\u001b[0m Trying to infer shape for input \"in_seq\"\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,780 kur.model.model:143]\u001b[0m Inferred shape for input \"in_seq\": (30, 30)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,780 kur.containers.layers.placeholder:127]\u001b[0m Inferred shape: (30, 30)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,784 kur.model.model:382]\u001b[0m   Value: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,785 kur.model.model:311]\u001b[0m Building node: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,785 kur.model.model:312]\u001b[0m   Aliases: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,785 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:21,785 kur.model.model:315]\u001b[0m   - in_seq: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:22,745 kur.model.model:382]\u001b[0m   Value: DimShuffle{1,0,2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:22,746 kur.model.model:311]\u001b[0m Building node: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:22,746 kur.model.model:312]\u001b[0m   Aliases: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:22,746 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:22,746 kur.model.model:315]\u001b[0m   - ..recurrent.0: DimShuffle{1,0,2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:22,765 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:22,765 kur.model.model:311]\u001b[0m Building node: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:22,765 kur.model.model:312]\u001b[0m   Aliases: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:22,765 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:22,765 kur.model.model:315]\u001b[0m   - ..batch_normalization.0: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,435 kur.model.model:382]\u001b[0m   Value: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,436 kur.model.model:311]\u001b[0m Building node: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,436 kur.model.model:312]\u001b[0m   Aliases: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,436 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,436 kur.model.model:315]\u001b[0m   - ..dropout.0: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,482 kur.model.model:382]\u001b[0m   Value: DimShuffle{1,0,2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,482 kur.model.model:311]\u001b[0m Building node: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,483 kur.model.model:312]\u001b[0m   Aliases: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,483 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,483 kur.model.model:315]\u001b[0m   - ..recurrent.1: DimShuffle{1,0,2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,500 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,500 kur.model.model:311]\u001b[0m Building node: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,500 kur.model.model:312]\u001b[0m   Aliases: ..dropout.1, ..for.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,500 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,500 kur.model.model:315]\u001b[0m   - ..batch_normalization.1: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,580 kur.model.model:382]\u001b[0m   Value: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,581 kur.model.model:311]\u001b[0m Building node: ..recurrent.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,581 kur.model.model:312]\u001b[0m   Aliases: ..recurrent.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,581 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,581 kur.model.model:315]\u001b[0m   - ..dropout.1: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,726 kur.model.model:382]\u001b[0m   Value: Subtensor{int64}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,726 kur.model.model:311]\u001b[0m Building node: ..dropout.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,726 kur.model.model:312]\u001b[0m   Aliases: ..dropout.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,726 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,726 kur.model.model:315]\u001b[0m   - ..recurrent.2: Subtensor{int64}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,786 kur.model.model:382]\u001b[0m   Value: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,786 kur.model.model:311]\u001b[0m Building node: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,786 kur.model.model:312]\u001b[0m   Aliases: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,786 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,786 kur.model.model:315]\u001b[0m   - ..dropout.2: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,788 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,791 kur.model.model:311]\u001b[0m Building node: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,791 kur.model.model:312]\u001b[0m   Aliases: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,792 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,792 kur.model.model:315]\u001b[0m   - ..dense.0: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,792 kur.model.model:382]\u001b[0m   Value: Softmax.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,793 kur.model.model:311]\u001b[0m Building node: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,793 kur.model.model:312]\u001b[0m   Aliases: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,793 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,793 kur.model.model:315]\u001b[0m   - ..activation.0: Softmax.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,793 kur.model.model:382]\u001b[0m   Value: Softmax.0\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:59:23,794 kur.model.model:284]\u001b[0m Model inputs:  in_seq\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:59:23,794 kur.model.model:285]\u001b[0m Model outputs: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,794 kur.model.model:229]\u001b[0m Loading model weights from: t2_dp/best.w.kur\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:59:23,831 kur.model.executor:313]\u001b[0m Best historical training loss: 1.305\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:59:23,831 kur.model.executor:320]\u001b[0m Best historical validation loss: 1.602\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:59:23,831 kur.model.executor:331]\u001b[0m Restarting from epoch 22.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,832 kur.model.executor:353]\u001b[0m Epoch handling mode: additional\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,832 kur.model.executor:101]\u001b[0m Recompiling the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:23,832 kur.backend.keras_backend:527]\u001b[0m Instantiating a Keras model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,641 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,641 kur.backend.keras_backend:538]\u001b[0m Layer (type)                     Output Shape          Param #     Connected to                     \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,641 kur.backend.keras_backend:538]\u001b[0m ====================================================================================================\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,641 kur.backend.keras_backend:538]\u001b[0m in_seq (InputLayer)              (None, 30, 30)        0                                            \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,641 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,641 kur.backend.keras_backend:538]\u001b[0m ..recurrent.0 (GRU)              (None, 30, 128)       61056       in_seq[0][0]                     \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,641 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,641 kur.backend.keras_backend:538]\u001b[0m ..batch_normalization.0 (BatchNo (None, 30, 128)       512         ..recurrent.0[0][0]              \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,641 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,641 kur.backend.keras_backend:538]\u001b[0m ..dropout.0 (Dropout)            (None, 30, 128)       0           ..batch_normalization.0[0][0]    \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,641 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,641 kur.backend.keras_backend:538]\u001b[0m ..recurrent.1 (GRU)              (None, 30, 128)       98688       ..dropout.0[0][0]                \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,641 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,641 kur.backend.keras_backend:538]\u001b[0m ..batch_normalization.1 (BatchNo (None, 30, 128)       512         ..recurrent.1[0][0]              \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,641 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,641 kur.backend.keras_backend:538]\u001b[0m ..dropout.1 (Dropout)            (None, 30, 128)       0           ..batch_normalization.1[0][0]    \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,641 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,642 kur.backend.keras_backend:538]\u001b[0m ..recurrent.2 (GRU)              (None, 128)           98688       ..dropout.1[0][0]                \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,642 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,642 kur.backend.keras_backend:538]\u001b[0m ..dropout.2 (Dropout)            (None, 128)           0           ..recurrent.2[0][0]              \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,642 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,642 kur.backend.keras_backend:538]\u001b[0m ..dense.0 (Dense)                (None, 30)            3870        ..dropout.2[0][0]                \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,642 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,642 kur.backend.keras_backend:538]\u001b[0m ..activation.0 (Activation)      (None, 30)            0           ..dense.0[0][0]                  \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,642 kur.backend.keras_backend:538]\u001b[0m ====================================================================================================\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,642 kur.backend.keras_backend:538]\u001b[0m Total params: 263,326\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,642 kur.backend.keras_backend:538]\u001b[0m Trainable params: 262,814\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,642 kur.backend.keras_backend:538]\u001b[0m Non-trainable params: 512\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,642 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,642 kur.backend.keras_backend:538]\u001b[0m \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,642 kur.backend.keras_backend:576]\u001b[0m Assembling a training function from the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:24,647 kur.backend.keras_backend:509]\u001b[0m Adding additional inputs: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:43,728 kur.backend.keras_backend:599]\u001b[0m Additional inputs for log functions: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:43,728 kur.backend.keras_backend:616]\u001b[0m Expected input shapes: in_seq=(None, 30, 30), out_char=(None, None)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:43,728 kur.backend.keras_backend:634]\u001b[0m Compiled model: {'func': <keras.backend.theano_backend.Function object at 0x111336860>, 'shapes': {'input': [(None, 30, 30), (None, None)]}, 'names': {'output': ['..activation.0', 'out_char'], 'input': ['in_seq', 'out_char']}}\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:43,728 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:43,728 kur.providers.batch_provider:102]\u001b[0m Maximum number of batches set to: 1\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 21:59:43,745 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:43,746 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:43,746 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:43,856 kur.providers.provider:144]\u001b[0m Data source \"out_char\": entries=13300, shape=(30,)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:43,856 kur.providers.provider:144]\u001b[0m Data source \"in_seq\": entries=13300, shape=(30, 30)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,091 kur.model.hooks.plot_hook:73]\u001b[0m Plotting hook received training message.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,091 kur.model.hooks.plot_hook:80]\u001b[0m Plotting hook does not handle this status.\u001b[0m\n",
      "\n",
      "Epoch 22/22, loss=N/A:   0%|                     | 0/13300 [00:00<?, ?samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:44,097 kur.providers.shuffle_provider:184]\u001b[0m Shuffling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,291 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,291 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,292 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,292 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,299 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,428 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,428 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,430 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,431 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,431 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,432 kur.loggers.binary_logger:144]\u001b[0m Writing logger summary.\u001b[0m\n",
      "Epoch 22/22, loss=0.771:   0%|          | 32/13300 [00:00<02:19, 95.15samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:44,433 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,433 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,434 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,563 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.074:   0%|         | 64/13300 [00:00<01:53, 116.55samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:44,563 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,563 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,565 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,662 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,662 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,662 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,664 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,794 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.098:   1%|        | 128/13300 [00:00<01:33, 141.10samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:44,794 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,794 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,796 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,922 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.097:   1%|        | 160/13300 [00:00<01:20, 162.23samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:44,922 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,922 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:44,924 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,047 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.127:   1%|        | 192/13300 [00:00<01:11, 182.07samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:45,048 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,048 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,049 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,155 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.136:   2%|▏       | 224/13300 [00:01<01:03, 206.10samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:45,155 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,155 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,157 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,276 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.181:   2%|▏       | 256/13300 [00:01<00:59, 220.80samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:45,276 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,276 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,278 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,407 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.184:   2%|▏       | 288/13300 [00:01<00:57, 226.05samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:45,410 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,411 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,413 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,556 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.256:   2%|▏       | 320/13300 [00:01<00:58, 223.57samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:45,557 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,557 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,558 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,690 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:   3%|▏       | 352/13300 [00:01<00:56, 228.24samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:45,690 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,691 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,693 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,802 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.274:   3%|▏       | 384/13300 [00:01<00:53, 242.63samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:45,803 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,803 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,804 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,917 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.289:   3%|▎       | 416/13300 [00:01<00:51, 252.13samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:45,918 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,918 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:45,919 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,048 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.309:   3%|▎       | 448/13300 [00:01<00:51, 250.05samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:46,048 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,049 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,050 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,173 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.288:   4%|▎       | 480/13300 [00:02<00:50, 251.93samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:46,173 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,173 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,175 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,298 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.291:   4%|▎       | 512/13300 [00:02<00:50, 253.09samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:46,298 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,299 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,300 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,401 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.288:   4%|▎       | 544/13300 [00:02<00:47, 268.07samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:46,401 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,401 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,403 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,518 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.264:   4%|▎       | 576/13300 [00:02<00:47, 269.54samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:46,518 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,519 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,520 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,646 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.241:   5%|▎       | 608/13300 [00:02<00:48, 263.25samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:46,646 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,647 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,648 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,783 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.228:   5%|▍       | 640/13300 [00:02<00:49, 253.32samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:46,784 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,784 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,786 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,916 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.245:   5%|▍       | 672/13300 [00:02<00:50, 249.67samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:46,916 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,917 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:46,918 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,050 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.251:   5%|▍       | 704/13300 [00:02<00:51, 246.51samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:47,050 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,050 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,051 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,163 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.256:   6%|▍       | 736/13300 [00:03<00:49, 256.00samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:47,164 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,164 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,165 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,291 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.249:   6%|▍       | 768/13300 [00:03<00:49, 254.27samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:47,292 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,292 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,293 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,415 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.263:   6%|▍       | 800/13300 [00:03<00:48, 255.41samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:47,416 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,416 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,418 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,540 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.261:   6%|▌       | 832/13300 [00:03<00:48, 255.55samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:47,541 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,541 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,544 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,650 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.263:   6%|▌       | 864/13300 [00:03<00:46, 265.51samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:47,650 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,650 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,652 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,779 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.259:   7%|▌       | 896/13300 [00:03<00:47, 260.03samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:47,779 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,780 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,781 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,902 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.251:   7%|▌       | 928/13300 [00:03<00:47, 260.03samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:47,902 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,902 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:47,904 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,023 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.237:   7%|▌       | 960/13300 [00:03<00:47, 261.00samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:48,024 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,024 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,025 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,138 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.241:   7%|▌       | 992/13300 [00:04<00:46, 266.19samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:48,138 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,139 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,140 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,262 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.243:   8%|▌      | 1024/13300 [00:04<00:46, 263.72samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:48,262 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,263 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,264 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,388 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.250:   8%|▌      | 1056/13300 [00:04<00:47, 260.45samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:48,389 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,390 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,391 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,518 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.258:   8%|▌      | 1088/13300 [00:04<00:47, 256.07samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:48,519 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,521 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,521 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,648 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.259:   8%|▌      | 1120/13300 [00:04<00:48, 253.39samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:48,648 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,648 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,650 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,779 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.264:   9%|▌      | 1152/13300 [00:04<00:48, 250.46samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:48,779 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,779 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,781 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,904 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.257:   9%|▌      | 1184/13300 [00:04<00:48, 252.02samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:48,904 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,905 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:48,906 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,035 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.265:   9%|▋      | 1216/13300 [00:04<00:48, 249.45samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:49,036 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,036 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,037 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,167 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.266:   9%|▋      | 1248/13300 [00:05<00:48, 247.42samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:49,167 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,168 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,169 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,275 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.259:  10%|▋      | 1280/13300 [00:05<00:46, 260.10samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:49,276 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,276 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,278 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,410 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.263:  10%|▋      | 1312/13300 [00:05<00:47, 252.89samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:49,411 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,411 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,412 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,551 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.261:  10%|▋      | 1344/13300 [00:05<00:48, 244.71samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:49,551 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,551 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,553 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,672 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.254:  10%|▋      | 1376/13300 [00:05<00:47, 250.38samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:49,672 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,672 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,674 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,803 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.257:  11%|▋      | 1408/13300 [00:05<00:47, 248.19samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:49,804 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,804 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,806 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,937 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.252:  11%|▊      | 1440/13300 [00:05<00:48, 245.61samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:49,937 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,937 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:49,939 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,058 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.248:  11%|▊      | 1472/13300 [00:05<00:47, 251.07samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:50,058 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,058 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,060 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,183 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.251:  11%|▊      | 1504/13300 [00:06<00:46, 252.11samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:50,184 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,184 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,185 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,315 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.259:  12%|▊      | 1536/13300 [00:06<00:47, 249.43samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:50,315 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,315 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,317 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,435 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.257:  12%|▊      | 1568/13300 [00:06<00:46, 254.11samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:50,436 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,436 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,437 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,567 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.261:  12%|▊      | 1600/13300 [00:06<00:46, 250.69samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:50,567 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,567 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,569 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,693 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.265:  12%|▊      | 1632/13300 [00:06<00:46, 251.21samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:50,694 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,694 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,696 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,838 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.266:  13%|▉      | 1664/13300 [00:06<00:48, 241.80samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:50,838 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,838 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,840 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,963 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.271:  13%|▉      | 1696/13300 [00:06<00:47, 245.33samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:50,964 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,964 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:50,966 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,121 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.266:  13%|▉      | 1728/13300 [00:07<00:50, 230.70samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:51,122 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,122 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,124 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,241 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.266:  13%|▉      | 1760/13300 [00:07<00:47, 240.48samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:51,242 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,242 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,245 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,367 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  13%|▉      | 1792/13300 [00:07<00:47, 244.55samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:51,368 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,368 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,370 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,490 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  14%|▉      | 1824/13300 [00:07<00:46, 249.12samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:51,490 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,491 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,492 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,615 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.273:  14%|▉      | 1856/13300 [00:07<00:45, 250.89samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:51,616 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,616 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,618 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,736 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  14%|▉      | 1888/13300 [00:07<00:44, 254.79samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:51,737 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,737 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,739 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,867 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.262:  14%|█      | 1920/13300 [00:07<00:45, 251.69samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:51,868 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,868 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,869 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,995 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.267:  15%|█      | 1952/13300 [00:07<00:45, 251.17samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:51,996 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,996 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:51,997 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,119 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.265:  15%|█      | 1984/13300 [00:08<00:44, 253.27samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:52,120 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,120 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,121 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,251 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.261:  15%|█      | 2016/13300 [00:08<00:45, 250.16samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:52,251 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,251 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,253 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,373 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.259:  15%|█      | 2048/13300 [00:08<00:44, 253.45samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:52,374 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,374 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,375 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,505 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.266:  16%|█      | 2080/13300 [00:08<00:44, 249.89samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:52,506 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,506 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,507 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,634 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.268:  16%|█      | 2112/13300 [00:08<00:44, 249.31samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:52,635 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,635 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,636 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,753 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.266:  16%|█▏     | 2144/13300 [00:08<00:43, 255.12samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:52,753 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,754 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,755 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,876 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.264:  16%|█▏     | 2176/13300 [00:08<00:43, 256.19samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:52,877 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,877 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:52,879 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,002 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.260:  17%|█▏     | 2208/13300 [00:08<00:43, 255.77samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:53,003 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,003 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,004 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,166 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.256:  17%|█▏     | 2240/13300 [00:09<00:47, 234.27samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:53,166 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,166 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,168 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,293 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.257:  17%|█▏     | 2272/13300 [00:09<00:46, 238.90samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:53,294 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,294 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,296 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,419 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.260:  17%|█▏     | 2304/13300 [00:09<00:45, 243.24samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:53,420 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,420 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,421 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,544 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.257:  18%|█▏     | 2336/13300 [00:09<00:44, 247.05samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:53,545 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,545 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,547 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,675 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.250:  18%|█▏     | 2368/13300 [00:09<00:44, 246.14samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:53,677 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,677 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,678 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,812 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.253:  18%|█▎     | 2400/13300 [00:09<00:45, 242.20samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:53,813 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,813 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,815 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,940 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.256:  18%|█▎     | 2432/13300 [00:09<00:44, 244.50samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:53,941 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,941 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:53,942 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,066 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.257:  19%|█▎     | 2464/13300 [00:09<00:43, 247.43samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:54,066 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,067 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,068 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,199 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.258:  19%|█▎     | 2496/13300 [00:10<00:43, 245.59samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:54,199 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,199 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,201 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,313 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.257:  19%|█▎     | 2528/13300 [00:10<00:42, 254.70samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:54,314 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,314 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,316 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,439 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.256:  19%|█▎     | 2560/13300 [00:10<00:42, 254.94samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:54,439 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,439 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,441 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,568 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.254:  19%|█▎     | 2592/13300 [00:10<00:42, 252.39samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:54,569 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,569 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,570 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,690 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.250:  20%|█▍     | 2624/13300 [00:10<00:41, 255.52samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:54,690 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,691 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,692 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,821 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.252:  20%|█▍     | 2656/13300 [00:10<00:42, 252.03samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:54,821 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,822 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,823 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,941 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.252:  20%|█▍     | 2688/13300 [00:10<00:41, 256.38samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:54,941 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,941 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:54,943 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,076 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.251:  20%|█▍     | 2720/13300 [00:10<00:42, 250.17samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:55,076 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,077 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,078 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,196 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.250:  21%|█▍     | 2752/13300 [00:11<00:41, 255.08samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:55,196 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,196 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,198 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,317 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.253:  21%|█▍     | 2784/13300 [00:11<00:40, 257.88samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:55,317 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,317 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,319 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,440 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.255:  21%|█▍     | 2816/13300 [00:11<00:40, 258.39samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:55,440 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,444 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,445 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,560 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.253:  21%|█▍     | 2848/13300 [00:11<00:40, 260.55samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:55,561 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,561 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,562 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,683 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.252:  22%|█▌     | 2880/13300 [00:11<00:40, 260.40samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:55,684 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,684 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,685 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,797 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.248:  22%|█▌     | 2912/13300 [00:11<00:38, 266.63samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:55,797 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,797 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,799 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,927 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.246:  22%|█▌     | 2944/13300 [00:11<00:39, 260.12samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:55,927 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,927 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:55,929 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,049 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.246:  22%|█▌     | 2976/13300 [00:11<00:39, 260.28samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:56,050 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,050 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,052 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,173 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.243:  23%|█▌     | 3008/13300 [00:12<00:39, 259.64samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:56,174 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,174 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,175 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,293 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.243:  23%|█▌     | 3040/13300 [00:12<00:39, 261.92samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:56,293 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,294 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,295 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,419 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.243:  23%|█▌     | 3072/13300 [00:12<00:39, 259.25samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:56,420 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,420 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,422 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,538 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.241:  23%|█▋     | 3104/13300 [00:12<00:38, 262.45samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:56,538 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,538 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,540 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,654 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.241:  24%|█▋     | 3136/13300 [00:12<00:38, 266.04samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:56,655 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,655 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,655 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,774 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.248:  24%|█▋     | 3168/13300 [00:12<00:38, 266.54samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:56,774 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,774 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,776 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,900 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.247:  24%|█▋     | 3200/13300 [00:12<00:38, 262.24samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:56,901 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,901 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:56,902 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,023 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.242:  24%|█▋     | 3232/13300 [00:12<00:38, 261.93samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:57,023 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,024 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,025 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,139 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.240:  25%|█▋     | 3264/13300 [00:13<00:37, 265.76samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:57,140 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,140 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,141 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,261 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.240:  25%|█▋     | 3296/13300 [00:13<00:37, 264.70samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:57,262 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,262 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,264 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,383 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.242:  25%|█▊     | 3328/13300 [00:13<00:37, 264.18samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:57,383 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,384 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,385 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,494 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.244:  25%|█▊     | 3360/13300 [00:13<00:36, 271.04samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:57,494 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,494 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,496 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,612 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.248:  26%|█▊     | 3392/13300 [00:13<00:36, 271.30samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:57,612 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,612 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,614 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,721 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.247:  26%|█▊     | 3424/13300 [00:13<00:35, 277.48samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:57,721 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,721 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,723 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,843 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.245:  26%|█▊     | 3456/13300 [00:13<00:36, 272.41samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:57,844 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,844 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,845 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,963 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.245:  26%|█▊     | 3488/13300 [00:13<00:36, 270.51samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:57,964 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,964 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:57,965 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,085 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.242:  26%|█▊     | 3520/13300 [00:13<00:36, 268.58samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:58,085 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,085 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,087 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,198 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.240:  27%|█▊     | 3552/13300 [00:14<00:35, 272.22samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:58,199 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,199 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,200 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,327 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.243:  27%|█▉     | 3584/13300 [00:14<00:36, 264.68samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:58,328 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,328 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,331 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,447 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.245:  27%|█▉     | 3616/13300 [00:14<00:36, 265.13samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:58,448 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,448 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,449 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,570 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.245:  27%|█▉     | 3648/13300 [00:14<00:36, 263.86samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:58,570 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,571 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,572 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,689 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.245:  28%|█▉     | 3680/13300 [00:14<00:36, 265.49samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:58,689 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,689 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,691 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,817 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.246:  28%|█▉     | 3712/13300 [00:14<00:36, 260.19samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:58,818 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,818 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,819 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,944 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.244:  28%|█▉     | 3744/13300 [00:14<00:37, 257.66samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:58,945 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,945 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:58,946 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,070 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.242:  28%|█▉     | 3776/13300 [00:14<00:37, 256.92samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:59,070 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,071 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,072 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,181 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.242:  29%|██     | 3808/13300 [00:15<00:35, 265.46samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:59,182 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,182 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,184 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,297 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.243:  29%|██     | 3840/13300 [00:15<00:35, 268.45samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:59,298 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,298 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,299 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,423 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.244:  29%|██     | 3872/13300 [00:15<00:35, 264.12samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:59,423 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,423 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,425 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,552 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.243:  29%|██     | 3904/13300 [00:15<00:36, 259.14samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:59,552 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,552 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,554 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,672 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.241:  30%|██     | 3936/13300 [00:15<00:35, 261.28samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:59,672 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,672 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,674 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,793 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.240:  30%|██     | 3968/13300 [00:15<00:35, 262.33samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:59,793 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,793 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,795 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,936 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.241:  30%|██     | 4000/13300 [00:15<00:37, 249.53samples/s]\u001b[1;34m[DEBUG 2017-03-05 21:59:59,936 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,936 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 21:59:59,938 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,057 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.240:  30%|██     | 4032/13300 [00:15<00:36, 253.28samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:00,058 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,058 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,060 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,180 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.239:  31%|██▏    | 4064/13300 [00:16<00:36, 255.85samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:00,180 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,180 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,182 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,296 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.239:  31%|██▏    | 4096/13300 [00:16<00:35, 260.97samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:00,297 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,297 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,299 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,416 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.240:  31%|██▏    | 4128/13300 [00:16<00:34, 262.92samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:00,416 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,417 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,418 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,571 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.243:  31%|██▏    | 4160/13300 [00:16<00:37, 242.95samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:00,572 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,572 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,574 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,719 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.242:  32%|██▏    | 4192/13300 [00:16<00:38, 234.52samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:00,719 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,719 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,721 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,845 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.244:  32%|██▏    | 4224/13300 [00:16<00:37, 239.70samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:00,846 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,846 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,847 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,962 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.244:  32%|██▏    | 4256/13300 [00:16<00:36, 248.80samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:00,963 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,963 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:00,965 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,100 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.245:  32%|██▎    | 4288/13300 [00:17<00:36, 243.88samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:01,100 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,100 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,102 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,221 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.246:  32%|██▎    | 4320/13300 [00:17<00:36, 249.41samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:01,222 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,222 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,223 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,345 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.248:  33%|██▎    | 4352/13300 [00:17<00:35, 252.39samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:01,345 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,345 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,346 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,472 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.248:  33%|██▎    | 4384/13300 [00:17<00:35, 252.15samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:01,472 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,472 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,474 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,595 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.246:  33%|██▎    | 4416/13300 [00:17<00:34, 254.28samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:01,596 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,596 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,597 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,708 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.245:  33%|██▎    | 4448/13300 [00:17<00:33, 262.22samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:01,709 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,709 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,710 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,842 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.245:  34%|██▎    | 4480/13300 [00:17<00:34, 254.98samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:01,842 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,843 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,844 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,967 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.244:  34%|██▎    | 4512/13300 [00:17<00:34, 255.43samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:01,967 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,967 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:01,969 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,080 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.245:  34%|██▍    | 4544/13300 [00:17<00:33, 263.15samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:02,080 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,080 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,082 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,197 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.245:  34%|██▍    | 4576/13300 [00:18<00:32, 265.73samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:02,198 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,198 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,199 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,313 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.245:  35%|██▍    | 4608/13300 [00:18<00:32, 268.92samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:02,313 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,314 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,315 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,427 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.245:  35%|██▍    | 4640/13300 [00:18<00:31, 272.02samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:02,428 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,428 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,429 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,566 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.247:  35%|██▍    | 4672/13300 [00:18<00:33, 258.19samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:02,567 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,567 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,568 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,680 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.249:  35%|██▍    | 4704/13300 [00:18<00:32, 264.42samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:02,681 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,682 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,683 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,799 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.250:  36%|██▍    | 4736/13300 [00:18<00:32, 266.08samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:02,799 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,799 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,801 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,916 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.251:  36%|██▌    | 4768/13300 [00:18<00:31, 268.17samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:02,916 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,916 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:02,918 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,042 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.253:  36%|██▌    | 4800/13300 [00:18<00:32, 263.82samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:03,042 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,042 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,044 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,183 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.254:  36%|██▌    | 4832/13300 [00:19<00:33, 251.55samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:03,183 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,183 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,185 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,302 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.253:  37%|██▌    | 4864/13300 [00:19<00:32, 256.19samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:03,303 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,303 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,305 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,412 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.254:  37%|██▌    | 4896/13300 [00:19<00:31, 265.70samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:03,413 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,413 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,414 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,530 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.253:  37%|██▌    | 4928/13300 [00:19<00:31, 267.52samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:03,531 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,531 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,532 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,649 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.252:  37%|██▌    | 4960/13300 [00:19<00:31, 267.98samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:03,649 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,650 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,651 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,783 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.253:  38%|██▋    | 4992/13300 [00:19<00:32, 258.65samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:03,783 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,788 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,789 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,910 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.255:  38%|██▋    | 5024/13300 [00:19<00:32, 256.32samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:03,911 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,911 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:03,912 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,035 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.258:  38%|██▋    | 5056/13300 [00:19<00:32, 256.24samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:04,036 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,036 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,037 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,150 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.263:  38%|██▋    | 5088/13300 [00:20<00:31, 262.80samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:04,150 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,150 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,152 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,272 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.263:  38%|██▋    | 5120/13300 [00:20<00:31, 262.39samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:04,273 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,273 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,274 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,391 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.265:  39%|██▋    | 5152/13300 [00:20<00:30, 264.10samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:04,392 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,392 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,393 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,528 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.267:  39%|██▋    | 5184/13300 [00:20<00:31, 254.52samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:04,528 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,528 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,530 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,653 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.269:  39%|██▋    | 5216/13300 [00:20<00:31, 254.88samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:04,653 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,653 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,655 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,774 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.268:  39%|██▊    | 5248/13300 [00:20<00:31, 257.44samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:04,775 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,775 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,776 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,898 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.270:  40%|██▊    | 5280/13300 [00:20<00:31, 257.96samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:04,898 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,898 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:04,900 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,021 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.269:  40%|██▊    | 5312/13300 [00:20<00:30, 258.71samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:05,021 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,021 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,023 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,138 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.268:  40%|██▊    | 5344/13300 [00:21<00:30, 262.87samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:05,138 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,138 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,140 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,261 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.270:  40%|██▊    | 5376/13300 [00:21<00:30, 261.76samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:05,262 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,262 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,263 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,392 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.270:  41%|██▊    | 5408/13300 [00:21<00:30, 256.15samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:05,393 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,393 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,394 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,528 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.271:  41%|██▊    | 5440/13300 [00:21<00:31, 249.95samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:05,528 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,528 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,530 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,649 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  41%|██▉    | 5472/13300 [00:21<00:30, 254.24samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:05,649 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,649 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,651 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,762 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.273:  41%|██▉    | 5504/13300 [00:21<00:29, 261.94samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:05,763 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,763 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,764 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,879 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.273:  42%|██▉    | 5536/13300 [00:21<00:29, 265.21samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:05,880 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,880 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,881 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,992 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.274:  42%|██▉    | 5568/13300 [00:21<00:28, 270.68samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:05,992 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,992 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:05,994 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,138 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  42%|██▉    | 5600/13300 [00:22<00:30, 252.66samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:06,139 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,139 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,141 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,268 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.275:  42%|██▉    | 5632/13300 [00:22<00:30, 250.53samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:06,269 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,269 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,270 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,394 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.275:  43%|██▉    | 5664/13300 [00:22<00:30, 251.92samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:06,394 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,394 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,396 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,512 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.274:  43%|██▉    | 5696/13300 [00:22<00:29, 257.26samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:06,512 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,513 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,514 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,623 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.273:  43%|███    | 5728/13300 [00:22<00:28, 265.62samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:06,624 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,624 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,626 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,739 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.274:  43%|███    | 5760/13300 [00:22<00:28, 268.75samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:06,740 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,740 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,741 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,873 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.273:  44%|███    | 5792/13300 [00:22<00:29, 258.86samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:06,874 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,874 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:06,875 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,001 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.271:  44%|███    | 5824/13300 [00:22<00:29, 256.11samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:07,002 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,002 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,003 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,125 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  44%|███    | 5856/13300 [00:23<00:29, 256.65samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:07,126 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,126 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,127 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,247 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.273:  44%|███    | 5888/13300 [00:23<00:28, 258.47samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:07,248 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,248 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,249 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,366 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.273:  45%|███    | 5920/13300 [00:23<00:28, 261.80samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:07,366 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,366 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,368 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,485 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  45%|███▏   | 5952/13300 [00:23<00:27, 263.83samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:07,485 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,486 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,487 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,601 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.273:  45%|███▏   | 5984/13300 [00:23<00:27, 267.47samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:07,601 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,601 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,603 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,743 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.271:  45%|███▏   | 6016/13300 [00:23<00:28, 252.86samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:07,744 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,744 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,745 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,862 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.269:  45%|███▏   | 6048/13300 [00:23<00:28, 257.33samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:07,863 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,863 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,865 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,986 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.271:  46%|███▏   | 6080/13300 [00:23<00:28, 257.65samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:07,987 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,987 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:07,989 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,106 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.273:  46%|███▏   | 6112/13300 [00:24<00:27, 260.44samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:08,106 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,107 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,108 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,223 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  46%|███▏   | 6144/13300 [00:24<00:27, 264.14samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:08,224 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,224 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,225 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,340 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  46%|███▎   | 6176/13300 [00:24<00:26, 267.21samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:08,340 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,340 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,342 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,469 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.273:  47%|███▎   | 6208/13300 [00:24<00:27, 260.82samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:08,470 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,470 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,473 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,591 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  47%|███▎   | 6240/13300 [00:24<00:26, 261.69samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:08,591 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,591 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,593 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,712 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  47%|███▎   | 6272/13300 [00:24<00:26, 262.14samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:08,713 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,713 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,714 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,834 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  47%|███▎   | 6304/13300 [00:24<00:26, 262.39samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:08,834 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,834 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,836 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,962 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.275:  48%|███▎   | 6336/13300 [00:24<00:26, 258.77samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:08,962 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,962 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:08,963 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,081 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  48%|███▎   | 6368/13300 [00:24<00:26, 261.22samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:09,082 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,082 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,084 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,206 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  48%|███▎   | 6400/13300 [00:25<00:26, 259.86samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:09,206 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,207 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,208 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,317 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  48%|███▍   | 6432/13300 [00:25<00:25, 267.90samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:09,317 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,317 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,319 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,438 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  49%|███▍   | 6464/13300 [00:25<00:25, 266.75samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:09,438 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,439 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,440 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,581 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  49%|███▍   | 6496/13300 [00:25<00:26, 252.26samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:09,581 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,581 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,583 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,696 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  49%|███▍   | 6528/13300 [00:25<00:26, 259.59samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:09,696 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,696 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,698 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,809 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.278:  49%|███▍   | 6560/13300 [00:25<00:25, 265.95samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:09,810 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,810 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,811 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,928 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  50%|███▍   | 6592/13300 [00:25<00:25, 266.79samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:09,929 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,929 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:09,929 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,065 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  50%|███▍   | 6624/13300 [00:25<00:26, 256.05samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:10,065 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,066 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,067 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,185 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.275:  50%|███▌   | 6656/13300 [00:26<00:25, 259.14samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:10,186 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,186 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,187 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,312 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  50%|███▌   | 6688/13300 [00:26<00:25, 257.25samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:10,312 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,312 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,314 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,448 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.275:  51%|███▌   | 6720/13300 [00:26<00:26, 250.23samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:10,448 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,448 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,450 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,561 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.274:  51%|███▌   | 6752/13300 [00:26<00:25, 259.06samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:10,561 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,561 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,563 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,681 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.275:  51%|███▌   | 6784/13300 [00:26<00:24, 261.33samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:10,681 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,682 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,683 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,801 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.275:  51%|███▌   | 6816/13300 [00:26<00:24, 262.86samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:10,801 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,802 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,803 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,931 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.274:  51%|███▌   | 6848/13300 [00:26<00:25, 257.48samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:10,932 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,932 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:10,933 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,053 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.273:  52%|███▌   | 6880/13300 [00:26<00:24, 258.90samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:11,054 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,054 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,055 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,169 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.273:  52%|███▋   | 6912/13300 [00:27<00:24, 263.93samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:11,169 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,170 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,171 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,289 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.274:  52%|███▋   | 6944/13300 [00:27<00:24, 264.46samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:11,290 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,290 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,292 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,471 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  52%|███▋   | 6976/13300 [00:27<00:27, 229.75samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:11,472 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,474 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,474 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,609 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  53%|███▋   | 7008/13300 [00:27<00:27, 230.63samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:11,609 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,609 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,611 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,730 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  53%|███▋   | 7040/13300 [00:27<00:26, 240.00samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:11,730 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,730 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,731 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,851 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.271:  53%|███▋   | 7072/13300 [00:27<00:25, 246.57samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:11,851 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,852 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,853 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,978 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  53%|███▋   | 7104/13300 [00:27<00:24, 247.97samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:11,979 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,979 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:11,980 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,099 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.274:  54%|███▊   | 7136/13300 [00:28<00:24, 252.81samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:12,100 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,100 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,101 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,221 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.274:  54%|███▊   | 7168/13300 [00:28<00:23, 255.54samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:12,222 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,222 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,223 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,343 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.274:  54%|███▊   | 7200/13300 [00:28<00:23, 257.89samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:12,343 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,343 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,345 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,467 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.273:  54%|███▊   | 7232/13300 [00:28<00:23, 257.86samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:12,467 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,468 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,469 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,600 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  55%|███▊   | 7264/13300 [00:28<00:23, 252.20samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:12,601 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,601 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,602 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,719 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  55%|███▊   | 7296/13300 [00:28<00:23, 257.26samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:12,719 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,719 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,721 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,838 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.273:  55%|███▊   | 7328/13300 [00:28<00:22, 260.49samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:12,838 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,839 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,840 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,951 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.271:  55%|███▊   | 7360/13300 [00:28<00:22, 266.85samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:12,952 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,952 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:12,953 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,073 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.271:  56%|███▉   | 7392/13300 [00:28<00:22, 265.26samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:13,074 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,074 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,076 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,197 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.271:  56%|███▉   | 7424/13300 [00:29<00:22, 263.31samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:13,198 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,198 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,199 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,310 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.270:  56%|███▉   | 7456/13300 [00:29<00:21, 269.16samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:13,310 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,310 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,312 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,447 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.271:  56%|███▉   | 7488/13300 [00:29<00:22, 257.26samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:13,448 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,448 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,449 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,571 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.270:  57%|███▉   | 7520/13300 [00:29<00:22, 257.75samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:13,571 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,571 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,573 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,691 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  57%|███▉   | 7552/13300 [00:29<00:22, 259.77samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:13,692 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,692 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,694 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,812 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  57%|███▉   | 7584/13300 [00:29<00:21, 261.49samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:13,812 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,813 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,813 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,938 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  57%|████   | 7616/13300 [00:29<00:21, 259.21samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:13,938 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,939 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:13,940 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,061 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.271:  58%|████   | 7648/13300 [00:29<00:21, 259.70samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:14,061 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,061 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,063 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,179 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.271:  58%|████   | 7680/13300 [00:30<00:21, 262.78samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:14,180 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,180 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,182 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,296 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.273:  58%|████   | 7712/13300 [00:30<00:21, 266.03samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:14,296 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,296 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,298 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,413 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.274:  58%|████   | 7744/13300 [00:30<00:20, 268.07samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:14,414 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,414 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,415 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,538 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.274:  58%|████   | 7776/13300 [00:30<00:20, 264.54samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:14,538 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,538 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,540 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,682 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  59%|████   | 7808/13300 [00:30<00:21, 249.98samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:14,683 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,683 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,684 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,813 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  59%|████▏  | 7840/13300 [00:30<00:21, 248.53samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:14,813 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,813 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,815 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,935 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  59%|████▏  | 7872/13300 [00:30<00:21, 252.54samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:14,935 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,935 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:14,937 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,055 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  59%|████▏  | 7904/13300 [00:30<00:21, 256.70samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:15,055 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,055 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,057 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,181 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.278:  60%|████▏  | 7936/13300 [00:31<00:21, 255.31samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:15,182 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,182 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,183 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,300 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  60%|████▏  | 7968/13300 [00:31<00:20, 259.63samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:15,300 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,300 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,302 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,422 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  60%|████▏  | 8000/13300 [00:31<00:20, 260.12samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:15,423 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,423 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,424 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,543 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  60%|████▏  | 8032/13300 [00:31<00:20, 261.43samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:15,544 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,544 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,545 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,665 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  61%|████▏  | 8064/13300 [00:31<00:20, 261.69samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:15,666 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,666 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,667 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,797 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.278:  61%|████▎  | 8096/13300 [00:31<00:20, 255.94samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:15,797 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,797 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,799 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,922 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  61%|████▎  | 8128/13300 [00:31<00:20, 255.81samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:15,922 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,923 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:15,924 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,061 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.278:  61%|████▎  | 8160/13300 [00:31<00:20, 247.67samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:16,061 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,061 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,063 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,182 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.278:  62%|████▎  | 8192/13300 [00:32<00:20, 252.34samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:16,182 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,183 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,184 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,292 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  62%|████▎  | 8224/13300 [00:32<00:19, 262.46samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:16,293 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,293 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,295 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,414 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  62%|████▎  | 8256/13300 [00:32<00:19, 262.41samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:16,415 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,415 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,417 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,539 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.275:  62%|████▎  | 8288/13300 [00:32<00:19, 260.58samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:16,540 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,540 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,541 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,658 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.274:  63%|████▍  | 8320/13300 [00:32<00:18, 263.48samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:16,658 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,658 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,660 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,783 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.273:  63%|████▍  | 8352/13300 [00:32<00:18, 261.02samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:16,783 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,783 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,785 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,934 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  63%|████▍  | 8384/13300 [00:32<00:20, 243.72samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:16,935 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,935 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:16,935 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:17,090 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  63%|████▍  | 8416/13300 [00:32<00:21, 230.87samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:17,091 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:17,091 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:17,093 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:17,293 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  64%|████▍  | 8448/13300 [00:33<00:23, 202.32samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:17,295 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:17,295 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:17,295 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:17,571 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.272:  64%|████▍  | 8480/13300 [00:33<00:29, 165.06samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:17,572 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:17,572 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:17,573 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:17,744 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.273:  64%|████▍  | 8512/13300 [00:33<00:28, 170.69samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:17,744 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:17,744 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:17,745 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:17,901 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  64%|████▍  | 8544/13300 [00:33<00:26, 179.36samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:17,901 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:17,902 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:17,903 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,025 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  64%|████▌  | 8576/13300 [00:33<00:23, 197.34samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:18,026 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,026 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,027 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,144 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  65%|████▌  | 8608/13300 [00:34<00:21, 214.56samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:18,145 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,145 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,146 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,266 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  65%|████▌  | 8640/13300 [00:34<00:20, 227.02samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:18,266 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,267 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,268 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,390 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  65%|████▌  | 8672/13300 [00:34<00:19, 235.42samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:18,391 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,393 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,397 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,533 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.278:  65%|████▌  | 8704/13300 [00:34<00:19, 231.89samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:18,533 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,534 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,535 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,649 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.278:  66%|████▌  | 8736/13300 [00:34<00:18, 243.37samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:18,650 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,650 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,651 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,774 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.278:  66%|████▌  | 8768/13300 [00:34<00:18, 247.20samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:18,774 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,775 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,776 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,893 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  66%|████▋  | 8800/13300 [00:34<00:17, 253.28samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:18,893 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,894 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:18,895 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,016 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  66%|████▋  | 8832/13300 [00:34<00:17, 255.29samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:19,016 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,017 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,018 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,132 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  67%|████▋  | 8864/13300 [00:35<00:16, 261.22samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:19,132 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,132 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,134 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,250 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.275:  67%|████▋  | 8896/13300 [00:35<00:16, 263.98samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:19,251 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,251 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,252 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,372 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.275:  67%|████▋  | 8928/13300 [00:35<00:16, 263.36samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:19,373 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,373 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,374 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,494 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.274:  67%|████▋  | 8960/13300 [00:35<00:16, 263.41samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:19,494 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,494 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,496 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,677 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.274:  68%|████▋  | 8992/13300 [00:35<00:18, 228.44samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:19,678 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,678 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,680 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,795 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.273:  68%|████▋  | 9024/13300 [00:35<00:17, 240.14samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:19,795 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,795 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,797 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,916 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.274:  68%|████▊  | 9056/13300 [00:35<00:17, 246.46samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:19,917 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,917 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:19,919 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,038 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.274:  68%|████▊  | 9088/13300 [00:35<00:16, 250.97samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:20,039 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,039 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,041 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,157 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.275:  69%|████▊  | 9120/13300 [00:36<00:16, 256.38samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:20,157 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,158 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,159 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,279 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  69%|████▊  | 9152/13300 [00:36<00:16, 257.99samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:20,280 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,280 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,281 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,393 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  69%|████▊  | 9184/13300 [00:36<00:15, 264.55samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:20,393 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,394 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,395 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,516 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  69%|████▊  | 9216/13300 [00:36<00:15, 263.05samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:20,517 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,517 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,518 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,636 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.275:  70%|████▊  | 9248/13300 [00:36<00:15, 264.44samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:20,636 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,636 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,638 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,754 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  70%|████▉  | 9280/13300 [00:36<00:15, 266.08samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:20,755 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,758 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,758 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,881 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  70%|████▉  | 9312/13300 [00:36<00:15, 261.84samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:20,881 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,882 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:20,883 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,021 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  70%|████▉  | 9344/13300 [00:36<00:15, 250.92samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:21,021 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,022 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,023 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,146 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  70%|████▉  | 9376/13300 [00:37<00:15, 252.11samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:21,147 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,147 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,149 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,263 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  71%|████▉  | 9408/13300 [00:37<00:15, 258.37samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:21,264 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,264 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,265 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,377 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  71%|████▉  | 9440/13300 [00:37<00:14, 264.50samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:21,378 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,378 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,380 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,501 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  71%|████▉  | 9472/13300 [00:37<00:14, 262.80samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:21,502 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,502 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,503 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,619 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  71%|█████  | 9504/13300 [00:37<00:14, 265.50samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:21,619 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,619 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,621 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,740 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.278:  72%|█████  | 9536/13300 [00:37<00:14, 264.96samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:21,740 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,741 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,742 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,861 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  72%|█████  | 9568/13300 [00:37<00:14, 265.13samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:21,861 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,862 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,863 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,984 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  72%|█████  | 9600/13300 [00:37<00:14, 263.03samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:21,985 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,985 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:21,987 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,105 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  72%|█████  | 9632/13300 [00:38<00:13, 263.68samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:22,105 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,106 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,107 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,225 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  73%|█████  | 9664/13300 [00:38<00:13, 264.78samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:22,225 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,227 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,227 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,382 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  73%|█████  | 9696/13300 [00:38<00:14, 243.05samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:22,382 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,385 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,385 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,546 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.275:  73%|█████  | 9728/13300 [00:38<00:15, 225.98samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:22,547 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,547 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,549 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,670 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.275:  73%|█████▏ | 9760/13300 [00:38<00:15, 235.18samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:22,670 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,670 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,672 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,783 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.275:  74%|█████▏ | 9792/13300 [00:38<00:14, 247.53samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:22,784 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,784 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,785 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,907 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.275:  74%|█████▏ | 9824/13300 [00:38<00:13, 250.35samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:22,908 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,908 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:22,910 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,026 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  74%|█████▏ | 9856/13300 [00:38<00:13, 255.82samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:23,027 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,027 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,029 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,164 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  74%|█████▏ | 9888/13300 [00:39<00:13, 248.12samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:23,165 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,168 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,171 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,280 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  75%|█████▏ | 9920/13300 [00:39<00:13, 256.22samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:23,280 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,280 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,282 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,399 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  75%|█████▏ | 9952/13300 [00:39<00:12, 259.71samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:23,399 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,400 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,401 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,697 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  75%|█████▎ | 9984/13300 [00:39<00:18, 182.10samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:23,698 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,698 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,699 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,832 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.278:  75%|████▌ | 10016/13300 [00:39<00:16, 195.69samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:23,833 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,833 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,836 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,997 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  76%|████▌ | 10048/13300 [00:39<00:16, 195.12samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:23,998 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:23,998 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,000 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,125 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.278:  76%|████▌ | 10080/13300 [00:40<00:15, 209.00samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:24,125 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,126 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,127 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,243 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.278:  76%|████▌ | 10112/13300 [00:40<00:14, 224.31samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:24,244 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,244 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,246 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,362 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  76%|████▌ | 10144/13300 [00:40<00:13, 236.12samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:24,363 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,363 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,364 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,488 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  77%|████▌ | 10176/13300 [00:40<00:12, 241.40samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:24,488 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,488 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,490 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,602 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  77%|████▌ | 10208/13300 [00:40<00:12, 251.89samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:24,602 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,603 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,604 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,723 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  77%|████▌ | 10240/13300 [00:40<00:11, 255.29samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:24,724 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,724 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,726 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,846 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.278:  77%|████▋ | 10272/13300 [00:40<00:11, 256.68samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:24,847 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,847 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,849 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,966 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.278:  77%|████▋ | 10304/13300 [00:40<00:11, 260.04samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:24,966 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,966 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:24,967 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,100 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.278:  78%|████▋ | 10336/13300 [00:41<00:11, 253.06samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:25,101 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,101 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,102 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,215 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  78%|████▋ | 10368/13300 [00:41<00:11, 260.43samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:25,215 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,215 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,217 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,345 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  78%|████▋ | 10400/13300 [00:41<00:11, 255.81samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:25,345 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,347 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,348 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,485 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  78%|████▋ | 10432/13300 [00:41<00:11, 246.71samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:25,486 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,486 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,488 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,619 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.278:  79%|████▋ | 10464/13300 [00:41<00:11, 244.39samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:25,620 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,620 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,622 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,738 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  79%|████▋ | 10496/13300 [00:41<00:11, 251.42samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:25,738 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,739 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,740 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,849 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  79%|████▋ | 10528/13300 [00:41<00:10, 261.18samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:25,850 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,850 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,852 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,967 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  79%|████▊ | 10560/13300 [00:41<00:10, 264.64samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:25,967 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,967 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:25,969 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,092 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  80%|████▊ | 10592/13300 [00:41<00:10, 261.61samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:26,093 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,093 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,095 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,214 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  80%|████▊ | 10624/13300 [00:42<00:10, 261.78samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:26,215 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,215 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,216 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,329 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  80%|████▊ | 10656/13300 [00:42<00:09, 266.48samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:26,330 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,330 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,332 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,445 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  80%|████▊ | 10688/13300 [00:42<00:09, 269.56samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:26,445 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,446 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,447 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,582 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  81%|████▊ | 10720/13300 [00:42<00:10, 257.67samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:26,582 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,582 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,584 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,721 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  81%|████▊ | 10752/13300 [00:42<00:10, 248.52samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:26,722 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,722 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,724 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,849 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  81%|████▊ | 10784/13300 [00:42<00:10, 249.32samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:26,849 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,849 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:26,851 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,030 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.278:  81%|████▉ | 10816/13300 [00:42<00:11, 221.62samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:27,031 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,031 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,033 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,157 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  82%|████▉ | 10848/13300 [00:43<00:10, 230.23samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:27,157 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,157 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,159 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,275 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  82%|████▉ | 10880/13300 [00:43<00:10, 240.96samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:27,276 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,276 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,278 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,389 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.276:  82%|████▉ | 10912/13300 [00:43<00:09, 251.65samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:27,390 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,390 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,391 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,511 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.277:  82%|████▉ | 10944/13300 [00:43<00:09, 254.60samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:27,512 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,512 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,514 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,631 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  83%|████▉ | 10976/13300 [00:43<00:09, 258.05samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:27,632 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,632 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,634 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,760 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  83%|████▉ | 11008/13300 [00:43<00:08, 255.25samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:27,761 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,761 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,762 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,890 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  83%|████▉ | 11040/13300 [00:43<00:08, 252.66samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:27,890 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,890 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:27,892 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,014 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.278:  83%|████▉ | 11072/13300 [00:43<00:08, 254.19samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:28,014 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,014 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,016 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,131 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  83%|█████ | 11104/13300 [00:44<00:08, 259.67samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:28,131 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,131 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,134 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,254 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.280:  84%|█████ | 11136/13300 [00:44<00:08, 259.98samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:28,254 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,254 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,256 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,376 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  84%|█████ | 11168/13300 [00:44<00:08, 260.42samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:28,376 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,377 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,378 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,496 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  84%|█████ | 11200/13300 [00:44<00:08, 262.00samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:28,497 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,497 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,498 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,625 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  84%|█████ | 11232/13300 [00:44<00:08, 257.73samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:28,626 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,626 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,628 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,745 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  85%|█████ | 11264/13300 [00:44<00:07, 260.42samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:28,746 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,746 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,748 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,868 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  85%|█████ | 11296/13300 [00:44<00:07, 260.56samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:28,868 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,868 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:28,870 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,014 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  85%|█████ | 11328/13300 [00:44<00:08, 246.37samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:29,015 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,015 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,016 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,140 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  85%|█████ | 11360/13300 [00:45<00:07, 248.88samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:29,140 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,140 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,142 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,262 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  86%|█████▏| 11392/13300 [00:45<00:07, 252.48samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:29,263 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,263 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,264 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,381 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  86%|█████▏| 11424/13300 [00:45<00:07, 257.31samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:29,381 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,382 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,383 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,516 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  86%|█████▏| 11456/13300 [00:45<00:07, 250.74samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:29,517 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,517 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,519 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,637 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  86%|█████▏| 11488/13300 [00:45<00:07, 255.14samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:29,637 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,637 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,639 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,761 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  87%|█████▏| 11520/13300 [00:45<00:06, 255.94samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:29,761 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,761 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,763 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,886 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.280:  87%|█████▏| 11552/13300 [00:45<00:06, 255.73samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:29,886 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,887 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:29,889 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,013 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.280:  87%|█████▏| 11584/13300 [00:45<00:06, 254.73samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:30,013 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,014 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,015 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,135 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.280:  87%|█████▏| 11616/13300 [00:46<00:06, 256.97samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:30,135 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,135 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,137 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,251 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.280:  88%|█████▎| 11648/13300 [00:46<00:06, 261.85samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:30,252 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,252 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,254 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,372 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.280:  88%|█████▎| 11680/13300 [00:46<00:06, 262.80samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:30,373 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,373 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,374 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,498 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.279:  88%|█████▎| 11712/13300 [00:46<00:06, 260.35samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:30,498 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,499 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,500 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,663 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.281:  88%|█████▎| 11744/13300 [00:46<00:06, 236.07samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:30,663 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,664 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,665 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,780 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.281:  89%|█████▎| 11776/13300 [00:46<00:06, 246.06samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:30,781 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,781 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,782 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,901 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.282:  89%|█████▎| 11808/13300 [00:46<00:05, 251.30samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:30,902 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,902 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:30,903 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,022 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.282:  89%|█████▎| 11840/13300 [00:46<00:05, 255.01samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:31,023 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,023 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,024 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,143 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.282:  89%|█████▎| 11872/13300 [00:47<00:05, 258.22samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:31,143 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,143 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,145 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,274 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.282:  90%|█████▎| 11904/13300 [00:47<00:05, 253.56samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:31,275 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,275 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,276 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,394 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.282:  90%|█████▍| 11936/13300 [00:47<00:05, 257.24samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:31,395 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,395 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,397 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,516 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.283:  90%|█████▍| 11968/13300 [00:47<00:05, 259.15samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:31,516 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,516 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,518 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,628 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.283:  90%|█████▍| 12000/13300 [00:47<00:04, 266.44samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:31,628 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,628 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,630 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,744 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.282:  90%|█████▍| 12032/13300 [00:47<00:04, 268.53samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:31,745 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,746 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,747 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,864 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.282:  91%|█████▍| 12064/13300 [00:47<00:04, 268.41samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:31,865 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,865 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,866 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,976 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.282:  91%|█████▍| 12096/13300 [00:47<00:04, 273.58samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:31,976 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,976 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:31,978 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,093 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.283:  91%|█████▍| 12128/13300 [00:47<00:04, 273.25samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:32,094 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,094 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,096 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,212 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.283:  91%|█████▍| 12160/13300 [00:48<00:04, 272.21samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:32,212 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,213 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,214 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,334 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.283:  92%|█████▌| 12192/13300 [00:48<00:04, 269.42samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:32,334 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,334 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,336 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,455 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.283:  92%|█████▌| 12224/13300 [00:48<00:04, 267.32samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:32,456 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,456 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,457 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,590 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.284:  92%|█████▌| 12256/13300 [00:48<00:04, 258.01samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:32,590 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,591 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,592 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,710 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.286:  92%|█████▌| 12288/13300 [00:48<00:03, 260.13samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:32,711 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,711 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,712 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,827 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.286:  93%|█████▌| 12320/13300 [00:48<00:03, 263.89samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:32,828 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,828 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,830 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,966 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.287:  93%|█████▌| 12352/13300 [00:48<00:03, 253.21samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:32,966 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,966 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:32,968 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,099 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.288:  93%|█████▌| 12384/13300 [00:49<00:03, 249.09samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:33,099 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,100 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,102 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,226 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.287:  93%|█████▌| 12416/13300 [00:49<00:03, 250.17samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:33,226 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,226 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,228 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,345 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.287:  94%|█████▌| 12448/13300 [00:49<00:03, 255.46samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:33,345 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,345 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,347 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,466 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.288:  94%|█████▋| 12480/13300 [00:49<00:03, 258.11samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:33,466 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,466 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,468 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,577 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.288:  94%|█████▋| 12512/13300 [00:49<00:02, 266.32samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:33,577 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,577 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,579 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,699 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.289:  94%|█████▋| 12544/13300 [00:49<00:02, 265.10samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:33,699 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,700 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,701 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,815 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.289:  95%|█████▋| 12576/13300 [00:49<00:02, 267.99samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:33,816 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,816 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,817 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,931 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.290:  95%|█████▋| 12608/13300 [00:49<00:02, 270.73samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:33,931 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,931 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:33,933 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,052 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.290:  95%|█████▋| 12640/13300 [00:49<00:02, 268.61samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:34,052 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,054 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,055 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,177 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.290:  95%|█████▋| 12672/13300 [00:50<00:02, 264.92samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:34,177 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,177 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,179 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,301 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.290:  96%|█████▋| 12704/13300 [00:50<00:02, 262.90samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:34,301 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,301 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,303 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,424 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.290:  96%|█████▋| 12736/13300 [00:50<00:02, 261.85samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:34,424 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,425 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,426 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,555 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.290:  96%|█████▊| 12768/13300 [00:50<00:02, 256.25samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:34,555 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,556 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,558 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,693 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.290:  96%|█████▊| 12800/13300 [00:50<00:02, 248.34samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:34,694 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,694 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,695 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,821 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.290:  96%|█████▊| 12832/13300 [00:50<00:01, 248.64samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:34,822 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,822 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,823 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,954 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.290:  97%|█████▊| 12864/13300 [00:50<00:01, 246.43samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:34,955 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,955 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:34,956 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,092 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.290:  97%|█████▊| 12896/13300 [00:50<00:01, 241.87samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:35,092 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,093 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,094 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,220 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.290:  97%|█████▊| 12928/13300 [00:51<00:01, 244.41samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:35,220 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,220 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,222 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,343 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.290:  97%|█████▊| 12960/13300 [00:51<00:01, 249.07samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:35,343 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,343 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,345 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,484 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.290:  98%|█████▊| 12992/13300 [00:51<00:01, 241.66samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:35,485 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,485 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,486 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,618 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.290:  98%|█████▉| 13024/13300 [00:51<00:01, 240.83samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:35,618 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,619 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,621 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,732 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.291:  98%|█████▉| 13056/13300 [00:51<00:00, 251.67samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:35,732 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,733 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,734 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,849 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.291:  98%|█████▉| 13088/13300 [00:51<00:00, 257.66samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:35,850 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,850 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,851 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,972 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.292:  99%|█████▉| 13120/13300 [00:51<00:00, 258.62samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:35,972 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,972 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:35,974 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,104 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.292:  99%|█████▉| 13152/13300 [00:52<00:00, 253.19samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:36,105 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,105 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,106 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,227 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.292:  99%|█████▉| 13184/13300 [00:52<00:00, 255.27samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:36,228 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,228 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,230 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,349 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.292:  99%|█████▉| 13216/13300 [00:52<00:00, 257.60samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:36,349 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,350 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,351 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,479 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.292: 100%|█████▉| 13248/13300 [00:52<00:00, 254.24samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:36,479 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,479 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,481 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,610 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.292: 100%|█████▉| 13280/13300 [00:52<00:00, 251.05samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:36,610 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,707 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 22/22, loss=1.293: 100%|██████| 13300/13300 [00:52<00:00, 252.80samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-05 22:00:36,708 kur.model.executor:464]\u001b[0m Training loss: 1.293\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:00:36,708 kur.model.executor:471]\u001b[0m Saving best historical training weights: t2_dp/best.w.kur\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,708 kur.model.executor:430]\u001b[0m Saving weights to: t2_dp/best.w.kur\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,708 kur.model.model:213]\u001b[0m Saving model weights to: t2_dp/best.w.kur\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,729 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,730 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,730 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,731 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,732 kur.loggers.binary_logger:144]\u001b[0m Writing logger summary.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,733 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: training_loss_out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,734 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: training_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,734 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: training_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,735 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: training_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,735 kur.loggers.binary_logger:144]\u001b[0m Writing logger summary.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,737 kur.model.executor:101]\u001b[0m Recompiling the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,737 kur.backend.keras_backend:543]\u001b[0m Reusing an existing model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,737 kur.backend.keras_backend:560]\u001b[0m Assembling a testing function from the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:36,741 kur.backend.keras_backend:509]\u001b[0m Adding additional inputs: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,663 kur.backend.keras_backend:599]\u001b[0m Additional inputs for log functions: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,663 kur.backend.keras_backend:616]\u001b[0m Expected input shapes: in_seq=(None, 30, 30), out_char=(None, None)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,664 kur.backend.keras_backend:634]\u001b[0m Compiled model: {'func': <keras.backend.theano_backend.Function object at 0x11ddfde80>, 'shapes': {'input': [(None, 30, 30), (None, None)]}, 'names': {'output': ['..activation.0', 'out_char'], 'input': ['in_seq', 'out_char']}}\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,664 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,664 kur.providers.batch_provider:102]\u001b[0m Maximum number of batches set to: 1\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:00:39,702 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,702 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,703 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=N/A:   0%|                        | 0/831 [00:00<?, ?samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:39,725 kur.providers.shuffle_provider:184]\u001b[0m Shuffling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,738 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,738 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,738 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,740 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,769 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,771 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,800 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,802 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=1.762:  12%|█▍          | 96/831 [00:00<00:00, 906.93samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:39,831 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,832 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,859 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,860 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,891 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,891 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,921 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,923 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=1.618:  27%|██▉        | 224/831 [00:00<00:00, 952.86samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:39,949 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,950 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,979 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:39,980 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,016 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,017 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,045 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,047 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=1.590:  42%|████▋      | 352/831 [00:00<00:00, 973.40samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:40,074 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,075 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,102 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,103 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,137 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,138 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,167 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,168 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=1.599:  58%|██████▎    | 480/831 [00:00<00:00, 993.26samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:40,197 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,198 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,227 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,228 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,259 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,260 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,290 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,291 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=1.627:  73%|████████   | 608/831 [00:00<00:00, 989.10samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:40,327 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,329 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,354 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,355 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,386 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,387 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,414 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,415 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=1.680:  89%|████████▊ | 736/831 [00:00<00:00, 1022.26samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:00:40,443 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,444 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,472 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,472 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=1.664: 100%|██████████| 831/831 [00:00<00:00, 1025.84samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-05 22:00:40,535 kur.model.executor:197]\u001b[0m Validation loss: 1.664\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,535 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: validation_loss_out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,536 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: validation_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,536 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: validation_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,537 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: validation_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,538 kur.loggers.binary_logger:144]\u001b[0m Writing logger summary.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,539 kur.model.hooks.plot_hook:73]\u001b[0m Plotting hook received training message.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,539 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: batch_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,540 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: batch_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,540 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: batch_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,540 kur.model.hooks.plot_hook:107]\u001b[0m Using per-batch training statistics for plotting.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,541 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,541 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:40,542 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:41,328 kur.model.hooks.plot_hook:130]\u001b[0m Loss-per-batch plot saved to: t2_dp/loss.png\u001b[0m\n",
      "Completed 22 epochs.\n",
      "\u001b[1;37m[INFO 2017-03-05 22:00:41,328 kur.model.executor:235]\u001b[0m Saving most recent weights: t2_dp/last.w.kur\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:41,329 kur.model.model:213]\u001b[0m Saving model weights to: t2_dp/last.w.kur\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:41,379 kur.model.hooks.plot_hook:73]\u001b[0m Plotting hook received training message.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:41,379 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: batch_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:41,380 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: batch_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:41,381 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: batch_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:41,381 kur.model.hooks.plot_hook:107]\u001b[0m Using per-batch training statistics for plotting.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:41,381 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:41,385 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:41,385 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:00:42,265 kur.model.hooks.plot_hook:130]\u001b[0m Loss-per-batch plot saved to: t2_dp/loss.png\u001b[0m\n",
      "CPU times: user 1.95 s, sys: 1.8 s, total: 3.75 s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kur -vv train char_rrn_demo_dp_fluid.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAG0CAYAAAA7Go31AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzs3Xl4FFW6P/DvyQpZIYQkEIhA2GQRIeOCLDaoM965uI24\noaC44PW6AM6Ioz8Rt9FBvYDjMooyg9vgICgobsMwtiCooxEVYUBBloSdREhCgCx9fn9UVVLpdHdV\nd1V3dXe+n+fpJ+nq6qqTTtL19jnveY+QUoKIiIgoHiU43QAiIiKicGGgQ0RERHGLgQ4RERHFLQY6\nREREFLcY6BAREVHcYqBDREREcYuBDhEREcUtBjpEREQUtxjoEBERUdxioENERERxi4EOERERxa0k\npxsQaUIIAaArgGqn20JERERByQSwRwaxUGebC3SgBDnlTjeCiIiIQtINwG6zO7fFQEfryekG9uoQ\nERHFikwoHRVBXbvbYqCjqZZSVjndCCIiIjKmZJ4Ej8nIREREFLcY6BAREVHcYqBDREREcast5+gQ\nEVGQSktLMwF0AT8ok/0aAewsKSmps/OgIoip6HFBCJEF4AiAbCYjExGZU1pamgDg3sTExElCiGQA\noWWGEvknPR7PQY/Hc0lJSUmr6eOhXr/Zo0NERGbcm5ycfEtBQUFdenp6rRCibX1KprDzeDyivLy8\nqLa29qHS0tKbSkpKPHYcl4EOEREFVFpampWYmDipoKCgLi8vr8Lp9lD8ys/Pr9q5c+fZjY2NnQAc\ntOOYHGMlIiIjBUKI5PT09FqnG0LxLSUlpU4IkQSgo13HZKBDRERGEqAsFcjhKgorXVFA2+KTqAl0\nhBD3CCGkEGKewX6XCSE2CyGOCyE2CCF+Hak2BiLc7kThdruE232V+jXR6TYRERG1dVER6AghTgMw\nBcB3BvsNB7AIwAIAQwG8DWCZEGJQ2BsZqF1u928A7ADwMYC/qV93qNuJiCgO5ebmDnn88cc7m91/\nyZIlWUKIktraWs5YiyDHAx0hRAaA1wHcBOBng92nAfhQSvmElPI/Usr7AXwN4LYwN9MvNZhZAqDQ\n66FCAEsY7BARNWuQEisqKjJf2LMnZ0VFRWZDGEucCCFKAt3uvPPOrlaO/91332383//930Nm9x83\nblz1zp07v01LSwvrECADqpaiYdbVswDek1L+Uwhxn8G+wwHM8dr2EYCL/T1BCJEKIFW3KTOkVvo6\ntjI89ZR21/thABLAPOF2L5cuV6Nd5yUiikUv79vX4e5t24r219cna9vyk5PrZxcX77q2oOCw3efb\nuXPnt03nfvnlnMcff7zrxo0bv9e2ZWdnt5q+7PF40NjYiOTkZO+HWunatWtDMO1p166dLCoqCuo5\nZJ2jPTpCiCsBDANwj8mnFADY77Vtv7rdn3ugFBjSbuVBNjOQUQC6wX/hLAGgu7ofEVGb9fK+fR2u\n27y5WB/kAMD++vrk6zZvLn55374Odp+zqKioQbtlZ2c3+tjm0Xo/3nrrraz+/fsPSElJGbZ69er0\n9evXtxs7dmzvnJycIenp6UOHDBnSf8WKFS0+KOuHrmpra4UQouTpp5/uNGbMmN7t27cf2qNHj0GL\nFy/O0vb37ml5/PHHO+fm5g5ZtGhRdo8ePQalp6cPHTNmTO89e/Y0dUIcP35cXH311UUZGRlDO3bs\nOGTq1Kldx40b12vcuHG9Qn1dGhoaMHXq1K55eXmnpKSkDBs4cODJy5cvb/rZamtrxYQJE07Kzc0d\nkpqaOqywsHDwrFmz8gElELz99tsLCwoKTklJSRmWn59/ypQpU7qF2pZIcCzQEUJ0h9Ibco2U8riV\nQ0HpOfHnMQDZupudv5AuNu9HRBQTPFKiqqEhwcytsr4+Yca2bUWBjjdj27aiyvp6U8fzhGG4a+bM\nmYVPPPFE2TfffLNxyJAhx6urqxPGjRt3+KOPPtry2WefbRo+fHjN5Zdf3nvnzp0Bu3r++Mc/dp04\ncWLFl19+uemss86quvHGG3tVVlb6vdZWV1cnPvfcc3mvvfbaTx988MGWn376qd20adOaUiHuvvvu\nLh988EHH+fPn/7Rq1aote/fuTf7kk0+y/B3P5M9asHDhwrzZs2eXffHFF5vOPPPMmssvv7zPli1b\nUgDggQceKFi9enXWokWLtm7YsOH7l156aXv37t3rAOD555/PeeWVVzo/99xzOzZu3Pj9okWLtg0Y\nMOCYlfaEm5NDVyUA8gCU6qaTJQIYLYS4DUCqlNJ7uGcfgHyvbXlo3cvTREp5AsAJ7b7uXHbYa/N+\nREQxoaaxMSH700+H2nW8A/X1yZ3WrjV1vCMjR67PSkqypWqu5qGHHtp9wQUXVGv3R48eXTt69Oim\nukHPP/98+fvvv99h6dKl2XfeeaffvJxrrrnm4PXXX/8zAMydO3f3okWLOq9bty593Lhx1b72r6ur\nEwsXLtxRXFxcDwCTJ08++OKLL+Zpj//1r3/N+/3vf797woQJRwDg1Vdf3dW9e/dsKz/rn//854Lp\n06fvnTx58s8AsGDBgrI1a9ZkPvnkk3kvvvhieVlZWUpxcfGx88477ygA9O3bt2ntqV27dqXk5+fX\nXXjhhVVJSUno06dP3dixY49aaU+4OTl0tQrAYACn6m5fQUlMPtVHkAMAnwE4x2vbeep2J6yBMhTm\n7+OFBFCm7kdERFFqxIgRLS7WFRUViddff333nj17DszMzDw1LS1t6O7du1N37dqVEug4Q4YMaerd\nyM/Pb0xOTpb79u3z26mQnZ3dqAU5ANClS5f6ysrKJAAoKytLqq6uThw+fHhT21JTU+XJJ58ccuHG\n3bt3Jx05ciRx9OjRNfrtp512Ws0PP/zQDgBuuummQ19//XVGz549B15//fXd9cNakyZNqjx8+HBS\nUVHR4AkTJpz0+uuvZzc0RHfakWM9OlLKagDf67cJIY4CqJBSfq/efwXAbimllsPzFIDVQojfAngP\nwJUAfgFlanrESZerUbjdU6HMupJomaujBT/TmIhMRPEmIzHRc2TkyPVm9v2osjLj8k2b+hjtt3jA\ngB9/lZNTY7RfRmKirb05AJCVldXimFOmTOleWlqa8fDDD5f369fvRFpamufCCy/sXVdXF3BYICUl\npcUHXyEEPB7/zU1KSvLeX3o8HgEA2qLbCQkt+ySklCEPTWht8S7+KKVsGvEYO3bs0e3bt29YunRp\n9qpVqzKvvvrq3uecc87h5cuXbx8wYEDdtm3bvl+2bFnWypUrs+64444eTz311PF169ZtSUqKhvlN\nrTk+vdxAEXT5LVLKdQCughLYfAtgPICLtcDICdLlektth3dXZjmA8erjRERxJUEIZCUleczcLunc\nuSo/Obk+0PHyk5PrLuncucrM8RLsTUHw6auvvsqYOHHiwYkTJx4+/fTTjxUUFDTs27cvYG+O3YqK\nihoyMzMb161bl65tO3HihNi8eXP7UI/ZvXv3hg4dOjR88sknLRKrS0tLM/r27duUL5ubm9t48803\nVy5evHjn888/v/2dd97Jqa6uTgCAzMxMz8SJEw+/8soru1asWPHDl19+mfHtt9+2C7VN4RZV4ZeU\n0hXovrrtTQBvRqhJpkiX6y3hdh+GMhwHAGMArGFPDhERkCQEZhcX77pu8+ZiX48LALOLi8uSIhDA\nmNWjR4/jy5Ytyxk3blxVQ0MD7r333kLvnpVImDx58oEnn3yya8+ePev69et3Yvbs2fnHjh1LMLMc\nxxdffNE+NTW1ab+kpCScfvrpx2655Zb9c+fO7dKjR48TQ4cOPfb000933rFjR7t33313KwDcd999\n+T169Kg77bTTagFg6dKlHbp06VKXmZnpmTNnTm5SUpI866yzjqalpXleeeWVnLS0NE+vXr3q/LXD\naVEV6MS4pj8m6XK5HWwHEVHUUevkbPNRR6dudnFxWTjq6Fjx7LPPlk2ePLmHy+Xqn5OT03DnnXfu\nPXz4cMSvmbNnz9578ODBpBtvvLFXcnKyZ9KkSQdPP/30an0A48/YsWNP1t9v3769p7a2dv1DDz20\nr6amJmHGjBlFhw8fTurTp8+xxYsX/6glHaenp3sef/zxLmVlZalJSUlyyJAhR5ctW7YVADp06NA4\nd+7cgnvvvbedlBL9+vU7tmTJkh87duxo+3CiXYQMY1XKaCSEyIJSTydbSlll23Hd7jEA/gUA0uWK\nno8lREQWlZaW9k9KSvqwT58+NWlpaVbKgaBBSnxYWZm5+8SJ5MLU1Przc3Kqo6knJ9o1NDTgpJNO\nGnzNNdccnD179j6n22O32tradj/++GNGQ0PD+SUlJZv1j4V6/WaPDhERRUySEBjXqZPPqdbU2saN\nG1NXrlyZcc4559TU1tYmPPHEE/mHDh1KnjRpktGSSaRioENERBSlhBBy4cKFne+7774iIYTs27fv\nsRUrVmwZOHDgCeNnE8BAx07seyUiIlsNGDCg7ptvvtlsvCf5E+3Ty4mIiIhCxkCHiIiI4hYDHSIi\nIopbDHTswxwdIiKiKMNAh4iIiOIWAx0iIiKKWwx0iIiIDFx00UU9zz///F7a/ZKSkn5TpkzpFug5\n+fn5pzz66KOdrZ7bruO0VQx07MMcHSIiA7JBomJFReaeF/bkVKyoyJQN4VuGaOzYsb1HjRrVx9dj\nH374YYYQouSLL74IaSXw999/f+vs2bP3WGthS3PmzMnt2LHjEO/t69ev33T77bdX2Hkub8uWLcsU\nQpQcOXIk7uICFgwkIqKI2Pfyvg7b7t5WVL+/eVHP5Pzk+uLZxbsKrrV/Uc/Jkycfuu6664q3bt2a\n3Lt373r9YwsWLMgdOHBg7RlnnHEslGPn5+c32tNKY127dm2I1LniUdxFbg5qChqF2z1NuN3JgXYm\nImpL9r28r8Pm6zYX64McAKjfX5+8+brNxfte3tfB7nNeeeWVhzt27Ngwf/78XP32I0eOJLz//vsd\nJ02adAgATpw4IS677LIehYWFg9u1azesR48eg/7whz/kBTq299DVrl27ksaMGdO7Xbt2w7p16zZ4\n/vz5Hb2fM3PmzPw+ffoMbN++/dCCgoJTJk2aVFRVVZUAKD0qv/3tb086fPhwkhCiRAhRMmPGjC5A\n66GrLVu2pIwdO7Z3+/bth2ZmZp46bty4Xnv27Gm6Bt1xxx1dBw0adPLTTz/dqWvXroMzMzNPvfDC\nC3ta6a1pbGzE9OnTu+bl5Z2SkpIybMCAASe//fbbWdrjx44dE1dffXVR586dT0lNTR1WWFg4+L77\n7ssHAI/Hg6lTp3bt0qXL4JSUlGF5eXmn3HDDDd1DbUuwGOjYQLjdswG8p9s0F8AxdTsRUdyRHomG\nqoYEM7f6yvqEbTO2FQU63rYZ24rqK+tNHU96zA13JScnY/z48RVvvPFGJ4/H07R94cKFHT0eD264\n4YZKQLmId+/evW7RokXbvvnmm+/vuuuuPY888kjhyy+/bDr4uuqqq3ru378/+YMPPtj8+uuvb3vu\nuefyjxw50mLUJCkpSc6dO3fXN998s/GFF17Yvnr16qw77rijEADOP//8mlmzZpVnZ2c37ty589ud\nO3d+e9999+33Pk9jYyMuuOCC3jU1NYkrV67csmzZsh+3bdvWbvz48b30+23fvr3dBx98kL18+fIf\n33jjja1r167NeuCBBwrM/jzeHnjggfyXXnop79FHHy378ssvN44cObL6iiuu6L1p06YUAHj44Yfz\n3W539muvvfbThg0bvl+wYMH2oqKiOgB46aWXOv7lL3/Je+aZZ3Zu3Ljx+zfeeGPboEGDQupJCwWH\nrixSg5kZPh5KBDBDuN2QLtfdEW4WEVFYNdY0Jnya/elQu45Xf6A+eW2ntaaON/LIyPVJWUke4z2B\nm2+++dALL7yQ//7772eOGzeuGgBeffXV3PPPP//nTp06NQJAWlqanDNnTlO+Tf/+/SvXrl2b8eab\nb+Zce+21hkNqpaWl7datW5f16aefbhoxYsQxAHjxxRd3nH766QP1+82aNeuA9n2/fv3q9u/fv/ve\ne+8tAlDWrl07mZWV1SiEkEVFRX6HqpYuXZr1008/tfvxxx839OzZsx4AFi5cuP3MM88csHbt2vba\n+QFg0aJFO7Kzsz0AcMkll1SsXr0608xr5stzzz1XMHXq1L033njjzwAwf/788k8//TTziSeeyP/r\nX/9atmvXrpQePXocP++882oSEhLQt2/fOu25u3btSsnLy6u/8MILq5KTk9GnT5+6sWPHHg21LcFi\nj44F6vDU7wx2+x2HsYiInDF06NDjQ4cOPbpgwYJOAPD999+nlpaWZtxwww0tknv/8Ic/5A0cOPDk\njh07DklLSxv65ptv5u7evTvFzDk2bNjQLjk5WQ4fPrwpyDjttNOOp6entwjG3nrrrawzzzyzb15e\n3ilpaWlD77jjjp4VFRVJx44dMz2ZZdOmTe0LCwvrtCAHAM4444xjaWlpng0bNjQlVnfr1u2EFuQA\nQJcuXeorKipCuhbt378/sbKyMmn06NE1+u2nnXZazQ8//NAOAG666aZDGzZsSO/Vq9egyZMnd1+2\nbFlTUDVp0qSfa2pqEouKigZfddVVJ7366qsdGhoil3bEHh1rboNxsJig7jc3/M0hIoqMxIxEz8gj\nI9eb2bfyo8qMTZdv8jn7SW/A4gE/5vwqp8Zov8SMRFO9OZpJkyYdvOeee4oqKyt3vfDCC7ndu3c/\n8etf/7pae/y5557LeeSRRwoffPDB8hEjRtRkZ2d7Hn744YKNGzemmTm+lFII0TpWkbJ5iG3jxo2p\nV155Ze9rr732wKOPPro7Nze3YeXKlZm/+93vTqqrqxPt27c3NR4npYSvcwFosT05OVl6P6YfvguG\n9nN4n1f/c5999tm127dv37B06dKsVatWZU2cOLH47LPPrlqxYsVPffv2rdu2bduGZcuWZa9cuTJz\n+vTpJ82bNy//888/35KcHP5+APboWDPS5v2IiGKCSBBIykrymLl1vqRzVXJ+cn2g4yXnJ9d1vqRz\nlZnjiYTgqnlMnjz554SEBCxYsCDnzTff7DRhwoRDCQnNl7+1a9dmlJSU1MyYMePgiBEjjg0aNOjE\n9u3b25k9/imnnHKsrq5OfPbZZ009Kl999VW72trappOsW7cuTQiBF198sXzs2LFHTznllBPePUYp\nKSmysbEx4A83cODAY+Xl5Sk7duxoihC++OKL9rW1tQmDBw8OS95LQUFBY05OTsMnn3ySod/+1Vdf\npfft2/e4dr9Tp06NU6ZM+fnvf//7zhdffHH7e++917GysjIBADIyMuQ111xz+OWXXy778MMPt3z9\n9dcZX3/9dUhT+4PFHh1rDD95BLkfEVHcEUkCxbOLd22+bnOx7x2A4tnFZSIpPOXIsrOzPePGjat8\n5JFHuh09ejTx5ptvbjFs1adPnxPvvPNOzttvv51VXFx8Yv78+bn/+c9/2p900kknzBy/pKTk+PDh\nw6tuvvnmHs8+++xOIQSmTZtWlJqa2tSr0r9//xN1dXXi0Ucf7XzxxRcfWbVqVeZrr73WYjZYcXHx\niZqamsQVK1ZklpSUHMvMzGzMyMho0TNz6aWXVvXq1ev4FVdc0XPOnDllx48fT7j99tuLhg8fXn3W\nWWdZDnS+/PLL9mlpaU1dP4mJiTjjjDOO3Xrrrfvmzp3bpVevXnUlJSW1zz33XOetW7e2X7JkyTYA\nuP/++/O7d+9ed/rpp9cKIbBkyZKOeXl59R06dPDMmzevkxACI0aMOJqWluZZuHBhp3bt2nmKi4vr\n/LfEPuzRseZVm/cjIopLBdcWHO6/sP82756d5Pzkuv5/7b8tHHV09G666aZDVVVViSNHjjyiz28B\ngBkzZhw499xzD0+aNKnXqFGjTq6qqkq8+uqrDwVz/EWLFu3o1KlT/S9/+cv+V111VfGUKVMOZGdn\nNyWijBo1qnbmzJnlc+fO7VJSUjJwyZIlHWfOnLlbf4xf/epXNVdcccWhiRMn9urateuQhx56qNUs\nqcTERLz77rtb09PTPeeee27/iy++uE+vXr1OLFmy5KdgXxNfzjvvvP4jRowYoN1Gjx59MgDMmjVr\n/4033njg97//ffdf/OIXA9esWZP5xhtvbB0wYEAdAGRkZHiefPLJLsOHDx8wYsSIk/fs2ZO8bNmy\nHxMSEtChQ4fGBQsWdB4zZkz/0047beDatWszFy9evDU3NzcitYiEfgyxLRBCZAE4AiBbSlll6Vhu\ndyKAnwEEymSvApAjXa6IFZciIrJTaWlp/6SkpA/79OlTk5aWdtz4Gf7JBonKDyszT+w+kZxamFqf\nc35Odbh6cij21NbWtvvxxx8zGhoazi8pKdmsfyzU6zeHriyQLlejcLuvA7AEvpeAkAAmM8ghIlKI\nJIFO4zpVG+9JZA8OXVkkXa63AIwH4L0OSRmA8erjRERE5AAGOjZQg5mbdZuuAdCTQQ4REZGzGOjY\nR5/s9G8OVxERETmPgY59mE1HRPHKA0BKKfk+R2GlmyAVWnVDHxjo2IdvAEQUr/ZJKeuPHj1qqlIw\nUajq6upSpJQNUGY024KzroiIKKCSkpKq0tLSV/bt23cLgE7p6em1Qoi2VZuEws7j8Yj9+/dneTye\nFWg9wSdkDHTso+/R4RsAEcWbR+vr67Fnz55JQog0sBeb7Cc9Hs8uKeWskpIS24auGOjYh//0RBS3\n1AvPI6WlpU8B6AKmPpD9GgDsKikpsXVpCAY69mGgQ0Rxr6SkpBoAC/5RzGBEbh/9a3m6ujwEERER\nOYiBjg2E2/0bAM/oNr0OYIe6nYiIiBzCQMciNZhZAiDH66FCAEsY7BARETmHgY4F6vDUU/Cdn6Nt\nm8dhLCIiImcw0LFmFIBuAR4XALqr+xEREVGEMdCxpqvN+xEREZGNHA10hBC3CCG+E0JUqbfPhBD/\nFWD/64QQ0ut2PJJt9pJn835ERERkI6fr6JQD+D2Arer9awEsF0IMlVJu9POcKgD9dPedrEJ80Ob9\niIiIyEaOBjpSyne9Nv0/IcQtAM4E4C/QkVLKfeFtmWmdbd6PiIiIbBQ1OTpCiEQhxJUA0gF8FmDX\nDCHETiFEmRBiuRBioMFxU4UQWdoNQKaNzT5g835ERERkI8cDHSHEYCFEDYATAJ4HcImUcpOf3bcA\nuB7ARQCugdL+dUKI7gFOcQ+AI7pbuV1tB7DH5H7FNp6TiIiITBJSOrvQthAiBUARgA4ALgVwI4Cz\nAwQ7+ucmA/gPgEVSypl+9kkFkKrblAkl2MmWUlZZarvbnQzgGACjOjllAHpKl6vRyvmIiIjaKnVU\n5giCvH47nYwMKWUdmpORvxJCnAZgKoCbTTy3XgixHkDvAPucgNJbBAAQwta1N0fAOMgBmmvpuO08\nOREREQXm+NCVDwlo2QPjlxAiEcAgAHvD2iL/gqmPE6iwIBEREYWB03V0HhVCjBJC9FBzdR4D4IKy\nKCaEEK+o27T97xdC/FII0UsIMQzAawB6AHjJgeYDwdXHuTRsrSAiIiKfnB66ygfwKoAuUMbdvgPw\nKynlSvXxIgAe3f4dAbwIoADAzwBKAZxlJp8nTIKZTdU+bK0gIiIin5yuo3ODweMur/vTAUwPZ5uC\nZHbWFdCch0REREQREo05OrFkDcxPV/9tOBtCRERErTHQsUCdLj4VxstQLJMu1wmDfYiIiMhmDHQs\nki7XWwDGQ6mn48sy6XJdEsEmERERkcrxgoGRFmrBIcPjut2JABq8NrdjTw4REZF1oV6/2aNjE19V\njxnkEBEROYuBjk3UHh0iIiKKIgx0bCDc7t8A2OFnOxERETmEgY5FajCzBEChj4eXCLd7Fnt7iIiI\nnMFAxwI1gHlKu+trFwAPANjP3h0iIqLIY6BjzSgoi3UaLYneCcBSBjtERESRxUDHmi5B7j+Pw1hE\nRESRw0DHmr1B7t8dSi8QERERRQADHWvWwnj5B28XhqMhRERE1Jqjq5fHgVEwzs/xNl243Z9Kl+st\ndRhrFJQhsL0A1vgqPEhEREShYY+ONWNDfN4Lwu0eD6X2zscA/qZ+3cGEZSIiIvsw0LGme4jPywXw\nJlrX3imEUnuHwQ4REZENGOhYs8vi872HvbT7nJ1FRERkAwY61nwchmMKcHYWERGRLRjoWPMJgIow\nHTvYGj1ERETkhYGOBeoMqacMdwxNsDV6iIiIyAsDHeu2hfCcCvivvyMBlAFYE3KLiIiICAADHTvk\nBbn/EQCvQsnF8Q52tPvTWE+HiIjIOgY61h0Icv9sANPU770DnXIA46XL9ZblVhERERErI9tgn4Xn\n6qeXjwErIxMREdmKPTrO0gc6DHKIiIhsxkDHunybjsO6OURERDZjoGOdXdPAWTeHiIjIZszRsW4t\ngEYAVpdsyBdu91XgKuZERES2YY+OdSNgPcgBgLngKuZERES2YqBjnZUhJ39FA7mKORERkQ0Y6Fhn\nJUfHX6DDVcyJiIhswEDHOi1HJ1j/QeDXn6uYExERWcRAx7pQc3TqTO7H2VhEREQhYqBjXaiBSI7J\n/biKORERUYgY6Fi3P8TnFRg8l6uYExERWcRAx7rRIT4vGYC/xTu5ijkREZENGOhYoM6I+p2FQ5T5\n2c5VzImIiGzAysjWjAKQbuH5Y3xs+xrA6ezJISIiss7RHh0hxC1CiO+EEFXq7TMhxH8ZPOcyIcRm\nIcRxIcQGIcSvI9VeH6zOiDrPx7afGeQQERHZw+kenXIAvwewVb1/LYDlQoihUsqN3jsLIYYDWATg\nHgArAFwFYJkQYpiU8vsItVkvHDOiRgu3ezyAt6H0GHUB178iIiIKiZDSX3FeZwghKgHcJaVc4OOx\nvwNIl1KO0237HMA3Usr/MXn8LABHAGRLKasstVXJ0TkA81PFg1ENIFN3vxzAVObtEBFRWxTq9Ttq\nkpGFEIlCiCuh5Lx85me34QD+6bXtI3W7v+OmCiGytBtaBg+WqD0sn9t1PC/e7eT6V0REREFyPNAR\nQgwWQtQAOAHgeQCXSCk3+dndV+2Z/ep2f+6BEgFqt3JrLXYM178iIiIKkuOBDoAtAE4FcCaAPwN4\nWQgxIIg/wbBIAAAgAElEQVTnC/hfHBMAHgOQrbt1C7GdrU+sBBxn2XU8M6cE178iIiIyzelkZEgp\n69CcjPyVEOI0AFMB3Oxj930A8r225SFAhWEp5QkovUUAACGEv11DMQpABzsPaBLXvyIiIjIhGnp0\nvCUASPXz2GcAzvHadh785/SEm1MBR6jLThAREbUpTtfReVQIMUoI0UPN1XkMgAvA6+rjr6jbNE8B\n+C8hxG+FEP2FEA8A+AWAZyLddpVTC26+zKRkIiIiY0736OQDeBVKns4qAKcB+JWUcqX6eBF0vSZS\nynVQaudMAfAtgPEALnaohg6gLLh50IHzcgYWERGRCVFXRyfc7KyjAwDC7b4cwN8tNyx4EsoMsp7h\nKCSoJlqzYCEREUWFmK+jE8MOOHTesM3AUnuKdgD4GMDf1K872INERESxhoGOdU7PgLL1/GowswTK\n8Jgeh8uIiCjmMNCxzqmEZNvPrw5XPaXd9X5Y/cqChUREFDMY6Fi3FoBTuSsHoCRE22UUlIKK/ooN\nsWAhERHFFAY61o0A4FQPx602JwibHQZzeriOiIjIFAY61jl10V8kXa4lNh/T7DCY08N1REREpjDQ\nsc6pi/67YTjmGihT1v3VHJAAymDvcBkREVHYMNCxzqmigbYHWOow2FTtrvfD6tdprKdDRESxgoGO\nRepF3x3JUyKMvSrS5XoLSsXp3V4PlQMYrz5OREQUExxfvTxObI7w+cLSq6KrhpwK4Fooy3IAykry\nC9iTQ0REsYaBjj3cAGZG6FxB96qYWc5BLQT4FJTp5d5KnQxyuBwFERGFioGOPdZAGVLyV3/GTssB\n8xd/PwFMuXC7p2oBk64asj9jAJTa0/zgmGk/ERGRP1zU045jut1j0TzME27lUAKrXwHI8dre4uLv\nFcDogzDtlz4eSuC0A8oSD/4Ctf0ACiPdi2Km/Qx2iIjahlCv3wx07Dim2/0QIjd05Y938HI2gDfR\nMhjy3r8cwHUwF6SNkS6X21oTzVN7rHbAfwAW1tXbiYgouoR6/ebQlT0iMWRlREC5+L8A/7k23vt3\nB+AyeXzThRFtyqnRlqPwexo0L0fhDvLYRETURnB6uT0qnW6ASgDIhXGQEwpTdXvU4aYdAD4G8Df1\n644QVj3nchRERGQZAx177He6ARb8jMDVkAHl5zOs26PLqSn0eqgQwJIggx0uR0FERJYx0LHHHqcb\nYMEcAO3RPPTly5NGQ0/qcNVT2l3vh9Wv89T9zOByFEREZBkDHXsYXZSjXQ6UtvsbgvvYxDG0nBp/\n+Ur6nBpDXstRtHpY/crlKIiIKCAGOjYwuCjHAi04qdVte1H3vZkAzvacGt1yFN7BDJejICIiUxjo\n2CsaZl+FSutx8cXM30lYcmrUYEb/nDFQppQzyCEiIkMMdGzglZ8SL27Sfb/CRCJxRHJqpMvl5nAV\nERGZxUDHHkY1X2JdHgxmTYU5pyaWe8qIiMhBLBhoj3iv5aLNyPqzcLvfBeCBj4KA0uV6S7jd4wEs\n9Xp+OZQgh8NNREQUUQx07NEWarkIKD07hwE0AMjSPVYu3O75ALai9WsxBlxtnIiIHMJAxx5roMxY\nSnO6IRHg62fsBuAhXzvbtD5WrE7bJyIihzFHxz4NTjcgjjFHh4iIQsIeHXuMQsuhHFLpKiFbXeST\niIgoaOzRsUe8JyNbcS/sWeSTiIgoaAx07NEWkpFD9RCsL/LJHB0iIgoJAx17rAFQ5XQjopjVRT6Z\no0NERCFhoGMDNd/kI6fbEWOCWuSTiIgoFAx07PO80w2IUcxvIiKisOGsK/t8AqACQCenGxJh9wOY\ngtCXwIjr/CZ1aI4zzoiIHMIeHZuoF68pTrcj0qTL9TCAHrpNz3jv4u+psGGRz2imJlvvAGecERE5\nhoGOvZZDqZDcpnj1UGz1tYuf+6Eu8hn11GBmCazPOCMiIgs4dGWve9E2loEIxngATwPoqtsWN4t8\n+hqaUh96StvF+ylQAr15wu1eHq+BHhFRtGCPjk3UC940p9sRaT56Jlr03qjBzGm6TRcC6BknQY7P\noSkoAW83+J8WzxlnREQRwh4d+4wCkON0IxywRLjd4w320Qc/nwEYJdxuU8m5agCZqr8fDb0guqEp\nb4UAHjR5GM44IyIKM0d7dIQQ9wghvhRCVAshDgghlgkh+hk85zohhPS6HY9UmwNoyxetebrvW/Ri\nCLd7LFr+nX0Lk8m5uh6TXN1mx5N51eAr0NCUWXE944yIKBo43aNzNoBnAXyptuVRAP8QQgyQUh4N\n8LwqAPqAKBqWCGirFy1tGEbzB6/HVwH4WXffOyDUknPHQ0nm1vJdesN3z0jT/g4Of41C4On0WrAj\n4TvwkVDylOJ2xhkRUbRwNNCRUp6vvy+EuA7AAQAlAFYHfqrcF8amhWINlItXIdr2kgXpPrZ11H3v\nLzn3LwAaYTz8Fw3JvGZ77/wFOUAczzgjIoom0ZaMnK1+rTTYL0MIsVMIUSaEWC6EGOhvRyFEqhAi\nS7sByLSttTrqRWtqOI7dBggov3uzOU5OJ/Oa7b27H8AJr23lAJzsjSIialOcHrpqIoRIgJLrsVZK\n+X2AXbcAuB7Ad1Aujr8DsE4IMUhKWeZj/3sAzLK7vb5Il+st4XZfCeDvkThfFPkZLXttIiXovCib\nKhUb9d5pQ1OPAhgJ4Jfq9jEhns82rNRMRG1NNPXoPAtgEIArA+0kpfxMSvmKlPIbKeUnAH4D4CD8\nVyV+DEpApN1CXarArANhPn40+tyh8waVF2VXpWKv3jujYohS9zy3w0EOKzUTUZsTFT06QohnAIwD\nMFpKWR7Mc6WU9UKI9VCSV309fgK64QMhwp4+0xZnXw2P8PkCJvP6KeJ3EfxPBw86uVntvYuZYogG\n0+GdTu4mIgobRwMdoUQdTwO4BIBLSrk9hGMkQukJ+sDm5oWqLc6+yjbexTYBk3nVC/pTaNlzVw6g\nvbaL91MQYnKzGuysBaAlxl8KIOqqHZuYDu90cnebxuFEovByeujqWQDXAJgAoFoIUaDetIsShBCv\nCCEe092/XwjxSyFELyHEMACvQVlU8qUIt90fLX8jGqa8R0okZ5n5TeY1WF+qE8JTqVh/QVoXpRco\nbTo8KzVHGQ4nEoWf04HOLVB6A9xQPslotyt0+xSh5XBQRwAvAvgPgPcBZAE4S0q5KQLtNWSQv9EW\nhLt4o8/lI2wq4hevw45mf654/fmjEhd+JYoMRwMdKaXwc1uo28clpbxOd3+6lPIkKWWqlLJASvnf\nUsr1TrTfH/VCPB7Abqfb4oA3ve5vs3CsQwD+qd8QoMfkbATutTAjXocdzf5c8frzRx2Tgfk8dT8i\nsiAqkpHjkZq/oa/0+zeHmxQp3uteFYd4nPuhTM9+3mhH9ZPviyGeB7CvUnHQQZYuP6MQSlK3hBIc\nPitdrnqL7dGYnQ7PSs06Yc6dMVNdWxtOdJs9KPN9iFpzeugqrkmXq1G6XG60LhoXz9ob72LKVihv\n2AGDB133v9k6PkbTwSPGKz/jNQC3ArgNwFwAx4TbPduO8xgUs2SlZh8ikDtj+3Ai832IfAsp0BFC\nnC+EGKm7f6sQ4hshxN+EEE4UjotaXl3UZJ72Ru23rpJB9783CWUozHs4sRxKQclU4Xa7ghkqUPcd\nod8UxHP95WdoEgHMsDHY0YZTvbFSs5cI5c7YOpzIfB8i/4SUwefLCiE2ALhbSvm+EGIwlEU550Cp\n/LpZSjnZ3mbaR10G4giAbCllVdjP53a7oFywyQbS5WoKJoJ4bbU/cm3h0Ab1/nIo66p5T0WfGujC\nrwY49wKYhpbLVuyBMtS2DUAelOKRUwGcrm+7+vwdMLcuWiOA9nYNYwm3uw5AsnrX8UrN0cbE70Yb\n5utp5XWz8zyRajOR00K9foeao9MTgDbL6VIAK6SU96rTvd8P8ZjxijNZwsfsa1sD4DoteBFut7b9\nIrQeygpYQE/9ZDwfynR1b10BPGSiPUb5GXqJUIa05unaYCUPo+lCqA6rUkthyZ3xJl2uRuF2T4XS\nC+O9yn2ww4kRaTNRrAo1R6cOQJr6/bkA/qF+Xwllujc140wWG6nDS1epvTn7TT4t0EKupme86IYH\nfAU5hnTDB8EGv00J3czDCLuITcUPMDsz2OFElg8gCiDUQOdTAHOEEDOhdMu/p27vC+WflJrlAvA4\n3Yg4or/AvwygwuTzgpmq26qAnk25Vlobgg1+t6ltMMrD8JWD460t1nYKRkSn4qvBTA/dplfhp1aU\nDW3Zr/+gwKnr1FaEGujcBiXPYTyAW6SU2ieS/wLwoR0NiwfqhWkxIls5uC0pRMscmUBCqfyr/wRs\nVF04mDYEUz27EcCzJuquCABvmAh2+LcYmNHvRgIog41T8b2Gp3aGkEdjps2HoHwwYE8gtTkhBTpS\nyl1SynFSyiFSygW67dOllHfY17zYFeSMIApNsK9rsF33+bphsq5GO5ttQ5DVs99VE5HNBFqJAN7k\nxSt0Qa5MHxVMtjkXnJFFbVSo08uHqbOttPsXCSGWCSEeFUKk2Ne8mGZHDwAZC+b1DXa4YS6aP/3O\nM9jXrN5A05DF5Qg8rCkBlKhBs9kgTVugk8MSIbIxdyZiDNpcidYJz0CAfDSieBLq0NULUPJxIITo\nBeANALUALgPwuD1Ni3lM/IsuZQDWqr0zochVv1rNcblJd1E5BKUXxh99rpDZIM1ogU4G3ib4yJ35\nAsHnzkSUjzb/D4DrEL4FbYliQqjTy/sC+Eb9/jIAq6WUE4QQI6AEPdPsaFyM42yr6LIHwE8wP63b\nm0Dzp2Jfn47N0k/zDWa2zBIABwF0DuI5ZIE6BVy7WxFNw1X+eLW5FEAfk0+Nu78XLodBmlB7dITu\nueeiuXZOGZo/+bZ1wSScUvidgdCDHI0W3ByyeBztomI2GO4NJUgzG+QASn5RU2+RcLsT1d4sfcFF\nzr6Jf21yQVeWYSC9UAOdrwDcJ4SYCGXVaG16eU+Yr20S14JMOKXYYrXHUruoaMGwP9psmQfhf6kI\nf+ZCfWP3etPX/8/zAhDfJByYReY0LodB3kINdKYBGAbgGQB/kFJuVbePB7DOjobFgwAJggx8Ytse\ni8//DDC92Kbw+hqMQihv+EsROFDiBcBYpHKb7HxvELE4i8wKE2UYABuSr7UeUvaKNovm1yTU6eXf\nSSkHSymzpZQP6h66C8C19jQtPugSBMcAmADgHCiJ2xR7tE+/ay0e5/+0N4EAya3aYqOBEkmNaPV1\ngMDH4OwbB3m95ifZ/TswO4ssmi9UQTCa7Wo5+ZrDYq1F+2sSao8OAEAIUSKEuEYIcbUQYpiU8riU\n0pbFB+OJdLkapcvlli7XIijTidOdbhOFREDpzbQ6Q+VWBH4TqIcyDLzVz+PhwNk3DtBdIDSTEIYL\nhI8ZWQ9AN4ss2i9UQQjrchgcFmstFl6TUOvo5AkhPoayavmfoAxhfSWEWCWECCZhsi2Ku9kNbchc\n9eubNhwr0JuANqSQb8N5gsW/zwiJ9AXCa3hqo3Y/Fi5UQQhb8nWkhsViSay8JqH26DwNZaHEgVLK\nHCllRwCDoCzo+Se7Ghen4mp2QxvTAcoFoaMNx2p6E/DxWAKUT9dzfTwWbvz79M3WvLpouUA42Y4w\nDZWFM/k67MNiMSgmXpNQ6+icD+BcKeV/tA1Syk1CiFvRvJI5+bYWyvpFAf+pExqBwRuAThVARSdg\nw2DA02Y+J0StyTYfT3sT8JaE4GdZWantoz2/HD4uAFbrkcRJPRO7k5G1C0Sg8+lrLoXCTJsj0Y7W\nB1V6iZ7yOne5cLunWinKqNYRmgrlA4n3/4TV5GuuEt9aTLwmoQY6CVDyCLzVw2LeTxswAgZBzqjV\nwG3PAHkHm7cd6Aw8cxuwZnSYW9e2WA0OwinYdvkqZCgDPAYf+7W6AFi9IIXrgmZFlAReTl8gtN95\nxNuhGyrzpg2VWVpmQ7pcb6mL276Elr2v5VD+xkM9dpusSWQgJl6TUIOSfwF4SgjRtNChEKIQSlf7\nv+xoWBwLuDjkqNXAg7OAzgdbbs89qGwftTqcTWtzTjjdAJt5BzLlUGbbXIrWM25a7ed9AbCau2FH\n7oc6vDFWuN0PqbexRkMcgYZEoijp1ukLhPa3EtF2RGqoTP1bnqHbNAbWl/BoczWJTIiJ1yTUQOc2\nKDk6O4QQ24QQWwFsB5ChPkb+5fl7IKFR6ckBWr8DJED5i7n1GWU/skU7pxsQRo1Q39h9zLjR83kB\nsHpBMvn8Pwu3+xx/ORpq8LEfwCoAM9XbKgD7/QUmgQKZKEu6jcQFwkxeUaQvVJHM6Wj6mdRZr5be\nOdtaTSIzYuU1CbWOTpmUchiA/4aSTPknAL8GcDGA++1rXlw64O+BwRuU4Sp/7wAJAPIPKvsRGWgR\nNPh7owlwAbB6QTLz/DwA/4SPnhVdUNLJx3M7AVjqHZgYBTIA5uvO7d0WIIKzQ6LlAuFAO5wesrMk\nFle2D7dYeE0s5dNIKVdKKZ+WUv5JSvlPKG9AN9jTtLjlt6pupwpzBzC7H7V5Vj4Vm73QdPUzTBTK\nhaoblABmPJSgxChPqSkw0fUg6YskarT7plfx1q0NprG9mrnuAuH9nmDXBcJUnleEL1TBrO8WlXz0\nkP4NUb6yfbj5eE3+gih6TZg4HHl+u4orfH129cHsftTmWflUbPaCNA++812s5HS8BN89Od66A7hd\nl1xsNHvIjAu9hr8056oBmK3UC0Ff3aZ/wYELhI8LVXWgdliYGm60vpvmJqdrrwTi1cO1y+mhmWjg\n9RrsiKbXhIFOhKm//L/BxxvvhsHK7CqPv+cC2N9Z2Y/IBCufio1yNzTeBUK1YaJck8/3JTuIfedC\nCUouCuE8vlwP38NfKQDeFG73bG2DjXVg9P/yB5y6QHidt95fO6wkdKvHnG+0H6Kg9koQuHZhlAt1\nejmFSH0zuAs+pvt6EpUp5A/OUt75fEWhH53Pejpk2h3C7f4jlJIGPnt3hNs9AUquzAEA+9TN+VB6\nZKYDWBzkObWp7HMAvAhl5fVwT+MvhP/FUYNlFGTNEG73l1D+RcMxbT6qL5o2TQ03u7TJWOF2x2LN\npbgXJSUaTAsq0BFCGP0Bd7DQlrhnMBMFgFInZ9aDrevoHE8B2tUBV74BbBwI/PuM8LeXYl4ulLyL\nQMuyvB7gsXIANVBmWAZDy3d5UHc/nLTgKlJeglIF3pstdWCiRKvfmYmZdBJK3tRyg4ue2WHNmQAm\nO1lzySnRHEgEqo3lUJMMBTt0dcTgthPAK3Y2MM4YzUQBoAQ7Vy0Cps0FHr5P+XrhO8DqUUBKPfDw\nTOCMzyPTYIp5VtaeK0TwQY5dqoLcP5KFH7MROOnZ9OwtdT99GdBoTiewa2q42WFRIDbX27Ikimo9\ntWJiZmNUCuqfSko52cwtXI2NA6aTQz2JwLenAv86R/lanwo8dD/wyWgl2HnofuDMz8LZVCLHqkZX\nI/TZm0b1YELNGzLLdB0Y3QXtI93mC6LhguaHLVPDvaa0G4mahSEjIcpqPbVgsjZWVIrmTw/xyFJ1\n0cYkpTfHfTaDHYprK6TLtQRKRefqIJ8bqLchDcon5EgIeLEPcEFLQ/T2YNhWRVk3pd2MqFgYMtyi\nZaHXAMz06EUlBjqRpS3oGbLGJOCR+5RgJ7lBCXaGr7OpdUTRYaRwuxPVi+EzXo8Z9dhcBuC4n31y\noEwE+MCWVgbm92JvJlcP4b+ghXJRsrWKcgh5N7YWEbRx1pxdPYTRvhJ4VBZxNIOzriLLcEFPM7Se\nHSmAMW5lltaDs4C1I603kCgK6FfLNnMRaargC2A5gAY/+2kJs8NMHC/UT6d+V4HXsW3FcF9Jq0G0\nNSheK4P7bA6ARWaTZkMILEyXS1CP3U9/X98ug4Ta5XAmEdj2qtE2JzUHMyIRVbMH2aMTWbZFxJ5E\npWdn1VilZ2fWg8CIT+06OpHjtP8V7zfMCQCOem3TV/AdBWXNPX8EgIIAj2vnC3bITH98oyUTzL4P\nXOj3JEpvxEwoZQFaJK16tcVW6mv8hL+HAdxlcrFWLT8pGKaKCOqOfZdus6/lRfwl1O6HM4nAti6w\namdSs/q6JwCoROAevaanBHuOcGKgE1m2rkTsSQQevbc52HngAWBkW1o3l5zmr7alHfz9rywH8Ibu\nvveipFY/TJQDmIXQZ5sdgtLGQMy+D0z3dVHSLXb6EJThOD39xTshwNBMSJ+41WNM8Pew+jXgsFuA\nQMNIq2EbH8NP4/0cu2nqPwLnwQi0rsodqURg24YG7Uxq1gVMq6D8vfkKYqKqB8cbA53ICmZapSla\nsPPPc4CkRqVnZ9Rqu45OFNCkMB33EJrfzH0tNBloVWorHybqAfSE+YJ2vuTCvunVWl2apqDBYLFT\noOVFaDn89/YY8feJPJhht9YPmstPCqQpkPXTY/EGAk/9fxYmSnz4eW5Y86ZMzEYzNTRoZ1JzEEFp\nOcwnl0ccA50IMlgpOGSeROCxe4CV5zYHO6M/sevoRH7tD9Nxc+F/SQejC9QaKEUO/ZEADvt9UPkf\ntdrzatf06lYLjaL5AmZGntd9/cUq1KEFq3kkpmqJBbAXCHgBDnTxFmj9mpgVkURgm4YGbUlqDiIo\nPQdRtICnL44GOkKIe4QQXwohqoUQB4QQy4QQ/Uw87zIhxGYhxHEhxAYhxK8j0V47BFgp2BJPIvDH\n3wP/OA9I9AD3PwSc7bbzDEStfGS8S0ha9WSYfqISRATq2hcwruCeG+x5vZidXj3P5PHGei1cGmqQ\noH+e0Xu/vw9iVvNIQh1abBq2saFXyAp/S6kEPYPL13PsGBr018YQ9jP79+aJlqrN/jg96+psKF2J\nX6pteRTAP4QQA6SU3gmHAAAhxHAAiwDcA2AFgKsALBNCDJNSfh+ZZlsjXa63hNutZfbfCpu6/DyJ\nwOy7ldlYv/oHMPNhQEhg9Whg8AagU4Wy8vmGwVwvi2wRrg9KgT5xtljuQbjdLtg4O0a9gMxFaDOv\nWsy40gUnXdG8ntgeXTvfgbKemJGZACYDeDPI9gTSP8TnacNu/oavjGadhdJb1jSrTp355QpwfjMO\nQglmQwmSfLW/H5QhNNPrnvmb9QVlwVOrM/LsSmq2fRaYUxwNdKSU5+vvCyGug/JmUALAX6bJNAAf\nSim17r37hRC/BHAbgP8JU1Ntp/7DrgHwqp3H9SQCj89Qgp3zPwLuexioyQCydXNIDnRWFg9dM9r/\ncYiiwDkABhjs87Hu+0rhdj8FJUE5VEY5KEa0i7GvC5lGP425EeZKThRCee+zS0eDx30GAerPNh2+\ng64WAYmf42qBUqG/c/hQrh7TasK5FoTdieAXqw0UwF2K1j1gftc9M1gY9UEf230J9BoYvcZmSiAA\nNs8Cc1K05ehoKwdXBthnOIB/em37SN3eihAiVQiRpd3g3No9vlh9U/XJkwg8cRfw9alAogSyvCbK\n5h5U6u4waZmi3H0AvPMRBJSEYV9yoFwo2oV4PgFrn04vV3trjRI4tWnMb8N8XS3tguWBPfl9P4fy\nJHXW0nN+HvYAeCJQrkaQyz8ASkB1HVrOZLPaK7QESi/6Ca99Dnnt6+u5/gI4U0m/Ni6jcLK/ITKD\nXFAzP4smF+YK3Eb9XN+oCXSEEAlQxqzXGgxBFaB1EuR++K+NcQ9aLjxabrGpdvJbJ8MO3Xb77n9P\ngLL91meAhKgeWSVq5RH4+VBjAwFrn05PEW73WBjnj2izgi4I8vgCyr+vHXkpm4N9gnC7Z0MJPPwt\nFJsAE8myaiB0pcnTXgZlWrO+/ouZadjej+lrLWlt0C+gMwbALQAq0Pr1rdA/Nwi+hmDtWkZhJgLU\nxQmQC9ridfBHPeZimIgRoj0/B3A+R0fvWQCDAIRS37fFuL2XxwDM0d3PRBQEO2okfk24jj94A5B3\n0P/jCQDyDyr7fXtquFpBZDs7h2+8CYQ2tKKZqd5iwS+E270EAaoqe1XV7QNghsExtffhecLtXq5d\nAP1Ubz4QZHtbDAWZqNDsrafBBTkHvoezJPxP5TfrQjTn09idz+J3iEyXC6pVCt8E4BSL09NjUlT0\n6AghngEwDsAYKaVRELIPQL7Xtjz4meoqpTwhpazSbgi94qndRsH/JyPLOlXYux9RG5Cg3uYjTt7g\nA7gZPqrm6h5P9nrcbO6I95R4n9V5EXxvdouhIF2PRZXJ5/srV6AJ52Ka09UhP8B8j+HHxrsAMGif\nV1BTZbL3xersvqjjaI+OEEIAeBrAJQBcUsrtJp72GZQkRf3UzPPQshsyFoQ1U73C5GeQwd8Bn44E\n6lLD2RqimHEIQJbTjYiATLTOV9TnFKVDWUk9VF0Mkm5D6ZlrMeNI7bEoAXCviee26GXywcxMp9uF\n270foa0p9oZwu6+EkpdVAf+9RFqi8Kcwn1Rvdm00CZha/yrqZ1EFy+mhq2eh1Ay4CEC1EELLszki\npTwGAEKIVwDsllLeoz72FIDVQojfAngPyljvLwBMiWjLrQtrpvqGwcrsqtyDvrvttNydi98BRqwD\nXp0IvP9rZcFQojYsloMcD6z10ltJjvW2H8DLAY5rJaFafyE2exxTi6QamKv7Ptj0h0Qo+U2Po/Wy\nHXoCShB4RZDHB0wEKIEWM9UNfZlePFU9pj5wikpOD13dAmWmlRvKhV+76X/JRdC9gFLKdVBq50wB\n8C2U7suLY6WGjo7ty0HoeRKVKeQCrRck0u6/fRGwPw/ofAi4cy7w8rXAuSuZoEwUo5x+PweU97MK\nAENgT9KtL0FdiHXsvBDrA4UvTD5HAvitwT6HoPweLw+hTUavSycEXv9qjrpQrJlhynrd9zvQPDSp\nOdnEMSJGSBnVa3HZTp1ifgRAtpqz41xbWnbthmU8dNRq4LZnWiYm7+8MPKvW0UmuA8atAK55DchR\nJ5xu7wEsuAFYOyJcrSIiCpmEOnNIuN1/gLmhKwAYI10ut3ZHuN0fA3DZ0J7XYO/EkoMILX+z6XXR\nb/M2sSgAACAASURBVBRut3aRb4B9ozhlUHrJtPP6ulJcaveyEKFevxnoON0eJdh5HmFMTE5oNK6M\n3O4Y8Ju3gCvfADLVlYL+0x946Ubg62Fo8Wds5nhEhMPwvdyEUdXlUGd9tRVaHktPAAthLsg4AKCr\nPhfFxkDnEwB94fzQjfa6FAMYAaU9vaGscm+ntVD+rgca7FcG49luQWGgY1K0BToAINzuCQBed7od\nAJBeA1zxd2D8EqD9cWXb10OVHp5NA333ELHSMpFP6wEM9dp2KfxXTNbvszRcjYoiVnOKxkApjHex\niX3nSpfrTv0GGwMdAPgzlFSMaBBqj5BZK6AEOf4Kd+q16EWzKtTrdzSM6ZISgUeFoxnAX24Arn4d\nWHIpUJcMDFuvDHX9+WalonJnr/o8rLRM5FOrClVqV34Pg+cNBPB7g32Oh9imaGJ1pmwXBF6pXu8d\nH9v0n/Kt5ksaTV/Xztdo8TxmhDPIAYCjUMoPmOF0LxcABjqOU4euzNaoiJifc5TgZuKrwIr/BhoF\n0P+H5pKueqy0TORTq6EntXLy2QbPewitCw+e43X/KwvtihZWE1b3QhkeMWOt/o46U0g/rGhmcdVA\nuprYRwB4V/0+lodSytFchNCIz/p2kcZAx0FeFSij0oF84P9+B/zR4POlvtIyEfm1Sr0ZSfe6P87r\nvr8K8rF0AQ00zdrIESiJtUUm9z+kzipyqcX7dqDlsOJcAE/4fKa9LoDSuxTL+Ve+ltiIaqya4qyw\nLOoZDmaTjVlpmSgszPQ41ADICHdDokQ2gJVB7J8F5TX09zoWArjLaqNMSIS5Ya5oZzZ28F7FwBHs\n0XFWVIxfmmG20nKPHUCi2U5NIrJTWwlywsHXqHwwvMuVxRJ/K5wHUm+8C4AwF8Y1i4GOs6Lij8AM\nrdKyv/9m7T9j4mvAoquAKxcpM7iIiNqAWBqK0gcyq+B7hfNAi672AVBr4jxlCH6pjLBgoOOssFZH\ntpOZSsv/PAeo7KhUWr55PvDmZcBtTwMFMRPOERGFJJYCHX2CwQ60nAW4DcbTxi9Cy6VS/F2/ptlZ\nQ8cK1tFxuj0RqI5sJzOVlsf+C7h8MdBLXaK1MUFZOHTx5UotHm8sQEhEFDH6gpV/kS7XDbrqyd8C\nKAFwAko+kT/6KstVaL1G3CLpck2wqb1NWDDQpGgLdAC/C61FLVOBiQRKSpWA5/QvmzdvHKAEPJ+O\nVJ7DAoRERI45CmASmgtUfgtlUdGPLR73fulyPWzxGK0w0DEpGgMdABBudzKUsdJwF3uKuB7bgcve\nBM79J5CiprDtLQDWnwr814fKfX1Xlke9P+tBBjtEFBFGy3JEkgRQjda9JOE8n/azfwtgNlou0BmK\nqAp0mKMTPUYgDoMcANjRE3hihrKO1isTgSNZQJd9wK99BDkACxASUcRFS5CjKXXw3HZkVUbV68lA\nJ3rEzFTzUP2cA/z1emU4a/F4ZZu//wYrBQgTGoEh3wBjVylfGSwRUYwoAzAeQGUEzym8vl8DZakK\nK36rpmREBRYMjB5tZm5SXSqwpb+5fe+co6ye/lMvYFsxsL0ncCzN//7M+SGiGNZTulyNwu2+0KkG\nqOevAJBn4TBZAJYIt3u8ur6boxjoRA9tqnkhoqzbLxzMFiAsKlNueru7KgGPFvz81AvY0xUYsVZZ\nXNSbtuhoqDk/nBVGRBHm1DVAywsyUyfHjHnC7V7u9DRzJiNHkVibam5FQqNSWDD3oO/xUw+Uoa4X\npgA9dwC9flJunQ/5Pt7xFCDRAyQ1+H7hPAAOdgYmLAouSGEPERFFUDmAqQAmALjUgfPXA2gP4EcY\n19Mxa4x0udx2HIizrkyK5kAHaAp25gMw2ecRu0atVnpaJFoGO4FmXWUdAXpuB4q3NQc/PbcD7U6Y\nO+dXw4BtvYHKnNa36kxA6hqitQ/grDAiigjtguwGMMahNkwH8ACU9cTsMEG6XIvsOBADHZNiINBJ\nhFKtMiZq6lhlVIDQjIRG4DdLgVv/bK0tDYnAzx2VW2VHYMh3QLvj9vYQEREZkFCGjrxXsI9Vjvfo\nMEcn+gSzorkWpcbsMNea0cDaEdZyYDyJwI99ze27/ALgeHsgp7LlLbsKSGpUhsb8DY/p6WeFfXuq\n+bYSERkQiI8gR0IZinN8vSsGOtEnmGnmMRvg6HkSrQcL2qKjgXJ+DnYG/jTVdxCVVA90ONwc+Jy1\nFrjgPePz3vIc8N444IszgAP51n4GIqI4IhAl610x0Ik+wUwzj6Zqno7SFh19cJYS1PjK+Xn2Nv89\nRQ3JwKHOyg0AjrU3F+j0+xHoN1f5/qeeSsDzxRnA94OARj//XZzFRUTxSnt/O+U7vH/9X1HphjvR\nJTnrKqJiKEenTUwzt5sdOT+AyVlhHYG3LwHO+DcwYJMy60tTkw589Qvg8zOBf5+uzCDz1z7O4iKi\neODr/Q3qTDKXtF5Ph8nIJkV7oAMEnGbOHhwT7OoxCWZWWGYV8IuvgDM/B07/N9DhSMtjbe6nrO/l\n+kS5z1lcFGnsSaRw8jdLFc25pOOtBjsMdEyKhUAH8LuieRmA9QAcq5rZ1oTSQ5TQCPTbogQ9Z3wB\n9PvB+DxWZnG1pQtYW/pZ7dTWexLt/LuJ9r9BJ9qn9YB3Puj3k7iWmNzTyjAWAx2TYiXQAZqGsUZB\nSVDeCyV7fTaA36q7BOrhaVQfZx6WRVbfODpWAuPfBCa8Ybzv3y8D/nWOUvm5PsV4f7svYHa/Sdp5\nvFi4WEfj6xeuelDRfsHX2Pl3E46/wVj9H0loBDKrgawqoOQrYOrTpp42xiVDn2rOQMekWAp0fBFu\n9/8BuFO3yVewIwFcDuBvAJIj1DQKYOwqYOYj5vevT1KCnR/6Krct/VoHP3ZfwOx+k7T7AhPtxRuj\n8fUz+qQd7xXD7fy7CcffYLT8jyTXAdlHlKBF+6rdso+03pZVBWRVB9c+1QSXDL14IAMdk2I50FGH\nsxYCyNRtbgSgf4v6GcCN0uV6S7jddWCgExWGfAPMm26835a+QJe9vt9E9MHPj72Ba18GOh625wIW\njqDJruOF62Jtp4i/fg8AX5copRA6/tx809/PqQTy9wM5Pxuf75shwKYBwL4CZd24vV2AA3nKbMRw\n/6y+2NHLYeffTTj+BiP5PyIB1KYBK89V3lv0QUxWFdD+uLnz+FKTDhxrB3SuMLU7e3QiIVYDHV2C\nsq/eG+i23ypdrufU59SDQ1dRwcwsrqY3ygSgYB/Q94eWt+wQ/lrf+zVQ3g1oTFRuDUmtv/cIYPo8\n5Q3P35t4ZQ5w67PKyvP1ycqtIcn3m7qZi8KhzsDNzwPptUBGjdIFnlmtfK/d17522Wsuz+nux4B/\nnxnc6xOJC6oEcDgbeHyGMjMvsdH3LalB/VoPXP03IP2o/+MB4Z+V0Jig/E3u7aIEQHu7APvygVue\nV2pOhSvotNrLITzK38zYfwE3/MV4/8qOShCg/T80JDXftPtpR4FBm4yPtXg8sLUPcCIVON6u+abd\n1742JAKvXxP4f6SyE/D7x4CMo81DRBk1rb9mVgOdDpkONPxqTACOZANVWc03X/e9t3kSjd/fwByd\nyIrFQMfElHP98NX/Spfrz+pzGiLTQjIjlLW9mkjl03m/LUrQc+ZnQPH2sDfZUGMCUJeiXAy0ACjB\nY666dDjs7qqsaK+/7SuAz/+aUC+oqceBrnuAbuVA4W7glO+A4Z/b/qOYUpPevGyJ960yR7nwTPuT\n8XHevhiQQgmwu+5RvppdP86Xp24H/n2G0obj7c0/L6heDgnkHlLWutNuPXYAJ+201kMRz1aPBL4b\n4juAOZoOS9Gzv/c39eIkwVlXkROjgY4LwMcmd79FulzPB/kcihC76vyYHQr77AzgSIeWPQbet9xD\nQFGZ8bEaEoAkj/F+wTiaBtRkKLfqTN9fO1YCE18P/Rw16cBPvVoGP133APepOVO+LqiP/D9gZw8l\nkNFu3cqV54UaxO3uAhzu6Lt3TX/rfAAYssH4eI/dDfzj/MD7BNWTqO+BkcrwV5e9zYFP1z3Ayf8B\neuwM4ocGUNte6S2rzGkOwLT72q2iE1CdEbiXQ0L5e3CfDfTcoQQ1GUd9n7MuWQlau+0xbt/cqcBP\nxcr/h3bT/l+0W4/twJWLjY/13SDgRDsg9YSyTl7qiZbfBxuA1aSrr01m61tNhhKkVGcqv5+pJgLa\naXPDu2SNv/e3L87E43Pecd1t9fgMdEyK0UDnKiiJxWZoPTpzoKxCS1HGzuGSoC9gPpgNmqbNBb4d\nAiTXK7ekBiClTvmaXN/8/cmbzM3AuPNJYH2J8X5mf9b/eV65ABZva76dtBNIqW/9nEDDP2aKVVVn\nKEOCuwuVYYjz/2H8c5i9yAT1+zBxPEs9iSG27WAnILMmuF6hRgEkBnk5akwAyroDO3oo+WvabU9X\n5XG7/kds+3+Tyv/JL74E/jDT+Ocz+zu28/3AKj/vb4cAFFhdDoKLesa3YJaF0PJ5poWpLWSRHWt7\nWV3yQs/sOmEbBisHrk8JPPX9xz7AVW8YH8/sa2D2Zz2cA6zPAdYPa348sQEo2tUy+Om3Gciq8X8+\nLcipSVN6dXYXKrc9XZuDm+qs5v0TGoFh602+fiYE9fswYc1oJZjx/qR9MISeRLNt03LN2h9TLnja\nGnKdKpoTpbX7OZVKzo/ZIGfNCMA9RgloyrsF/lu063/Etv83oeS5fT7c3t+xne8HVvl5f8sFcDaA\nf4W/Ba2xRycGBJmjcyuAewLsS3HErqEwOz/1h+N42jHt+FnNTvV/+D6lppHZtkX76+dExfBg2nbW\nWuDhWcb7Bjv8YtffTTiOFa3/I2HysHS57rdyAA5dmRSLgQ5guCyEfttccMiqTbHzAmbnm2Q43nTt\n+FntHhrSxMLrZ5dw/W7DNfwSrZWRo/V/JEwY6ERKrAY6QMBlIaYBWKre/wuA6206ZS2ANJuORTEg\nGiv72i1WLqjhOJ6dwtG2cPRyRLto/h3b7BzpclkaumKgY1IsBzqA72UhpMvVKNxu7RdpZ4/OFQD+\nbtOxiKJGW7ygxopo7smikNUCyHIqGZmBTpzQBTp25uiUo2XvEVHc4AU1erWhXo62goFOJMVjoONV\nHHAugHUAtKoPVoIdMzNtiWIWL6hEEXO/dLketnKAmAx0hBCjAdwFoATKUMwlUsplAfZ3wXcRvC5S\nyn0mzxlXgY6fvJ1yKHV3JoA9MkRE5LwKAPlWenVCvX77WZYiYtIBfAvgtiCf1w9KYKTdDtjcrpig\nm4lV6PVQIZQAcjqAMQDejnDTiIiI9DpByS+NOEcLBkopPwDwAQAIEdQIyQEp5eGwNCpGqMNVT2l3\nvR+GMuw0B0BPKAHtJZFrHRERUStdnDip0z06ofpGCLFXCLFSCDEi0I5CiFQhRJZ2A5AZoTaG2ygo\nw1L+IkQBoLu63ydQug2JiIicst+Jk8ZaoLMXwP8AuFS9lQFwCyGGBXjOPVDG9LRbebgbGSFmI+Mu\n6pjoFDQXFyQiImoTYirQkVJukVK+IKUslVKuk1JeD2WGUaC6MY8ByNbd4iU51+z6V3sBQLpcbwEY\nj/gJ9IiIKLbkO3HSmAp0/Pg3gN7+HpRSnpBSVmk3ANWRa1pYrYEStPjrpZFQerzWNG1Qgp0eUBKU\nrdCOfZ/BPkRERJqgFqi2SzwEOqfCoRfPSepw1FTtrvfD6tdp3lP51PtrrZxaOzYAd4D9dls4BxER\nxZdj0H3wjiRHAx0hRIYQ4lQhhLZ0Xk/1fpH6+GNCiFd0+08TQlwkhOgthBgkhJgHYCyAZx1ovuN0\nw1HeQUU5gPHq4y2oU9KDCUIqAxz77ADPSw3iHEREFN9+sloZOVROFwx0wXcBwJellNcJIRYC6CGl\ndKn7z4CSVFsIpaT0dwAekv+/vTsPl6yu7zz+/tLIIktDB5pmlcVBjUaZ4BJi0Go6CBl5NMjaYCad\nzEAUjCxRCTwPGGg32qUhYjSiozJAE2mIGBnpZ4gU9kQkAcXAuAWkGVp6R6DZGuj+zR+/37l97rln\nr1N1qup+Xs9Tz7119vpV3Trf+9u+zqUdI+ucYzVhIGTnv0rZLisDep55+BRAydxalwMfydlPsyqL\niEjkCWCPNiYMVAqIaSIEQyuolgNrLbBP8oNp3e6JwI2NXqCIiIy7ua7T6dbdue79u9UJA2Wgonl3\nqjg7JciZAfxdY1clIiLThSYMlL6q/AFznc7SlMVHAnv2fjkiIjLNaNSV9FXZD9gz0S/W7XZCDU5c\nKxG5iIiMtEnTnQySAp3po8y8O08Cm2LL7gBWhE7MkWk3lF9ERHq2pK1RVwp0pomS8+7MBGYl1u0L\nLI0FO8uBdX25SE0yKCIyruantBAMhAKdaaRg3p3HSQ80ohFaV1i3OyMETGc1cTlAMrpf38BxRURk\n+ERJpgdOgc40k0gDcVr4uQD4LcplQo86KS/q8VIMmIHPUxZdR968PCIiMtpa6eOp4eXTUKiV6UbP\nrdudX3LXiQ+p63QusG53O3wqiF6scZ3OknAdx/R4LBERGV4adSWtqZQJPeaWhs9dJQfXlgbOLSIi\ng/EEGnUlLaqcCb3kfnnSjrk5sT5rPwf8psY5RUSkHbdo1JW0psdM6OeQLgpIqhwz3kdodcZxVwIf\nxfcpEhGR0bCxrRMr0BGgXib0xH5JK8PyE6oeM/gDfAfl0/Edlk8Pzw8CHsx9MSIiMmweauvESuop\nk5TNhJ6yX/RB+jawOL5fhezqOwFPh6cHuU5nRca5LgYuq/CyRESkXTu4TmdT8WbZlNRTGpEckVXD\nz5LZaRs45oQwceGlTRxLREQG5ggaug9UpaYrGRmhZujKtq9DREQqay1Pomp0ZJQcCezX9kWIiEhl\na9o6sWp0ZFildR5T5nQREalEgY70LJGobf8eErdlpaCIVJlVcyN+GLqIiLRvTlsnVqAjPQmdg1fE\nFp0GrIhlO29SlQkKdwG+X+MczwIbauwnIiLZZrd1YgU6UlsIZpYC+yZW7QssbTrYKZigME28qavs\nPArPAadUOEfTnm3x3CIi/bK2rRMr0JFaEiOgkk1O0fMrKjZjxT+Pv5e2b5hksGyTVLypq6hZLPJb\nwNtLbtu0rwE7tnRuEZF+OqStEyvQkbqiEVBZAYQB+4ftCoXan5/GFt1AdhPYzwoOl5Wba5g9Chzd\n9kWIiPTJGT303+yJAh2pq+wIqMLtYk1g+yRWTWkCC38oi8lvijLg5cC7S15jUhdYV3Pfuq4mP3AU\nERllpf/xbZoCHamr7Aio3O1qNIEV1SRFZuGDp0iZPjpRTdCdwLUlti9jfcntlL9LRMZdK1OEKNCR\nuopGQJVtPqraBFb2DyXteHnBTjKj+rdLnifNE/hap7n4IZUnAMkcLxuAJbHnVYbOi4iMola+5xTo\nSC2JEVDJACIZNOSp2gRW5Q8lHux8kfxh48mM6lWGsifNBM4FZrlOZ3M45g9j6+cDewE3xpYtZ/DN\nZSIig7KWlvpNKtCR2sIN/ETg14lVyaAhT9UmsLoByFmJfeaFx2n4mpeD4tdbYyh7XFqTWzzo+mE4\nfvzv70jgAzXPJyIy7K4r8Y9vX5hzdf5hHV1107xLtnAzPxJf67IKWF72Ax32XYHveJzV3LQSH4hs\nDvtEnZfJ2CeLi7Z3nU6p/azb/SxwfoVzJM11nU7Xut3vs7X57VRgM/Al/HD2yErgHuCPezifiMgw\nmuc6ne/1coC6928FOtK6nMAl+nBOqR0K+1xJ/SSf2xYFY+EcXwV2q3kO8DVGm4Drge0Lto1e7+eA\n81CNq4iMj9YCHX2RSuvqNIGFZQcCl0SLKp42d5hjLPiaWfG4Sa8MxykKcmBrkHcyvtYH6vUREhEZ\nNnu1dWIFOjIUYoHLXDL6zaTss9l1Ogvxo5qSQVKRzE7QBUPey4pGnZ1Z8RjRKLN1pL8uBT4iMopa\nG1m6bVsnFkkKTUndGvvdbN3uLfhamqOAi0vslvdHFw15Lzw1PjCZ6PsTWw5+EsDLShwnzd6u01kS\ne11R/6d340d0gW/e+jW+qatuE56ISL89RYsz1SvQkbEQBUnW7S4H/ozszs2RvD+6skPeH8f34TmN\nyYHGSnwwskPJ46RZBZODv9Cc9t9i2ywO5zoPeD3lAjwRkUFb1taIK1DTlYyZkvP7AByZk3elbBXr\nx4GLyG5ym13yOMlrnDLRYqzP0C6J7fcFvgm8WPE8G2tcm4hIHQ+0eXKNupKxlDEqawuTg/uVwDkp\nI7qKhrzHpR4jHOc04LoKl506yqzCEHwrec3gh8x/rsK1iYjU9SixKULq0qgrkZhE5+bFYXHy8z4l\naWjYN69WKCn1GMFjVa6Z7FFmZdNkfLnCudZQf+ZnEZEqWkvoCQp0ZIyFgGU5cBLpN/S0GYyjfbOG\nvJc+BuXyga0FTid/lFnZPkMPAh8tue1j1J/5WUSkqlYSeoICHRl/VZOGTojVCp1XcI7UY5TsL/R+\n1+lc7zqdbk61bpU0GZ/AB1dZJvoAhdd3Mn6WZhGRfmpteHmrgY6Zvc3M/snMHjMzZ2aFU9+bWcfM\nfmRmm8zsQTNbMIBLldFVNWnoJCH4WFP3GA3lAyudKb4gR9eUZKuu01nK1skJq3I519SkLQM4h4j0\nR+oAi0Fqu0ZnJ+AnlExmaGYHAbcCdwCHAVcAXzGzY/p2hTLqqiYNbfwYdSZDTOxfKVN8znFTg6sQ\n7JxEfs2OS1m/Eh/ELUpZtxn4PznHK2Mhvqw+3+NxRKQdU76f2jA0o67MzAHHO+e+lbPN5cA7nXOv\niy27AdjNOXdsyfNo1NU0UidpaD+O0YSMkWSP4r9EkiPHoj/sF4E/pUSyVet2TwRuZOoEiIRlJwPr\nSUneat3uy4CzgUOAh4Av4Gti1jA5cWkZk8rTut1z2dqhXERGx3rgL8r+U1ek7v171CYMPAK4PbFs\nGb5mJ5WZbc/kPEPJeUhkjIUb5Tn4OWiyZjDO/W+jiWM0ITEDdNlM8c51OktKHn+pdbsnkJ4s9UOh\n5idr3xdJ+Tu0bvdMshO2ZgWNMLk815a4fBEZLo7q/+T0RdtNV1XNYWp/iTXArma2Y8Y+F+IjwOiR\n11FTxlAT/WQa6mvTs5Dfq+s6nSUFHZgjlXJ1pTSzRWplHc4pt/UZu6SVZ9Vh+iLSvrwRqQM1aoFO\nmqgws9rgPonPQB09lBNoGuq1n0xTxxgF8WCqoeMly+0SsgOwtPIs6owtIsMpc1TrII1a09VqpqZ6\nnw085Zx7Pm0H59wmYFP03KxuMmoZdXWThjZ9jBHV0x9OLBfZe4BLc453PFubuib2zWk6FJHh19oc\nOjB6NTp3AfMSy44Oy0WEic7TE0/brjaOhOu4smCzG0Kn6EkqTOAYt5jsJjIRGZzW5tCB9ufR2dnM\nDjOzw8Kig8LzA8L6T5rZNbFdvgQcYmaLzOzVZnYWfiSIRmSIMDEya0Vs0bbAiowUFVU00WxUNHkj\nwAzgxrTrzWg6PIn0fndfd53O+bT8BSsibKDFOXSg/aarN+LnxIlESQa/ASzAV3cdEK10zj1sZu8M\n252D/4L77865ZQO5WpEhFstwnhTl4xpYp+kMZauvHb4D4y3JztZpTYfW7f4jPoiKf5fcH34+WfJ8\nag4T6Y8r25xDB1qu0XHOdZ1zlvJYENYvcM51Evvc4Zz7z8657Z1zhzjnvt7CpYsMlUSzUPKmPSyj\nH8rWrlTqwBjLaZY8xpRNc84nIs17Bp+WplWj1kdHRNLVzumVJREUvbGBIKnq6KlSNUApzXUAF6c0\nf1Xp39MmTWQq4+LOtmtzQIGOyLjoKadXUkrwcDU99vUpyMOVprAGKNZct29i1cywPD5h2cHAugrn\nr2otfkRZr3ZGQ+llPDzc9gWAAh2RcdFETi8gN3iI+vr0EuyUyZheKglgQXNd5ODY728F9ix3paVE\nSU0X4ztG7wP8ooHjboOa02Q8DMWIaAU6IuOhdIbzvIMMoq9PImN62vUasKRElXeZ5rp4+peytV4f\nBx4vsV00i/P5sVmqNcpLZKuhaC5WoCMyBqpmOM/ReF+fNCHYWZRxHgd8uETNUdVJyJKTjWY5G5hV\nYrsFObM4DxM1g0kbWh9WHlGgIzImGsrH1WhfnyyhRug0smt0oLjmqGrtyWJ8k1nRjX9myeNNCZxC\nIPnlitc16RCUu8Y8WxLP+9kvKW4t8DcDOpcMv9aHlUcU6IiMkQbycTXW16dAEzVHdXJgRf1f8vYp\n2z8mqwx2L7l/Vs3bZzPWRzYUHDd6fVHfof0obtb8TcExy9gH+FjBuYo0MeJMNVjte4ohGFYeUaAj\nMmZqZDiPa6SvTwk91xzVGMUFW4OAXm6GmWUQaqDeW/I4yfQUUc3bBWSnu7gZX5M0j+x+RFGgdiKw\n3HU6L5JdTlE5nEnx+/4kBc1yBU2oZbyfyclfy0z4uCnxfCXwwxrnlua80PYFxCnQEZEJDfb1KdJU\nzdEtlOs4HGfU/+4rKoMjKTeyawu+xiq15i2lZi7yi3DeLeT3I5pUIxZr1kyKgqulFL/vfx67psVM\nbRJbYd3ue2rmJYs8FgvSFwILY+uymuCWkChH4LHY+ktqXIdkK9MUugctZyyPazsFhIgMGdfp3BwS\na16Jb/aIrMTf4JtIIxHVHO1LdofklRTXHB3J5Lly+m0D8Bc5ZVC2pmob4AjX6XSzNoinu7Bu9/qa\n55nYLryv8XVz8TU+m2PrC99363ZnAeemnGtSqhHrdm/Bvz97A2vwqX2y3m/wtTd573c0Um8vfAAc\npfxwyXJMvM5P4IMd3e/qewrYNfx+LnBdiX1azVgepxodEZmigb4+Rcdvquaoly/TdSnnzuOA5/C1\nSFmq9F2qc+1RkNBzjVhas2bR+15l+oFEE+r3KG7S+k78esKou4tj6/8ZHyxtSgQ2ue9hOObavG0S\nqn4uxt0/MjmwnV1yv6GZakERroikSkug2fDxm6g5qvNlGtUWnQ98k/JJPePNQd2MbZbjb5RlOwQ9\nFAAAEX5JREFUmq96uRE0VSM2dcf89z3qRJ4ls4xy3u/IAxMH2TppZfK1TdQa5VxDmqdLbFP3czEo\nLwDbtXDeJ4DPxJ5HoxezJras/dnrF9XoiEhrGqg5qjryKl5btJR6fUmKOkifVeIa6nbotth5rid/\n1FqZSRer6qkTeeL9/mZi9cHW7c5I1BolTdQaxZbNSZmGoChAyatFrPu56Ld/b+m8C5jaHyxr9GKT\n/fgao0BHRFrVyyixkk1gcZPmFErceBembJ8mtyYmNhli6urws6cbQWweoszLAOb3IVt9E01mm0PT\nU/L1n4HPr3YR5aYeiPwXcvKwheUHJxYn5xrK+1xEI8CSI84eDctPw4+C67dhmogyCnJyy3FYKNAR\nkZFWMFHiSRTUFsVuvJfS0ND6MET8JKaOUGnqRlClCSl9g263UyMQairVyOXA/JRV+1IvMWpeHral\nTO2msQ2T5xrK/FzERoAdyNTP0kL88PZvVLxel/hZZD1wdMHxHHBDxeOWkRdwzgDOow/9+Jpkzk2v\nPldmtiu+d/9M51wTk1OJyBAIN+1olM8qYiOKKhwj6hsCk7/goy/KSkFKE9cUjhOdf5HrdC6wbnc+\nvumqyGmu01kSjvEe4KbE+pXAORVfU09lFPrX3Jhzirp9Y6K+IQeF4x9fdvu6tWs5ZVHkUfyw+A/X\n2DfreOeGflCXA3+FD0IGYeIz1m91798KdEREYsLNK9lhduJG0sL1zABeCk+XAH+CD57uyNxpq7mu\n0+nmdO6tG8DVKqPwWlZRrrN23YBnLvCXQFGutInt84b5ZwmvZQX5Q+bTnAd83nU6mzPKMWlLOH5W\nx991wH6u03mxh8CrF7XKr4669281XYmIxPR7aH0V4ca1IrZofni+ByWbkMp27q3SjNVDGZWdUDG6\ntjr/ie9NtZv8PjXOAcVpTJKi9+Tz8bmL8OWY198qa3QTYfls4K0FQ//7oalZ0vtOw8tFRBL6PbS+\njMR/53H74kcsfRrf9JGs+ZjU4dm63Q41h4TnqVlGVeYOugSfmiLv2tNUHbZfdl6YpCqvJbMTeniP\n7q55DfFrKeq31aShHF2VRTU6IiJDpuTEfPOBkynOVj+QjPQllQ1C1uJnND6QrbVG8+hPHrYqkwnG\nVQmoijqh93ovXsVgZyIeytFVWVSjIyIyfMqOqlqPDwbyOjwPKiN9GUUTHUbOjr2GbrTQut1z8LVc\nRbVYVa7pseJNUpWZtPFxfDB6Z1bNR6i5u6rmNcQn5xtUbqmJPkYDOl/PVKMjIjJ8StfClJiHaFAZ\n6QuVzG6+KMxFlLZ/3lQC8RqGtCBoyuHo4XWXnMPpTNfpfK8gyFkKzKlzCeFn1HxUdfLMOueb1Mdo\nVCjQEREZPo3VwjSYV6wROcHKWuCkMAdR0f4HUq0jdF9ed4XAa4oGOg8nJzmMv89V3FC8CeCvcST6\n5CRpeLmIyJApMXS58hwwQzpsvuc5hjKOfTNb59E5gT6/7jqvJXQSLzNFQCQaSn4evrkt9RzW7V4M\nXFbhuB+gXNPZJWGCxNZoHp2SFOiIyChoevLCcMy+BRfDJB7ouE7HhvF1V5j0Me6Eove85Pw+6/FT\nFIAvlyU52/c8sWJT6t6/1RlZRGQINZTdPXnM1ofND8jEDTvUnCwf1KR2FVTt/L24zHseOmNnddqO\nfAX46/D7FnzA9ZGMQ/YrQezAqI+OiMiQGqbJC0dFqAk7JrboDnKSfraoaufhb5c9cE7focjG2O/b\n4D9beZ22+5EgdmDUdCUiImOh6VQX/ZZzvXG1m45iTXbvAt5L+qzUZfvoDCzVQxalgBARkWmrH6ku\n+i1W87Iha5Pws9Zop7DPLOBctvbJSSpb0zXICQkbpUBHRETGQVHuqXiqi6ERgp298CkvHk+s7mkG\n4pJD2I8qebhBTCjZF+qMLCIi42CYUl1UEmpeFlq3+wmaHR3WRP6r+OzLI0mBjoiIjINhSnVRSx9G\nxfUa1I1U8s4saroSEZFxMDSpLoZIr0HdSCXvzKJAR0RERt6wpboYEnXyXy1kzKYy0PByEREZG8OW\n6qJtOTNsZ2l9GHkWpYAoSYGOiMh4G8aUD23KCP6ShibVQxYFOiUp0BERkekmBH8X4RN+JlNDDOWE\nikmaMFBERERSuU5nc8g+fgJTU0OMRafjLKrRERERmUZGtWlvpJuuzOxs4MPAHOAnwF865/41Y9sF\nwNcSizc553YoeS4FOiIiIiNmZJuuzOwU4HPApcDv4gOdZWY2O2e3p/CRaPR4Rb+vU0REREZP64EO\ncD5wtXPua865nwLvA54F/jxnH+ecWx17rBnIlYqIiMhIaTXQMbPtgMOB26Nlzrkt4fkRObvubGaP\nmNmjZnaLmb025xzbm9mu0QPYpanrFxERkeHWdo3OHsAMIFkjswbfXyfNL/C1Pe8G3ot/DT8ws/0z\ntr8Q36YXPVb2eM0iIiIyItoOdLIYGVNWO+fucs5d45y7zzl3J/AeYB1wZsaxPgnMjD16zeQqIiIi\nI6Lt7OXrgc3AXonls5lay5PKOfeimf0YeGXG+k3Apui5WZkZsEVERGQctFqj45x7AbgXmBctM7Nt\nwvO7yhzDzGYAr6P3LK0iIiIyZtqu0QE/tPwaM7sH+FfgXGAnwlw5ZnYN8Gvn3IXh+SXAD4EHgd3w\n8+8cCHxl4FcuIiIiQ631QMc59w9mtic+/8Yc4D7g2NiQ8QOALbFddgeuDtv+Bl8j9PthaLqIiIjI\nhKGYGXmQYjMr7gdsbPlyREREpJxd8COnK82M3HqNTguieXQ0zFxERGT07ILPkFDKdKzRMWAf+lOb\nE0Wbqi0aPJV9u1T+7VHZt0dlP3i7AI+5CsHLtKvRCYWTTFHfiNjQ9Y1KGDpYKvt2qfzbo7Jvj8q+\nFZXLeVgnDBQRERHpmQIdERERGVsKdJq1CbiU2EzMMjAq+3ap/Nujsm+Pyn4ETLvOyCIiIjJ9qEZH\nRERExpYCHRERERlbCnRERERkbCnQERERkbGlQKchZna2ma0ws+fN7G4ze3Pb1zRqzOxCM/s3M9to\nZmvN7Ftm9qrENjuY2RfMbIOZPW1mN5nZXoltDjCzW83s2XCcT5vZtoltOmb2IzPbZGYPmtmCAbzE\nkRHeC2dmV8SWqez7xMz2NbNrQ9k+Z2b3m9kbY+vNzC4zs1Vh/e1m9p8Sx5hlZteZ2VNm9oSZfdXM\ndk5s83ozWx6+px41s48M6jUOKzObYWYLzezhULYPmdnFFpsNUOU/4pxzevT4AE7BDy/8M+C3gS/j\nM6vPbvvaRukB3AYsAF4LvAG4FXgE2Cm2zReB/wccBRwO3AX8S2z9DOB+4H8DhwF/BKwDPhHb5iDg\nGeCzwGuADwAvAce0XQbD8ADeBDwM/AS4QmXf9/LeHVgBfA14cyijdwCHxLa5AHgC+GPg9cAtwK+A\nHWLbfBe4D3gL8AfAfwDXx9bvCqwGrg1/Y6cCzwJntl0GLZf/RcB64J3AgcCJ+HQOH1T5j8ej9QsY\nhwdwN3BV7Pk2+DQTf932tY3yA9gTcMDbwvOZwAvAibFtXh22+b3w/I+AzcBesW3eh89Yv114fjnw\nQOJcNwC3tf2a234AOwO/BP4Q6EaBjsq+r2X+KWB5znoDVgEfii2bCTwPnBqevya8F2+MbXMssAXY\nJzx/P/B49F7Ezv3ztsug5fL/DvDVxLKbgGtV/uPxUNNVj8xsO/x/t7dHy5xzW8LzI9q6rjExM/x8\nPPw8HHgZk8v65/hahqisjwDud86tiR1nGf6/qdfGtrmdyZah9wvgC8Ctzrlk+ajs++ddwD1mdmNo\n7vuxmZ0RW38QMIfJZf8k/h+seNk/4Zy7J7bf7fgb7Vti23zfOfdCbJtlwKvMbPdGX9Fo+QEwz8wO\nBTCzN+BrZL4b1qv8R5wCnd7tga+yX5NYvgb/xyE1mNk2wBX4ppEHwuI5wAvOuScSm8fLeg7p7wUl\nttnVzHbs9dpHlZmdCvwucGHKapV9/xyM/2//P4BjgC8Bf2tm/zWsj8ou7ztmDrA2vtI59xL+n4Qq\n78909Cl8reLPzexF4Mf4mszrwnqV/4ibdtnLB8jwVZlSzxeA1+H/sypStqzztrES24wtM9sfuBJ4\nh3Pu+Sq7orLv1TbAPc65i8LzH5vZa/HBzzU5+xm+xiBP0fsz3cse4GTgdOA04P/i+5ddYWaPOee+\nkbOfyn9EqEand+sJ/RISy2czNXqXEszsKuA4YK5zbmVs1WpgOzPbLbFLvKxXM/W9iJ7nbTMbeKri\nTX6cHI4vg3vN7CUzewl4O/DB8PsaVPb9sgr4aWLZz4ADwu+rw8+875jV4fmEMNptd4rLHqb3d9Wn\ngU85525wzt3vnPufwGK21myq/EecAp0ehfbWe4F50bLQ7DIPPypFSgpDOK8CjgeOcs49nNjkXuBF\nJpf1ofgbQlTWdwG/Y2bxL52jgafYejO5K36M2DbT+f36Z+B38P/NRo97gOtiv6vs++NfgFcllh2K\nH3EIfgTcaiaX/a74vh/xst/NzA6PHeMo/Hf83bFt3mZmL4ttczTwC+fcbxp4HaPq5UytmdnM1vuj\nyn/Utd0behwebB1e/qf43vd/jx9evlfb1zZKD+Dv8EM4345vs44eO8a2+SL+BjAXXwvxA+AHsfXR\nEOdl+CHqx+DbzpNDnJ8FFuFHDp3FNB/inPF+dJk6vFxl33w5vwkfRF4EvBLfhPIMcHpsmwvCd8q7\n8AHpt0gf3vwj/BD1t+JHz8WHN8/E37CvwXcOPyWcZ1oPbwa+Dqxk6/Dy4/HTIlyu8h+PR+sXMC4P\n/Hwgj+ADnruBt7R9TaP2wLdTpz0WxLbZAd9/5/HwJXEzMCdxnFcA/yvcUNcBnwG2TWwzF9/pcBPw\nUPwcekyUUTLQUdn3r6yPwweJz+Obrc5IrDfgsnCjfB4/oufQxDazgOvxc8A8CfwPYOfENm8Alodj\nrAQuaPu1t/0AdsEPfHgEeC58Jj/G5GHgKv8RflgofBEREZGxoz46IiIiMrYU6IiIiMjYUqAjIiIi\nY0uBjoiIiIwtBToiIiIythToiIiIyNhSoCMiIiJjS4GOiExrZrbAzJJZ2UVkTCjQEZGhYGZfNzMX\ne2wws9vM7PUVjvE3ZnZfP69TREaLAh0RGSa3AXuHxzx8HqzvtHpFIjLSFOiIyDDZ5JxbHR73AZcD\n+5vZngBmdrmZ/dLMnjWzX5nZwigbtJktAD4KvCFWK7QgrNvNzP7ezNaY2fNm9oCZHRc/sZkdY2Y/\nM7OnQ03S3oN84SLSH9u2fQEiImnMbGfgdOBBYENYvBFYADyGzyJ9dVi2CPgH4HXAscAfhu2fNLNt\n8JmldwHei0/a+NvA5tjpXg58CPgTYAtwLT4h6el9eXEiMjAKdERkmBxnZk+H33cCVgHHOee2ADjn\nPhbbdoWZfQY4FVjknHsu7PuSc251tJGZvQN4M/Aa59wvw+JfJc77MuB9zrmHwj5XAZc0/NpEpAUK\ndERkmNwBvD/8Pgs4C/iumb3ZOfeImZ0CfBA4BNgZ/x32VMExDwNWxoKcNM9GQU6wCphd5wWIyHBR\noCMiw+QZ59yD0RMzuxd4EjjDzG4FrsP3w1kWlp8K/FXBMZ8rcd4XE88dYGUvWkSGlwIdERlmDt9n\nZkfg94FHnHMfj1aa2SsS278AzEgs+3dgPzM7tKBWR0TGkAIdERkm25vZnPD77sAH8E1U/wTsChxg\nZqcC/wa8Ezg+sf8K4CAzOwxYCWx0zt1pZt8HbjKz8/Gdm18NOOfcbf1+QSLSLg0vF5Fhciy+f8wq\n4G7gTcBJzrmuc+7bwGLgKuA+fA3PwsT+N+Hn4rkDWAfMD8tPwAdHS4Cf4kdpJWt+RGQMmXOu7WsQ\nERER6QvV6IiIiMjYUqAjIiIiY0uBjoiIiIwtBToiIiIythToiIiIyNhSoCMiIiJjS4GOiIiIjC0F\nOiIiIjK2FOiIiIjI2FKgIyIiImNLgY6IiIiMLQU6IiIiMrb+Pz3EdKbWNyLWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename='t2_dp/loss.png') \n",
    "# with 15 epochs, the training error is not lower enough. Now we want the model to be overfit a little more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss_batch          training_loss_batch       validation_loss_out_char\r\n",
      "batch_loss_out_char       training_loss_out_char    validation_loss_time\r\n",
      "batch_loss_time           training_loss_time        validation_loss_total\r\n",
      "batch_loss_total          training_loss_total\r\n",
      "summary.yml               validation_loss_batch\r\n"
     ]
    }
   ],
   "source": [
    "%ls t2_dp/log/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%pycat t2_dp/log/summary.yml\n",
    "# {batches: 8790, epochs: 21, samples: 281028, sessions: 2, version: 2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### What summary.yml is tell us?\n",
    "- batches: 8790 == so far we have ran 8790 batches in training????\n",
    "- epochs: 21 == we have ran 21 epochs in traning\n",
    "- samples: 281028 == no idea what it means????\n",
    "- sessions: 2   == no idea what it means???? \n",
    "- version: 2  == no idea what it means????\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-03-05 22:08:42,957 kur.kurfile:699]\u001b[0m Parsing source: char_rrn_demo_dp_fluid.yaml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:08:42,961 kur.kurfile:699]\u001b[0m Parsing source: char_rnn_demo_dp_defaults.yaml, included by char_rrn_demo_dp_fluid.yaml.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:08:42,971 kur.kurfile:82]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:08:43,207 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:08:43,207 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:08:43,208 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:08:44,276 kur.backend.keras_backend:195]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:08:44,276 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:08:44,276 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:08:44,276 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:08:46,508 kur.model.model:284]\u001b[0m Model inputs:  in_seq\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:08:46,508 kur.model.model:285]\u001b[0m Model outputs: out_char\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:08:49,883 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "Evaluating: 100%|██████████████████████| 831/831 [00:00<00:00, 1462.52samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-05 22:08:50,508 kur.model.hooks.output_hook:40]\u001b[0m Saving model output as pickle: t2_dp/output.pkl\u001b[0m\n",
      "CPU times: user 173 ms, sys: 186 ms, total: 359 ms\n",
      "Wall time: 9.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kur -v evaluate char_rrn_demo_dp_fluid.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting view_outputs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile view_outputs.py\n",
    "\n",
    "\"\"\"\n",
    "Copyright 2016 Deepgram\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import view_data\n",
    "from vocab import *\n",
    "\n",
    "\n",
    "if len(sys.argv) < 2:\n",
    "    pickle_fname = 't2_dp/output.pkl'\n",
    "else:\n",
    "    pickle_fname = sys.argv[1]\n",
    "\n",
    "with open(pickle_fname, 'rb') as infile:\n",
    "    prediction_data = pickle.load(infile)\n",
    "\n",
    "data = view_data.get_data('evaluate')\n",
    "\n",
    "batch_size = len(prediction_data['truth']['out_char'])\n",
    "\n",
    "for j in range(10):\n",
    "    predicted_char = int_to_char[np.argmax(prediction_data['result']['out_char'][j])]\n",
    "    correct_char = int_to_char[np.argmax(data['out_char'][j])]\n",
    "    print(\n",
    "        '\"%s\" --> \"%s\"' % (\n",
    "            ''.join([\n",
    "                int_to_char[np.argmax(_)]\n",
    "                for _ in data['in_seq'][j]\n",
    "            ]),\n",
    "            predicted_char\n",
    "        )\n",
    "    )\n",
    "    if predicted_char == correct_char:\n",
    "        print((' ' * (seq_len + 5)) + 'CORRECT')\n",
    "    else:\n",
    "        print((' ' * (seq_len + 5)) + 'INCORRECT (%s)' % correct_char)\n",
    "\n",
    "accuracy = sum(\n",
    "    [\n",
    "        int(\n",
    "            np.argmax(prediction_data['result']['out_char'][i]) == np.argmax(prediction_data['truth']['out_char'][i])\n",
    "        )\n",
    "        for i in range(batch_size)\n",
    "    ]\n",
    ") / float(len(prediction_data['truth']['out_char']))\n",
    "\n",
    "print('accuracy = %s' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ng your time with me. mr. bing\" --> \"l\"\r\n",
      "                                   CORRECT\r\n",
      "\"g your time with me. mr. bingl\" --> \"e\"\r\n",
      "                                   CORRECT\r\n",
      "\" your time with me. mr. bingle\" --> \"y\"\r\n",
      "                                   CORRECT\r\n",
      "\"your time with me. mr. bingley\" --> \" \"\r\n",
      "                                   CORRECT\r\n",
      "\"our time with me. mr. bingley \" --> \"w\"\r\n",
      "                                   INCORRECT (f)\r\n",
      "\"ur time with me. mr. bingley f\" --> \"o\"\r\n",
      "                                   CORRECT\r\n",
      "\"r time with me. mr. bingley fo\" --> \"r\"\r\n",
      "                                   INCORRECT (l)\r\n",
      "\" time with me. mr. bingley fol\" --> \"l\"\r\n",
      "                                   CORRECT\r\n",
      "\"time with me. mr. bingley foll\" --> \"o\"\r\n",
      "                                   CORRECT\r\n",
      "\"ime with me. mr. bingley follo\" --> \"w\"\r\n",
      "                                   CORRECT\r\n",
      "accuracy = 0.5270758122743683\r\n"
     ]
    }
   ],
   "source": [
    "!python view_outputs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-03-05 22:09:03,629 kur.kurfile:699]\u001b[0m Parsing source: char_rrn_demo_dp_fluid.yaml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:09:03,632 kur.kurfile:699]\u001b[0m Parsing source: char_rnn_demo_dp_defaults.yaml, included by char_rrn_demo_dp_fluid.yaml.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:09:03,642 kur.kurfile:82]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:09:03,885 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:09:03,885 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:09:03,885 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:09:05,031 kur.backend.keras_backend:195]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:09:05,032 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:09:05,032 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:09:05,032 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:09:06,459 kur.model.model:284]\u001b[0m Model inputs:  in_seq\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:09:06,459 kur.model.model:285]\u001b[0m Model outputs: out_char\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:09:09,948 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "Testing, loss=1.703: 100%|█████████████| 831/831 [00:00<00:00, 1072.87samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-05 22:09:10,788 kur.model.executor:197]\u001b[0m Test loss: 1.703\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!kur -v test char_rrn_demo_dp_fluid.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Improvement**\n",
    "- divergence is significantly reduced\n",
    "- gradually converging\n",
    "\n",
    "**Unsatistifed**\n",
    "- original kurfile can achieve loss of 1.65 at epoch 3\n",
    "- dropout version loss is \n",
    "- best loss with dropout version is 1.60 at epoch 12\n",
    "\n",
    "**What more can be done?**\n",
    "- should learning rate can adjusted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Try to write dlnd_character model (tensorflow) in kur\n",
    "1. I am not sure I [understand the model in tensorflow correct](https://hyp.is/DzyoQAChEeeLJ0dOqLuF9A/nbviewer.jupyter.org/github/udacity/deep-learning/blob/master/intro-to-rnns/Anna%20KaRNNa.ipynb). I implemented below anyway, could you verify it for me?\n",
    "- Assume I understand the tensorflow code properly, **does kur's LSTM and dropout operate similarly as Tensorflow's LSTM and dropout**? \n",
    "- In TF's doc, it says its `tf.contrib.rnn.BasicLSTMCell` is built based on [this paper](https://hyp.is/lHmEDACbEeeB_C_Ua4oMgA/arxiv.org/pdf/1409.2329.pdf)\n",
    "- I see Kur does have gradient clipping, to write equivalence code to the [tensorflow code](https://hyp.is/ITh66ACqEee3Jd9Zbpvh4A/nbviewer.jupyter.org/github/udacity/deep-learning/blob/master/intro-to-rnns/Anna%20KaRNNa.ipynb), we shall use `clip: norm: grad_clip`, right?\n",
    "- [tips on overfitting and underfitting](https://hyp.is/VavmwgCtEeeLXLcVygsOyw/nbviewer.jupyter.org/github/udacity/deep-learning/blob/master/intro-to-rnns/Anna%20KaRNNa.ipynb) from karpathy post\n",
    "- [tips on balancing data size and model complexity](https://hyp.is/-gOR4gCuEeemKK-35u3Qnw/nbviewer.jupyter.org/github/udacity/deep-learning/blob/master/intro-to-rnns/Anna%20KaRNNa.ipynb)\n",
    "- [how to find the best weights](https://hyp.is/0SE0QACvEeeDd8dKyAHb-w/nbviewer.jupyter.org/github/udacity/deep-learning/blob/master/intro-to-rnns/Anna%20KaRNNa.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### dlnd_character model parameter values\n",
    "\n",
    "- batch_size = 100    \n",
    "- num_steps = 100     \n",
    "- lstm_size = 512    \n",
    "- num_layers = 2    \n",
    "- learning_rate = 0.001    \n",
    "- keep_prob = 0.5    \n",
    "\n",
    "**Where can I find information about batch_size of original kur character example**?\n",
    "- `batch_size` is not given in the original kurfile\n",
    "- how do I know the default `batch_size`?\n",
    "\n",
    "**To match dlnd parameters above with kur parameters**\n",
    "```\n",
    "- batch_size      = 100     vs     = 32 is default for kurfile, found it in `-vv` mode, we can set it freely \n",
    "- num_steps       = 100     vs     = 30 = seq_len = how many characters are used to predict the next character??\n",
    "                                   = to set seq_len to 100, we have to go back to make_data stage, right????????\n",
    "- lstm_size       = 512     vs     = 128 = rnn.size = how should I visualize `lstm_size` in my mind?\n",
    "- num_layers      = 2       vs     = 3 = rnn.depth\n",
    "- learning_rate   = 0.001   vs     = optimizer is missing in kurfile, adam optimizer with lr=0.001 used\n",
    "- keep_prob       = 0.5     vs     = dropout \n",
    "```\n",
    "- no fully connected (fc) layer specified in dlnd_example tensorflow code, does tensorflow automatically add fc_layer?\n",
    "- is fc_layer or dense layer required in RNN model????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" width=\"800\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in Karpathy's post, there are 5 types of RNN models\n",
    "Image(width=800, height=500, url = \"http://karpathy.github.io/assets/rnn/diags.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://nbviewer.jupyter.org/github/udacity/deep-learning/blob/master/intro-to-rnns/assets/charseq.jpeg\" width=\"500\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##QUESTION: The following graph is the fifth type of RNN model(see above), not what we want for dlnd_project, right?\n",
    "from IPython.display import Image\n",
    "Image(width = 500, height = 300, url='http://nbviewer.jupyter.org/github/udacity/deep-learning/blob/master/intro-to-rnns/assets/charseq.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://lh3.googleusercontent.com/-guWd9n2kabBmfhG3QSbd1PGBExjngc6uzWAKAuX-k-ML02o5Q_QyXyV-BP5F0KqVKbKiVBdtrt9_OsCJpaWseAPiF-rxhNqvmMaVmTYeTmQammdG3qoImzvjygsMJMHLN17IxbHt8u677szipeNBDQbOr_qviR6gLuMQMUwh6DydLuu8lYDCABE9M1QpBSbk6cHKeiFMI0FuqHfPNsjSPGgGuXYnVm7F6Mi6jEh2ewqNzmJjm6p106ClRlXxPLs2nAThCJKLQbnYanonEK-TSqay5XECSRhU_4YpAKNBpO_F32y-R1iUPfHuJv9JAFMJWPPfRDTbQ39mROyzpra3bp1fRL03OzS7zRrK0pWOXEKy4iDn-KyUp9ZAqCy__R2mCPZWaUMz3_K3G2jb0H0_IQHBxPH-hzrs_vcqIh1Xn7NsUt__6sM0ncjyiimAxhYqcx3eQs5kQBuzQA1HKGBjKPpglHEZvZFiSfnFVlm7qEw_eS9IhgXhjBY5PBY0B5NrRJgZtYpf4mH_O1JZd2gPlVf_klI6sGQdxiYOYjWQiqwy5LtPDsnWz8qN730TlS3eqxNpee0-GK9lPRbNAJFHQAIOncg4chHVoIKp-kyS0KAAwosWhlP=w1788-h1264-no\" width=\"1000\" height=\"800\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### QUESTION: we want type 4 model for this project, I draw the model in the graph below, please verify it for me\n",
    "Image(retina = True, width = 1000, height=800, url=\"https://lh3.googleusercontent.com/-guWd9n2kabBmfhG3QSbd1PGBExjngc6uzWAKAuX-k-ML02o5Q_QyXyV-BP5F0KqVKbKiVBdtrt9_OsCJpaWseAPiF-rxhNqvmMaVmTYeTmQammdG3qoImzvjygsMJMHLN17IxbHt8u677szipeNBDQbOr_qviR6gLuMQMUwh6DydLuu8lYDCABE9M1QpBSbk6cHKeiFMI0FuqHfPNsjSPGgGuXYnVm7F6Mi6jEh2ewqNzmJjm6p106ClRlXxPLs2nAThCJKLQbnYanonEK-TSqay5XECSRhU_4YpAKNBpO_F32y-R1iUPfHuJv9JAFMJWPPfRDTbQ39mROyzpra3bp1fRL03OzS7zRrK0pWOXEKy4iDn-KyUp9ZAqCy__R2mCPZWaUMz3_K3G2jb0H0_IQHBxPH-hzrs_vcqIh1Xn7NsUt__6sM0ncjyiimAxhYqcx3eQs5kQBuzQA1HKGBjKPpglHEZvZFiSfnFVlm7qEw_eS9IhgXhjBY5PBY0B5NrRJgZtYpf4mH_O1JZd2gPlVf_klI6sGQdxiYOYjWQiqwy5LtPDsnWz8qN730TlS3eqxNpee0-GK9lPRbNAJFHQAIOncg4chHVoIKp-kyS0KAAwosWhlP=w1788-h1264-no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing char_rnn_demo_dlnd_defaults.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile char_rnn_demo_dlnd_defaults.yaml\n",
    "\n",
    "---\n",
    "\n",
    "settings:\n",
    "\n",
    "\n",
    "  vocab:                                         \n",
    "    size: 30                          # This cannot be changed, it is fixed with dataset           \n",
    "                \n",
    "  \n",
    "\n",
    "# QUESTION: what cause the following error, given the model build fine. see `build` in the next cell               \n",
    "# we get the following error meassage: \n",
    "# Traceback (most recent call last):\n",
    "#   File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/executor.py\", line 224, in train\n",
    "#     **kwargs\n",
    "#   File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/executor.py\", line 648, in wrapped_train\n",
    "#     raise ValueError('Model loss is NaN.')\n",
    "# ValueError: Model loss is NaN.\n",
    "# \n",
    "# During handling of the above exception, another exception occurred:\n",
    "# \n",
    "# Traceback (most recent call last):\n",
    "#   File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/bin/kur\", line 11, in <module>\n",
    "#     load_entry_point('kur', 'console_scripts', 'kur')()\n",
    "#   File \"/Users/Natsume/Downloads/kur_road/kur/kur/__main__.py\", line 382, in main\n",
    "#     sys.exit(args.func(args) or 0)\n",
    "#   File \"/Users/Natsume/Downloads/kur_road/kur/kur/__main__.py\", line 62, in train\n",
    "#     func(step=args.step)\n",
    "#   File \"/Users/Natsume/Downloads/kur_road/kur/kur/kurfile.py\", line 371, in func\n",
    "#     return trainer.train(**defaults)\n",
    "#   File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/executor.py\", line 246, in train\n",
    "#     info={'Reason' : reason}\n",
    "#   File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/hooks/plot_hook.py\", line 123, in notify\n",
    "#     vbatch = numpy.arange(1, len(vloss)+1)\n",
    "# TypeError: object of type 'NoneType' has no len()\n",
    "# CPU times: user 511 ms, sys: 504 ms, total: 1.02 s\n",
    "# Wall time: 31.6 s\"\"\"    \n",
    "\n",
    "  rnn:\n",
    "    size: 512                                    # num_neurons of a rnn/lstm layer\n",
    "    depth: 2                                     # num_rnn_layers for this RNN model\n",
    "        \n",
    "\n",
    "\n",
    "model:\n",
    "  - input: in_seq\n",
    "\n",
    "\n",
    "  - recurrent:\n",
    "      size: \"{{ rnn.size }}\"\n",
    "      type: lstm\n",
    "      sequence: True                             # yes, meaning return the whole sequence of 30 characters??\n",
    "      bidirectional: no\n",
    "  - batch_normalization\n",
    "  - dropout: \"{{drop_neurons}}\"\n",
    "        \n",
    "  - recurrent:\n",
    "      size: \"{{ rnn.size }}\"\n",
    "      type: lstm\n",
    "      sequence: False                     # no, meaning only return only 1 character of the whole 30 char sequence??\n",
    "      bidirectional: no\n",
    "  - batch_normalization\n",
    "  - dropout: \"{{drop_neurons}}\"\n",
    "\n",
    "\n",
    "  - dense: \"{{ vocab.size }}\"                   # now it is like 30 class-classification problem, \n",
    "                                                # that's why we need 30 neurons here, right? \n",
    "\n",
    "  - activation: softmax\n",
    "\n",
    "  - output: out_char                               # make a name of output layer\n",
    "           \n",
    "\n",
    "loss:\n",
    "  - target: out_char\n",
    "    name: categorical_crossentropy\n",
    "\n",
    "train:\n",
    "  data:\n",
    "    - jsonl: data/train.jsonl\n",
    "  epochs: \"{{ num_epochs|default(5) }}\"     \n",
    "  weights:\n",
    "    initial: t3_dlnd/best.w.kur\n",
    "    best: t3_dlnd/best.w.kur\n",
    "    last: t3_dlnd/last.w.kur\n",
    "  log: t3_dlnd/log\n",
    "  hooks:                                   \n",
    "    - plot: t3_dlnd/loss.png\n",
    "        \n",
    "                                # QUESTION: by default optimizer is Adam, lr = 0.001\n",
    "                                # given 'grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)`\n",
    "                                # I should set clip as follows, right?\n",
    "  clip:                                              \n",
    "    norm: \"{{grad_clip}}\"                            \n",
    "\n",
    "validate:\n",
    "  data:\n",
    "    - jsonl: data/validate.jsonl\n",
    "  weights: t3_dlnd/best.w.kur\n",
    "\n",
    "\n",
    "test:\n",
    "  data:\n",
    "    - jsonl: data/test.jsonl\n",
    "  weights: t3_dlnd/best.w.kur\n",
    "\n",
    "\n",
    "evaluate:\n",
    "  data:\n",
    "    - jsonl: data/evaluate.jsonl\n",
    "  weights: t3_dlnd/best.w.kur\n",
    "\n",
    "  destination: t3_dlnd/output.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting char_rrn_demo_dlnd_fluid.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile char_rrn_demo_dlnd_fluid.yaml\n",
    "\n",
    "---\n",
    "settings: \n",
    "#   num_epochs: 15                    # leave it empty means inf number of epochs\n",
    "                                 # so to use default value, just comment this line out\n",
    "  drop_neurons: 0.5\n",
    "\n",
    "\n",
    "include: char_rnn_demo_dlnd_defaults.yaml\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-03-05 22:26:41,982 kur.kurfile:699]\u001b[0m Parsing source: char_rrn_demo_dlnd_fluid.yaml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:26:41,986 kur.kurfile:699]\u001b[0m Parsing source: char_rnn_demo_dlnd_defaults.yaml, included by char_rrn_demo_dlnd_fluid.yaml.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:26:41,999 kur.kurfile:82]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:41,999 kur.kurfile:784]\u001b[0m Parsing Kurfile section: settings\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,000 kur.kurfile:784]\u001b[0m Parsing Kurfile section: train\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,004 kur.kurfile:784]\u001b[0m Parsing Kurfile section: validate\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,005 kur.kurfile:784]\u001b[0m Parsing Kurfile section: test\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,006 kur.kurfile:784]\u001b[0m Parsing Kurfile section: evaluate\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,008 kur.containers.layers.placeholder:63]\u001b[0m Using short-hand name for placeholder: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,009 kur.containers.layers.placeholder:97]\u001b[0m Placeholder \"in_seq\" has a deferred shape.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,016 kur.containers.layers.output:50]\u001b[0m Using short-hand name for output: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,016 kur.kurfile:784]\u001b[0m Parsing Kurfile section: loss\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:26:42,017 kur.loggers.binary_logger:71]\u001b[0m Loading log data: t3_dlnd/log\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,018 kur.loggers.binary_logger:158]\u001b[0m Reading logger summary.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,019 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: training_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,019 kur.loggers.binary_logger:192]\u001b[0m No such log column exists: t3_dlnd/log/training_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,019 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: training_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,019 kur.loggers.binary_logger:192]\u001b[0m No such log column exists: t3_dlnd/log/training_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,019 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: training_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,019 kur.loggers.binary_logger:192]\u001b[0m No such log column exists: t3_dlnd/log/training_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,019 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,020 kur.loggers.binary_logger:192]\u001b[0m No such log column exists: t3_dlnd/log/validation_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,020 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,020 kur.loggers.binary_logger:192]\u001b[0m No such log column exists: t3_dlnd/log/validation_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,020 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:42,020 kur.loggers.binary_logger:192]\u001b[0m No such log column exists: t3_dlnd/log/validation_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:45,895 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 32\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:46,263 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 32\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:46,264 kur.backend.backend:187]\u001b[0m Using backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:26:46,264 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:26:46,264 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:26:46,265 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:46,265 kur.backend.keras_backend:124]\u001b[0m Using the system-default Keras backend.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:46,265 kur.backend.keras_backend:189]\u001b[0m Overriding environmental variables: {'TF_CPP_MIN_LOG_LEVEL': '1', 'THEANO_FLAGS': None, 'KERAS_BACKEND': None}\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:26:47,358 kur.backend.keras_backend:195]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:26:47,359 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:26:47,359 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,359 kur.model.model:272]\u001b[0m Assembled Node: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,359 kur.model.model:274]\u001b[0m   Uses: \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,359 kur.model.model:276]\u001b[0m   Used by: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,359 kur.model.model:277]\u001b[0m   Aliases: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,359 kur.model.model:272]\u001b[0m Assembled Node: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,359 kur.model.model:274]\u001b[0m   Uses: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,359 kur.model.model:276]\u001b[0m   Used by: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,359 kur.model.model:277]\u001b[0m   Aliases: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,359 kur.model.model:272]\u001b[0m Assembled Node: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,359 kur.model.model:274]\u001b[0m   Uses: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,359 kur.model.model:276]\u001b[0m   Used by: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,359 kur.model.model:277]\u001b[0m   Aliases: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,359 kur.model.model:272]\u001b[0m Assembled Node: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,359 kur.model.model:274]\u001b[0m   Uses: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:276]\u001b[0m   Used by: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:277]\u001b[0m   Aliases: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:272]\u001b[0m Assembled Node: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:274]\u001b[0m   Uses: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:276]\u001b[0m   Used by: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:277]\u001b[0m   Aliases: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:272]\u001b[0m Assembled Node: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:274]\u001b[0m   Uses: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:276]\u001b[0m   Used by: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:277]\u001b[0m   Aliases: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:272]\u001b[0m Assembled Node: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:274]\u001b[0m   Uses: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:276]\u001b[0m   Used by: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:277]\u001b[0m   Aliases: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:272]\u001b[0m Assembled Node: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:274]\u001b[0m   Uses: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:276]\u001b[0m   Used by: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:277]\u001b[0m   Aliases: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:272]\u001b[0m Assembled Node: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:274]\u001b[0m   Uses: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:276]\u001b[0m   Used by: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,360 kur.model.model:277]\u001b[0m   Aliases: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,361 kur.model.model:272]\u001b[0m Assembled Node: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,361 kur.model.model:274]\u001b[0m   Uses: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,361 kur.model.model:276]\u001b[0m   Used by: \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,361 kur.model.model:277]\u001b[0m   Aliases: out_char\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:26:47,361 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,361 kur.model.model:311]\u001b[0m Building node: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,361 kur.model.model:312]\u001b[0m   Aliases: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,361 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,361 kur.containers.layers.placeholder:117]\u001b[0m Creating placeholder for \"in_seq\" with data type \"float32\".\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,361 kur.model.model:125]\u001b[0m Trying to infer shape for input \"in_seq\"\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,361 kur.model.model:143]\u001b[0m Inferred shape for input \"in_seq\": (30, 30)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,361 kur.containers.layers.placeholder:127]\u001b[0m Inferred shape: (30, 30)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,366 kur.model.model:382]\u001b[0m   Value: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,366 kur.model.model:311]\u001b[0m Building node: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,366 kur.model.model:312]\u001b[0m   Aliases: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,366 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:47,366 kur.model.model:315]\u001b[0m   - in_seq: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:48,276 kur.model.model:382]\u001b[0m   Value: DimShuffle{1,0,2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:48,276 kur.model.model:311]\u001b[0m Building node: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:48,276 kur.model.model:312]\u001b[0m   Aliases: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:48,276 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:48,277 kur.model.model:315]\u001b[0m   - ..recurrent.0: DimShuffle{1,0,2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:48,294 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:48,294 kur.model.model:311]\u001b[0m Building node: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:48,294 kur.model.model:312]\u001b[0m   Aliases: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:48,294 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:48,294 kur.model.model:315]\u001b[0m   - ..batch_normalization.0: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,007 kur.model.model:382]\u001b[0m   Value: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,007 kur.model.model:311]\u001b[0m Building node: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,007 kur.model.model:312]\u001b[0m   Aliases: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,007 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,007 kur.model.model:315]\u001b[0m   - ..dropout.0: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,169 kur.model.model:382]\u001b[0m   Value: Subtensor{int64}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,169 kur.model.model:311]\u001b[0m Building node: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,169 kur.model.model:312]\u001b[0m   Aliases: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,169 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,169 kur.model.model:315]\u001b[0m   - ..recurrent.1: Subtensor{int64}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,178 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,178 kur.model.model:311]\u001b[0m Building node: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,178 kur.model.model:312]\u001b[0m   Aliases: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,178 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,178 kur.model.model:315]\u001b[0m   - ..batch_normalization.1: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,228 kur.model.model:382]\u001b[0m   Value: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,228 kur.model.model:311]\u001b[0m Building node: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,229 kur.model.model:312]\u001b[0m   Aliases: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,229 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,229 kur.model.model:315]\u001b[0m   - ..dropout.1: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,231 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,231 kur.model.model:311]\u001b[0m Building node: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,231 kur.model.model:312]\u001b[0m   Aliases: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,231 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,231 kur.model.model:315]\u001b[0m   - ..dense.0: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,232 kur.model.model:382]\u001b[0m   Value: Softmax.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,232 kur.model.model:311]\u001b[0m Building node: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,232 kur.model.model:312]\u001b[0m   Aliases: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,232 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,232 kur.model.model:315]\u001b[0m   - ..activation.0: Softmax.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,232 kur.model.model:382]\u001b[0m   Value: Softmax.0\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:26:49,232 kur.model.model:284]\u001b[0m Model inputs:  in_seq\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:26:49,233 kur.model.model:285]\u001b[0m Model outputs: out_char\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:26:49,233 kur.kurfile:357]\u001b[0m Ignoring missing initial weights: t3_dlnd/best.w.kur. If this is undesireable, set \"must_exist\" to \"yes\" in the approriate \"weights\" section.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:26:49,233 kur.model.executor:315]\u001b[0m No historical training loss available from logs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:26:49,233 kur.model.executor:323]\u001b[0m No historical validation loss available from logs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:26:49,233 kur.model.executor:329]\u001b[0m No previous epochs.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,233 kur.model.executor:353]\u001b[0m Epoch handling mode: additional\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,233 kur.model.executor:101]\u001b[0m Recompiling the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:49,233 kur.backend.keras_backend:527]\u001b[0m Instantiating a Keras model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,019 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,019 kur.backend.keras_backend:538]\u001b[0m Layer (type)                     Output Shape          Param #     Connected to                     \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,019 kur.backend.keras_backend:538]\u001b[0m ====================================================================================================\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,019 kur.backend.keras_backend:538]\u001b[0m in_seq (InputLayer)              (None, 30, 30)        0                                            \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,019 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,019 kur.backend.keras_backend:538]\u001b[0m ..recurrent.0 (LSTM)             (None, 30, 512)       1112064     in_seq[0][0]                     \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,019 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,019 kur.backend.keras_backend:538]\u001b[0m ..batch_normalization.0 (BatchNo (None, 30, 512)       2048        ..recurrent.0[0][0]              \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,019 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,019 kur.backend.keras_backend:538]\u001b[0m ..dropout.0 (Dropout)            (None, 30, 512)       0           ..batch_normalization.0[0][0]    \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,019 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,019 kur.backend.keras_backend:538]\u001b[0m ..recurrent.1 (LSTM)             (None, 512)           2099200     ..dropout.0[0][0]                \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,019 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,019 kur.backend.keras_backend:538]\u001b[0m ..batch_normalization.1 (BatchNo (None, 512)           2048        ..recurrent.1[0][0]              \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,019 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,019 kur.backend.keras_backend:538]\u001b[0m ..dropout.1 (Dropout)            (None, 512)           0           ..batch_normalization.1[0][0]    \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,019 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,019 kur.backend.keras_backend:538]\u001b[0m ..dense.0 (Dense)                (None, 30)            15390       ..dropout.1[0][0]                \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,020 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,020 kur.backend.keras_backend:538]\u001b[0m ..activation.0 (Activation)      (None, 30)            0           ..dense.0[0][0]                  \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,020 kur.backend.keras_backend:538]\u001b[0m ====================================================================================================\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,020 kur.backend.keras_backend:538]\u001b[0m Total params: 3,230,750\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,020 kur.backend.keras_backend:538]\u001b[0m Trainable params: 3,228,702\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,020 kur.backend.keras_backend:538]\u001b[0m Non-trainable params: 2,048\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,020 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,020 kur.backend.keras_backend:538]\u001b[0m \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,020 kur.backend.keras_backend:576]\u001b[0m Assembling a training function from the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:26:50,025 kur.backend.keras_backend:509]\u001b[0m Adding additional inputs: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:11,477 kur.backend.keras_backend:599]\u001b[0m Additional inputs for log functions: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:11,477 kur.backend.keras_backend:616]\u001b[0m Expected input shapes: in_seq=(None, 30, 30), out_char=(None, None)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:11,477 kur.backend.keras_backend:634]\u001b[0m Compiled model: {'names': {'input': ['in_seq', 'out_char'], 'output': ['..activation.0', 'out_char']}, 'shapes': {'input': [(None, 30, 30), (None, None)]}, 'func': <keras.backend.theano_backend.Function object at 0x11cd4cc88>}\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:11,478 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:11,479 kur.providers.batch_provider:102]\u001b[0m Maximum number of batches set to: 1\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:27:11,561 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:11,562 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:11,562 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:11,933 kur.providers.provider:144]\u001b[0m Data source \"out_char\": entries=13300, shape=(30,)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:11,933 kur.providers.provider:144]\u001b[0m Data source \"in_seq\": entries=13300, shape=(30, 30)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:12,283 kur.model.hooks.plot_hook:73]\u001b[0m Plotting hook received training message.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:12,283 kur.model.hooks.plot_hook:80]\u001b[0m Plotting hook does not handle this status.\u001b[0m\n",
      "\n",
      "Epoch 1/5, loss=N/A:   0%|                       | 0/13300 [00:00<?, ?samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:27:12,295 kur.providers.shuffle_provider:184]\u001b[0m Shuffling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:12,809 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:12,810 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:12,810 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:12,810 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:12,813 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:13,571 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:13,571 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:13,573 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:13,573 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:13,574 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:13,577 kur.loggers.binary_logger:144]\u001b[0m Writing logger summary.\u001b[0m\n",
      "Epoch 1/5, loss=4.625:   0%|            | 32/13300 [00:01<08:52, 24.91samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:27:13,580 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:13,580 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:13,581 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:14,632 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/5, loss=nan:   0%|              | 64/13300 [00:02<08:22, 26.34samples/s]\u001b[1;31m[ERROR 2017-03-05 22:27:14,632 kur.model.executor:647]\u001b[0m Received NaN loss value for model output \"out_char\". Make sure that your inputs are all normalized and that the learning rate is not too high. Sometimes different algorithms/implementations work better than others, so you can try switching optimizers or backend.\u001b[0m\n",
      "\n",
      "\u001b[1;31m[ERROR 2017-03-05 22:27:14,634 kur.model.executor:227]\u001b[0m Exception raised during training.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/executor.py\", line 224, in train\n",
      "    **kwargs\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/executor.py\", line 648, in wrapped_train\n",
      "    raise ValueError('Model loss is NaN.')\n",
      "ValueError: Model loss is NaN.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:27:14,635 kur.model.executor:235]\u001b[0m Saving most recent weights: t3_dlnd/last.w.kur\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:14,635 kur.model.model:213]\u001b[0m Saving model weights to: t3_dlnd/last.w.kur\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:14,709 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:14,710 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:14,710 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:14,711 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:14,712 kur.loggers.binary_logger:144]\u001b[0m Writing logger summary.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:14,713 kur.model.hooks.plot_hook:73]\u001b[0m Plotting hook received training message.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:14,713 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: batch_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:14,714 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: batch_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:14,714 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: batch_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:14,714 kur.model.hooks.plot_hook:107]\u001b[0m Using per-batch training statistics for plotting.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:14,714 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:14,715 kur.loggers.binary_logger:192]\u001b[0m No such log column exists: t3_dlnd/log/validation_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:14,715 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:14,715 kur.loggers.binary_logger:192]\u001b[0m No such log column exists: t3_dlnd/log/validation_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:14,715 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:14,715 kur.loggers.binary_logger:192]\u001b[0m No such log column exists: t3_dlnd/log/validation_loss_time\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/executor.py\", line 224, in train\n",
      "    **kwargs\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/executor.py\", line 648, in wrapped_train\n",
      "    raise ValueError('Model loss is NaN.')\n",
      "ValueError: Model loss is NaN.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/bin/kur\", line 11, in <module>\n",
      "    load_entry_point('kur', 'console_scripts', 'kur')()\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/__main__.py\", line 382, in main\n",
      "    sys.exit(args.func(args) or 0)\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/__main__.py\", line 62, in train\n",
      "    func(step=args.step)\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/kurfile.py\", line 371, in func\n",
      "    return trainer.train(**defaults)\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/executor.py\", line 246, in train\n",
      "    info={'Reason' : reason}\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/hooks/plot_hook.py\", line 123, in notify\n",
      "    vbatch = numpy.arange(1, len(vloss)+1)\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "CPU times: user 655 ms, sys: 658 ms, total: 1.31 s\n",
      "Wall time: 36.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kur -vv train char_rrn_demo_dlnd_fluid.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The model above is built ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-03-05 22:27:24,931 kur.kurfile:699]\u001b[0m Parsing source: char_rrn_demo_dlnd_fluid.yaml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:27:24,935 kur.kurfile:699]\u001b[0m Parsing source: char_rnn_demo_dlnd_defaults.yaml, included by char_rrn_demo_dlnd_fluid.yaml.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:27:24,949 kur.kurfile:82]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:24,950 kur.kurfile:784]\u001b[0m Parsing Kurfile section: settings\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:24,950 kur.kurfile:784]\u001b[0m Parsing Kurfile section: train\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:24,956 kur.kurfile:784]\u001b[0m Parsing Kurfile section: validate\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:24,957 kur.kurfile:784]\u001b[0m Parsing Kurfile section: test\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:24,958 kur.kurfile:784]\u001b[0m Parsing Kurfile section: evaluate\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:24,962 kur.containers.layers.placeholder:63]\u001b[0m Using short-hand name for placeholder: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:24,963 kur.containers.layers.placeholder:97]\u001b[0m Placeholder \"in_seq\" has a deferred shape.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:24,972 kur.containers.layers.output:50]\u001b[0m Using short-hand name for output: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:24,972 kur.kurfile:784]\u001b[0m Parsing Kurfile section: loss\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:27:24,974 kur.__main__:96]\u001b[0m Trying to build a \"train\" model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:28,816 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 32\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:28,817 kur.backend.backend:187]\u001b[0m Using backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:27:28,817 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:27:28,817 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:27:28,817 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:28,817 kur.backend.keras_backend:124]\u001b[0m Using the system-default Keras backend.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:28,818 kur.backend.keras_backend:189]\u001b[0m Overriding environmental variables: {'KERAS_BACKEND': None, 'TF_CPP_MIN_LOG_LEVEL': '1', 'THEANO_FLAGS': None}\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:27:29,572 kur.backend.keras_backend:195]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:27:29,572 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:27:29,573 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,573 kur.model.model:272]\u001b[0m Assembled Node: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,573 kur.model.model:274]\u001b[0m   Uses: \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,573 kur.model.model:276]\u001b[0m   Used by: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,573 kur.model.model:277]\u001b[0m   Aliases: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,573 kur.model.model:272]\u001b[0m Assembled Node: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,573 kur.model.model:274]\u001b[0m   Uses: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,573 kur.model.model:276]\u001b[0m   Used by: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,573 kur.model.model:277]\u001b[0m   Aliases: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,574 kur.model.model:272]\u001b[0m Assembled Node: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,574 kur.model.model:274]\u001b[0m   Uses: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,574 kur.model.model:276]\u001b[0m   Used by: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,574 kur.model.model:277]\u001b[0m   Aliases: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,574 kur.model.model:272]\u001b[0m Assembled Node: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,574 kur.model.model:274]\u001b[0m   Uses: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,574 kur.model.model:276]\u001b[0m   Used by: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,574 kur.model.model:277]\u001b[0m   Aliases: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,574 kur.model.model:272]\u001b[0m Assembled Node: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,574 kur.model.model:274]\u001b[0m   Uses: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,574 kur.model.model:276]\u001b[0m   Used by: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,575 kur.model.model:277]\u001b[0m   Aliases: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,575 kur.model.model:272]\u001b[0m Assembled Node: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,575 kur.model.model:274]\u001b[0m   Uses: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,575 kur.model.model:276]\u001b[0m   Used by: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,575 kur.model.model:277]\u001b[0m   Aliases: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,575 kur.model.model:272]\u001b[0m Assembled Node: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,575 kur.model.model:274]\u001b[0m   Uses: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,575 kur.model.model:276]\u001b[0m   Used by: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,575 kur.model.model:277]\u001b[0m   Aliases: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,575 kur.model.model:272]\u001b[0m Assembled Node: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,575 kur.model.model:274]\u001b[0m   Uses: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,575 kur.model.model:276]\u001b[0m   Used by: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,575 kur.model.model:277]\u001b[0m   Aliases: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,575 kur.model.model:272]\u001b[0m Assembled Node: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,575 kur.model.model:274]\u001b[0m   Uses: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,575 kur.model.model:276]\u001b[0m   Used by: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,576 kur.model.model:277]\u001b[0m   Aliases: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,576 kur.model.model:272]\u001b[0m Assembled Node: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,576 kur.model.model:274]\u001b[0m   Uses: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,576 kur.model.model:276]\u001b[0m   Used by: \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,576 kur.model.model:277]\u001b[0m   Aliases: out_char\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:27:29,576 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,576 kur.model.model:311]\u001b[0m Building node: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,576 kur.model.model:312]\u001b[0m   Aliases: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,576 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,576 kur.containers.layers.placeholder:117]\u001b[0m Creating placeholder for \"in_seq\" with data type \"float32\".\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,576 kur.model.model:125]\u001b[0m Trying to infer shape for input \"in_seq\"\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,576 kur.model.model:143]\u001b[0m Inferred shape for input \"in_seq\": (30, 30)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,576 kur.containers.layers.placeholder:127]\u001b[0m Inferred shape: (30, 30)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,580 kur.model.model:382]\u001b[0m   Value: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,581 kur.model.model:311]\u001b[0m Building node: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,581 kur.model.model:312]\u001b[0m   Aliases: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,581 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,581 kur.model.model:315]\u001b[0m   - in_seq: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,930 kur.model.model:382]\u001b[0m   Value: DimShuffle{1,0,2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,930 kur.model.model:311]\u001b[0m Building node: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,931 kur.model.model:312]\u001b[0m   Aliases: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,931 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,931 kur.model.model:315]\u001b[0m   - ..recurrent.0: DimShuffle{1,0,2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,945 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,946 kur.model.model:311]\u001b[0m Building node: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,946 kur.model.model:312]\u001b[0m   Aliases: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,946 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:29,946 kur.model.model:315]\u001b[0m   - ..batch_normalization.0: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,480 kur.model.model:382]\u001b[0m   Value: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,480 kur.model.model:311]\u001b[0m Building node: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,480 kur.model.model:312]\u001b[0m   Aliases: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,480 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,480 kur.model.model:315]\u001b[0m   - ..dropout.0: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,579 kur.model.model:382]\u001b[0m   Value: Subtensor{int64}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,579 kur.model.model:311]\u001b[0m Building node: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,579 kur.model.model:312]\u001b[0m   Aliases: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,579 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,579 kur.model.model:315]\u001b[0m   - ..recurrent.1: Subtensor{int64}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,588 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,588 kur.model.model:311]\u001b[0m Building node: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,588 kur.model.model:312]\u001b[0m   Aliases: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,588 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,588 kur.model.model:315]\u001b[0m   - ..batch_normalization.1: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,633 kur.model.model:382]\u001b[0m   Value: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,634 kur.model.model:311]\u001b[0m Building node: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,634 kur.model.model:312]\u001b[0m   Aliases: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,634 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,634 kur.model.model:315]\u001b[0m   - ..dropout.1: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,635 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,635 kur.model.model:311]\u001b[0m Building node: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,635 kur.model.model:312]\u001b[0m   Aliases: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,635 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,635 kur.model.model:315]\u001b[0m   - ..dense.0: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,636 kur.model.model:382]\u001b[0m   Value: Softmax.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,636 kur.model.model:311]\u001b[0m Building node: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,636 kur.model.model:312]\u001b[0m   Aliases: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,636 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,636 kur.model.model:315]\u001b[0m   - ..activation.0: Softmax.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,636 kur.model.model:382]\u001b[0m   Value: Softmax.0\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:27:30,636 kur.model.model:284]\u001b[0m Model inputs:  in_seq\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:27:30,636 kur.model.model:285]\u001b[0m Model outputs: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,636 kur.model.executor:101]\u001b[0m Recompiling the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:30,637 kur.backend.keras_backend:527]\u001b[0m Instantiating a Keras model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,523 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,523 kur.backend.keras_backend:538]\u001b[0m Layer (type)                     Output Shape          Param #     Connected to                     \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,523 kur.backend.keras_backend:538]\u001b[0m ====================================================================================================\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,523 kur.backend.keras_backend:538]\u001b[0m in_seq (InputLayer)              (None, 30, 30)        0                                            \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,523 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,523 kur.backend.keras_backend:538]\u001b[0m ..recurrent.0 (LSTM)             (None, 30, 512)       1112064     in_seq[0][0]                     \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,523 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,523 kur.backend.keras_backend:538]\u001b[0m ..batch_normalization.0 (BatchNo (None, 30, 512)       2048        ..recurrent.0[0][0]              \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,523 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,523 kur.backend.keras_backend:538]\u001b[0m ..dropout.0 (Dropout)            (None, 30, 512)       0           ..batch_normalization.0[0][0]    \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,524 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,524 kur.backend.keras_backend:538]\u001b[0m ..recurrent.1 (LSTM)             (None, 512)           2099200     ..dropout.0[0][0]                \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,524 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,524 kur.backend.keras_backend:538]\u001b[0m ..batch_normalization.1 (BatchNo (None, 512)           2048        ..recurrent.1[0][0]              \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,524 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,524 kur.backend.keras_backend:538]\u001b[0m ..dropout.1 (Dropout)            (None, 512)           0           ..batch_normalization.1[0][0]    \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,524 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,524 kur.backend.keras_backend:538]\u001b[0m ..dense.0 (Dense)                (None, 30)            15390       ..dropout.1[0][0]                \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,524 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,524 kur.backend.keras_backend:538]\u001b[0m ..activation.0 (Activation)      (None, 30)            0           ..dense.0[0][0]                  \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,524 kur.backend.keras_backend:538]\u001b[0m ====================================================================================================\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,524 kur.backend.keras_backend:538]\u001b[0m Total params: 3,230,750\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,524 kur.backend.keras_backend:538]\u001b[0m Trainable params: 3,228,702\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,524 kur.backend.keras_backend:538]\u001b[0m Non-trainable params: 2,048\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,524 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,524 kur.backend.keras_backend:538]\u001b[0m \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,524 kur.backend.keras_backend:576]\u001b[0m Assembling a training function from the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:31,529 kur.backend.keras_backend:509]\u001b[0m Adding additional inputs: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:54,471 kur.backend.keras_backend:599]\u001b[0m Additional inputs for log functions: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:54,471 kur.backend.keras_backend:616]\u001b[0m Expected input shapes: in_seq=(None, 30, 30), out_char=(None, None)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:54,471 kur.backend.keras_backend:634]\u001b[0m Compiled model: {'func': <keras.backend.theano_backend.Function object at 0x11a7e6cf8>, 'shapes': {'input': [(None, 30, 30), (None, None)]}, 'names': {'output': ['..activation.0', 'out_char'], 'input': ['in_seq', 'out_char']}}\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:54,471 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:54,471 kur.providers.batch_provider:102]\u001b[0m Maximum number of batches set to: 1\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:27:54,517 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:54,517 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:27:54,517 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!kur -vv build char_rrn_demo_dlnd_fluid.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# More tasks to do\n",
    "\n",
    "**implement lots of tutorials in kur == deep learning understanding**\n",
    "- try to implement as many deep learning tutorials in kur as possible\n",
    "- I wonder would [keras](https://keras.io/layers/recurrent/) and [TL docs](http://tensorlayer.readthedocs.io/en/latest/modules/layers.html#fixed-length-recurrent-layer) or [TFLearn doc](http://tflearn.org/layers/recurrent/) provide more accessible knowledge for me to fill the gap presented by tensorflow?\n",
    "\n",
    "\n",
    "**Add more Kur features**\n",
    "- understand how data supplier of kur work: without it, I can't try other datasets\n",
    "- **feature**: plot input to show the shape or structure of an input data point\n",
    "    - so we know what exactly we feed to network\n",
    "- **feature**: plotting weights and activation of each layer: [example](https://hyp.is/4O19zgFVEeeZcue639FnDQ/nbviewer.jupyter.org/github/EmbraceLife/Learn-from-Hvass-Labs-tensorflow/blob/master/2.CNN-MNIST.ipynb) of plot weights for mnist\n",
    "- Here it can help me understand: what exact does **`output` layer** do, [kur doc](https://hyp.is/vrwiIAC4EeekgltPOmLV_Q/kur.deepgram.com/containers.html) does not make much sense for me?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# How to prepare data for RNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "heading_collapsed": true
   },
   "source": [
    "## Create Vocab: 30 unique characters in total\n",
    "- set the length of context sequence as 30\n",
    "- set vocab as a list of 30 unique characters: 26 letters with 4 symbols\n",
    "- create two dictionaries: {char:index}, {index:char}\n",
    "- get length of vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%pycat vocab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_supplier_char_rnn/vocab.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_supplier_char_rnn/vocab.py\n",
    "\"\"\"\n",
    "Copyright 2016 Deepgram\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "# length of the context sequence                        # what does context sequence mean? \n",
    "                                                        # a sequence of words used to predict the next word\n",
    "seq_len = 30\n",
    "\n",
    "                                                        # to create a list of 26 alphabet letter\n",
    "lowercase_letters = [                             \n",
    "    chr(97 + i) for i in range(26)\n",
    "]\n",
    "\n",
    "                                                        # define a list of 4 symbols: space, \", \\, .\n",
    "symbols = [' ', '\"', '\\'', '.']\n",
    "\n",
    "                                                        # define a list of unique characters we allow in our data\n",
    "vocab = lowercase_letters + symbols\n",
    "\n",
    "                                                        # create a dictionary: {char:index}\n",
    "                                                        # give each character an index\n",
    "char_to_int = dict(\n",
    "    (c, i) for i, c in enumerate(vocab)\n",
    ")\n",
    "                                                        # create a dictionary: {index:char}\n",
    "                                                        # give each index an character\n",
    "int_to_char = dict(enumerate(vocab))\n",
    "                                                        # get the length of vocab\n",
    "n_vocab = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import data_supplier_char_rnn \n",
    "from data_supplier_char_rnn import vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 26,\n",
       " '\"': 27,\n",
       " \"'\": 28,\n",
       " '.': 29,\n",
       " 'a': 0,\n",
       " 'b': 1,\n",
       " 'c': 2,\n",
       " 'd': 3,\n",
       " 'e': 4,\n",
       " 'f': 5,\n",
       " 'g': 6,\n",
       " 'h': 7,\n",
       " 'i': 8,\n",
       " 'j': 9,\n",
       " 'k': 10,\n",
       " 'l': 11,\n",
       " 'm': 12,\n",
       " 'n': 13,\n",
       " 'o': 14,\n",
       " 'p': 15,\n",
       " 'q': 16,\n",
       " 'r': 17,\n",
       " 's': 18,\n",
       " 't': 19,\n",
       " 'u': 20,\n",
       " 'v': 21,\n",
       " 'w': 22,\n",
       " 'x': 23,\n",
       " 'y': 24,\n",
       " 'z': 25}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.char_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'a',\n",
       " 1: 'b',\n",
       " 2: 'c',\n",
       " 3: 'd',\n",
       " 4: 'e',\n",
       " 5: 'f',\n",
       " 6: 'g',\n",
       " 7: 'h',\n",
       " 8: 'i',\n",
       " 9: 'j',\n",
       " 10: 'k',\n",
       " 11: 'l',\n",
       " 12: 'm',\n",
       " 13: 'n',\n",
       " 14: 'o',\n",
       " 15: 'p',\n",
       " 16: 'q',\n",
       " 17: 'r',\n",
       " 18: 's',\n",
       " 19: 't',\n",
       " 20: 'u',\n",
       " 21: 'v',\n",
       " 22: 'w',\n",
       " 23: 'x',\n",
       " 24: 'y',\n",
       " 25: 'z',\n",
       " 26: ' ',\n",
       " 27: '\"',\n",
       " 28: \"'\",\n",
       " 29: '.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.int_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "heading_collapsed": true
   },
   "source": [
    "## Make data for RNN model\n",
    "- split all words into characters, and put them into a long list, the list into cleaned.txt\n",
    "- create dataset for each section: train(80%), validate(5%), evaluate(5%), test(5%), we can **change pct here**\n",
    "- each dataset is saved into jsonl file\n",
    "- each jsonl file has 2 variables, `in_seq` as x, `out_char` as y, they are both list of list inside jsonl file\n",
    "- total length of x or y of all sections tegother is about total_character - 30, (30 == len of sequence)\n",
    "- each x input is a 3-d tensor (num_data_points, num_sequence_to_try, one-hot-encoding_input)\n",
    "- each y output is a 2-d tensor (num_data_points, one-hot-encoding_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "source": [
    "**Read make_data.py in detail**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%pycat make_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_supplier_char_rnn/make_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_supplier_char_rnn/make_data.py\n",
    "\"\"\"\n",
    "Copyright 2016 Deepgram\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from vocab import *\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "if not os.path.exists('./data/'):             ### create a data folder if not already available ###\n",
    "    os.mkdir('./data/')\n",
    "\n",
    "                                              ### convert character from indices to one-hot-encoding ###\n",
    "def one_hot(v, ndim):                         # v: indices of all the unique characters of a text\n",
    "    v_one_hot = np.zeros(                     # ndim: num of all unique characters of the text\n",
    "        (len(v), ndim,)\n",
    "    )\n",
    "    for i in range(len(v)):\n",
    "        v_one_hot[i][v[i]] = 1.0              # for each unique character, make 1 at its index on column\n",
    "    return v_one_hot\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "all_chars = []\n",
    "\n",
    "                                              # for each book\n",
    "for book in [\n",
    "    'pride_and_prejudice.txt',\n",
    "    'shakespeare.txt'\n",
    "]:                                          \n",
    "                                              # for every line of this book\n",
    "    with open('books/%s' % book, 'r') as infile:\n",
    "                                              # split every word into separate characters, a space separate each word\n",
    "        chars = [\n",
    "            c for c in ' '.join(infile.read().lower().split())\n",
    "            if c in set(vocab)\n",
    "        ]\n",
    "        all_chars += [' ']\n",
    "        all_chars += chars\n",
    "                                              # put all words of book into a long list (see **example1** below)\n",
    "            \n",
    "\n",
    "                                              # get rid of the space in the beginning\n",
    "all_chars = list(' '.join(''.join(all_chars).split()))\n",
    "num_chars = len(all_chars)                    # count num of characters in the book including spaces\n",
    "\n",
    "\n",
    "                                              # create an empty file named `cleaned.txt` to write data into\n",
    "with open('cleaned.txt', 'w') as outfile:\n",
    "                                              # write all the whole list of characters of the books into it\n",
    "    outfile.write(''.join(all_chars))         # without a space in the beginning\n",
    "\n",
    "\n",
    "x, y = [], []                                 # set x, y as empty list\n",
    "\n",
    "                                            \n",
    "                                              # define portions for each section: train, validate, evalute, test\n",
    "data_portions = [\n",
    "    ('train', 0.8),\n",
    "    ('validate', 0.05),\n",
    "    ('test', 0.05),\n",
    "    ('evaluate', 0.05),\n",
    "]\n",
    "\n",
    "dev = True                                     # reduce amount of data for training x10 times\n",
    "if dev:\n",
    "                                               # shrink every section data by x10 times\n",
    "        \n",
    "    for i in range(len(data_portions)):\n",
    "        data_portions[i] = (\n",
    "            data_portions[i][0],\n",
    "            data_portions[i][1] * 0.1\n",
    "        )\n",
    "        \n",
    "        \n",
    "                                               # sum up num of data points in each section (train, validate...) \n",
    "                                               # max_i = sum_above - seq_len\n",
    "max_i = sum([\n",
    "    int(round(len(all_chars) * fraction))\n",
    "    for name, fraction in data_portions\n",
    "]) - seq_len                                   # seq_len is defined inside vocab.py as 30\n",
    "\n",
    "\n",
    "                                               # for every element of max_i \n",
    "for i in range(max_i):\n",
    "                                               # create a short list of length 30, assign to in_char_seq\n",
    "                                               # every in_char_seq only has 1 character different to its neighbour\n",
    "    in_char_seq = all_chars[i: i + seq_len]\n",
    "\n",
    "                                               # one hot representation\n",
    "                                               # create a matrix of 0s with dim(30, 30), assigned to sample_x\n",
    "    sample_x = np.zeros((len(in_char_seq), n_vocab,))\n",
    "    \n",
    "                                                \n",
    "    for j, c in enumerate(in_char_seq):        # j as index from 0 to 29, c as character \n",
    "        sample_x[j][char_to_int[c]] = 1        # find unique index of the character, and put 1 in the index of column\n",
    "                                               # by now, a list of 30 characters turned into a matrix of 0s and 1s \n",
    "                                               # in other words, 30 character in one-hot-encoding format \n",
    "                                                \n",
    "    x.append(sample_x)                         # tranform all characters into one-hot-encoding format and save them\n",
    "                                               # all in x (x was an empty list)\n",
    "                                               # what does x look like? - a list of arrays see **example2** #######\n",
    "        \n",
    "        \n",
    "                                               # create a 1-d array of 0s of length 30 for sample_y \n",
    "    sample_y = np.zeros(n_vocab)\n",
    "    \n",
    "                                               # get all characters from the 30th onward\n",
    "                                               # get the unique index of the character\n",
    "                                               # make the location of the index from 0 to 1\n",
    "    sample_y[char_to_int[all_chars[i + seq_len]]] = 1\n",
    "    y.append(sample_y)                         # now, we transformed the y from character to one-hot-encoding\n",
    "\n",
    "    \n",
    "                                               # convert x, y from lists to arrays of arrays\n",
    "x, y = np.array(x).astype('int32'), np.array(y).astype('int32') \n",
    "                                               # x as 3-d (max_i, 30, 30), y as 2-d (max_i, 30)  \n",
    "                                                                                        # see **example2** #######\n",
    "\n",
    "\n",
    "                                                # set starting index\n",
    "start_i = 0     \n",
    "                                                # for each section: train, validate, evaluate, test\n",
    "for name, fraction in data_portions:\n",
    "                                                # get ending index of each section\n",
    "    end_i = start_i + int(round(len(x) * fraction))    # len(x) == max_i\n",
    "    print(start_i, end_i)\n",
    "    x0 = x[start_i: end_i]\n",
    "    y0 = y[start_i: end_i]\n",
    "                                                # print dim of each section's x and y\n",
    "    print('dims:')\n",
    "    print(x0.shape)\n",
    "    print(y0.shape)\n",
    "                                                # set current section's ending index as starting index of next section\n",
    "    start_i = end_i\n",
    "                                                # open an empty jsonl file and write to it\n",
    "    with open('data/%s.jsonl' % name, 'w') as outfile:\n",
    "        for sample_x, sample_y in zip(x0, y0):  # zip sample_x and sample_y together\n",
    "            outfile.write(json.dumps({\n",
    "                'in_seq': sample_x.tolist(),    # write into file as list of list rather than numpy arrays\n",
    "                'out_char': sample_y.tolist()\n",
    "            }))\n",
    "            outfile.write('\\n')\n",
    "\n",
    "    del x0, y0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "source": [
    "### example1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_chars = []\n",
    "\n",
    "                                              # for each book\n",
    "for book in [\n",
    "    'pride_and_prejudice.txt',\n",
    "    'shakespeare.txt'\n",
    "]:                                          \n",
    "                                              # for every line of this book\n",
    "    with open('books/%s' % book, 'r') as infile:\n",
    "                                              # split each line into characters, and join them by a space\n",
    "        chars = [\n",
    "            c for c in ' '.join(infile.read().lower().split())\n",
    "            if c in set(vocab.vocab)\n",
    "        ]\n",
    "        all_chars += [' ']\n",
    "        all_chars += chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'o',\n",
       " 'j',\n",
       " 'e',\n",
       " 'c',\n",
       " 't',\n",
       " ' ',\n",
       " 'g',\n",
       " 'u',\n",
       " 't',\n",
       " 'e',\n",
       " 'n',\n",
       " 'b',\n",
       " 'e',\n",
       " 'r',\n",
       " 'g',\n",
       " ' ',\n",
       " 'e',\n",
       " 'b',\n",
       " 'o',\n",
       " 'o',\n",
       " 'k',\n",
       " ' ',\n",
       " 'o']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chars[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_chars = list(' '.join(''.join(all_chars).split()))\n",
    "num_chars = len(all_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1750339"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "source": [
    "### example2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.zeros((30,30,))\n",
    "b = np.ones((30,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.]]),\n",
       " array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.]])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "c.append(a)\n",
    "c.append(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 30, 30)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(c).shape # 2 * 30 = number of data points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.array(c)\n",
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "30\n",
      "30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(d.tolist()))\n",
    "print(len(d.tolist()[0]))\n",
    "print(len(d.tolist()[0][0]))\n",
    "type(d.tolist()[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Combine vocab.py and make_data.py\n",
    "- both files work together to produce datasets;\n",
    "- Users like to change length of sequence freely, but they can't change length of sequence in kurfile at all;\n",
    "- combine both files into one single function and make length of sequence an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m__pycache__\u001b[m\u001b[m/                      \u001b[34mdata\u001b[m\u001b[m/\r\n",
      "\u001b[34mbooks\u001b[m\u001b[m/                            \u001b[34mdata_supplier_char_rnn\u001b[m\u001b[m/\r\n",
      "char_rnn_demo.yaml                make_data.py\r\n",
      "char_rnn_demo_defaults.yaml       steps.sh\r\n",
      "char_rnn_demo_dlnd_defaults.yaml  \u001b[34mt1\u001b[m\u001b[m/\r\n",
      "char_rnn_demo_dp_defaults.yaml    \u001b[34mt2_dp\u001b[m\u001b[m/\r\n",
      "char_rnn_kur.ipynb                \u001b[34mt3_dlnd\u001b[m\u001b[m/\r\n",
      "char_rrn_demo_dlnd_fluid.yaml     view_data.py\r\n",
      "char_rrn_demo_dp_fluid.yaml       view_logs.py\r\n",
      "char_rrn_demo_fluid.yaml          view_outputs.py\r\n",
      "cleaned.txt                       vocab.py\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%pycat char_rnn_demo.yaml\n",
    "# there is only vocab variable inside kurfile, but it can't be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting make_data_func.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile make_data_func.py\n",
    "# vocab.py\n",
    "\n",
    "# length of the context sequence                        # what does context sequence mean? \n",
    "                                                        # a sequence of words used to predict the next word\n",
    "# seq_len = 30\n",
    "\n",
    "                                                        # to create a list of 26 alphabet letter\n",
    "lowercase_letters = [                             \n",
    "    chr(97 + i) for i in range(26)\n",
    "]\n",
    "\n",
    "                                                        # define a list of 4 symbols: space, \", \\, .\n",
    "symbols = [' ', '\"', '\\'', '.']             ############# QUESTION: all other numbers and symbols are ignored, ### \n",
    "                                                        # in what situations this is common practice on vocab? ###\n",
    "                                                        # in what situations should we keep all num and symbols ##\n",
    "\n",
    "                                                        # define a list of unique characters we allow in our data\n",
    "vocab = lowercase_letters + symbols\n",
    "\n",
    "                                                        # create a dictionary: {char:index}\n",
    "                                                        # give each character an index\n",
    "char_to_int = dict(\n",
    "    (c, i) for i, c in enumerate(vocab)\n",
    ")\n",
    "                                                        # create a dictionary: {index:char}\n",
    "                                                        # give each index an character\n",
    "int_to_char = dict(enumerate(vocab))\n",
    "                                                        # get the length of vocab\n",
    "n_vocab = len(vocab)\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "# make_data.py\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from vocab import *\n",
    "import json\n",
    "import os\n",
    "\n",
    "                                                 # given 30 character in sequence to predict the next character\n",
    "                                                 # dev = True, is to only use 10% of data\n",
    "def make_data(seq_len=30, dev=True):\n",
    "\n",
    "    if not os.path.exists('./data/'):             ### create a data folder if not already available ###\n",
    "        os.mkdir('./data/')\n",
    "\n",
    "                                                  ### convert character from indices to one-hot-encoding ###\n",
    "    def one_hot(v, ndim):                         # v: indices of all the unique characters of a text\n",
    "        v_one_hot = np.zeros(                     # ndim: num of all unique characters of the text\n",
    "            (len(v), ndim,)\n",
    "        )\n",
    "        for i in range(len(v)):\n",
    "            v_one_hot[i][v[i]] = 1.0              # for each unique character, make 1 at its index on column\n",
    "        return v_one_hot\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    all_chars = []\n",
    "\n",
    "                                                  # for each book\n",
    "    for book in [\n",
    "        'pride_and_prejudice.txt',\n",
    "        'shakespeare.txt'\n",
    "    ]:                                          \n",
    "                                                  # for every line of this book\n",
    "        with open('books/%s' % book, 'r') as infile:\n",
    "                                                  # split every word into separate characters, a space separate each word\n",
    "            chars = [\n",
    "                c for c in ' '.join(infile.read().lower().split())\n",
    "                if c in set(vocab)\n",
    "            ]\n",
    "            all_chars += [' ']\n",
    "            all_chars += chars\n",
    "                                                  # put all words of book into a long list (see **example1** below)\n",
    "\n",
    "\n",
    "                                                  # get rid of the space in the beginning\n",
    "    all_chars = list(' '.join(''.join(all_chars).split()))\n",
    "    num_chars = len(all_chars)                    # count num of characters in the book including spaces\n",
    "\n",
    "\n",
    "                                                  # create an empty file named `cleaned.txt` to write data into\n",
    "    with open('cleaned.txt', 'w') as outfile:\n",
    "                                                  # write all the whole list of characters of the books into it\n",
    "        outfile.write(''.join(all_chars))         # without a space in the beginning\n",
    "\n",
    "\n",
    "    x, y = [], []                                 # set x, y as empty list\n",
    "\n",
    "\n",
    "                                                  # define portions for each section: train, validate, evalute, test\n",
    "    data_portions = [\n",
    "        ('train', 0.8),\n",
    "        ('validate', 0.05),\n",
    "        ('test', 0.05),\n",
    "        ('evaluate', 0.05),\n",
    "    ]\n",
    "\n",
    "#     dev = True                                     # reduce amount of data for training x10 times\n",
    "    if dev:\n",
    "                                                   # shrink every section data by x10 times\n",
    "\n",
    "        for i in range(len(data_portions)):\n",
    "            data_portions[i] = (\n",
    "                data_portions[i][0],\n",
    "                data_portions[i][1] * 0.1\n",
    "            )\n",
    "\n",
    "\n",
    "                                                   # sum up num of data points in each section (train, validate...) \n",
    "                                                   # max_i = sum_above - seq_len\n",
    "    max_i = sum([\n",
    "        int(round(len(all_chars) * fraction))\n",
    "        for name, fraction in data_portions\n",
    "    ]) - seq_len                                   # seq_len is defined inside vocab.py as 30\n",
    "\n",
    "\n",
    "                                                   # for every element of max_i \n",
    "    for i in range(max_i):\n",
    "                                                   # create a short list of length 30, assign to in_char_seq\n",
    "                                                   # every in_char_seq only has 1 character different to its neighbour\n",
    "        in_char_seq = all_chars[i: i + seq_len]\n",
    "\n",
    "                                                   # one hot representation\n",
    "                                                   # create a matrix of 0s with dim(30, 30), assigned to sample_x\n",
    "        sample_x = np.zeros((len(in_char_seq), n_vocab,))\n",
    "\n",
    "\n",
    "        for j, c in enumerate(in_char_seq):        # j as index from 0 to 29, c as character \n",
    "            sample_x[j][char_to_int[c]] = 1        # find unique index of the character, and put 1 in the index of column\n",
    "                                                   # by now, a list of 30 characters turned into a matrix of 0s and 1s \n",
    "                                                   # in other words, 30 character in one-hot-encoding format \n",
    "\n",
    "        x.append(sample_x)                         # tranform all characters into one-hot-encoding format and save them\n",
    "                                                   # all in x (x was an empty list)\n",
    "                                                   # what does x look like? - a list of arrays see **example2** #######\n",
    "\n",
    "\n",
    "                                                   # create a 1-d array of 0s of length 30 for sample_y \n",
    "        sample_y = np.zeros(n_vocab)\n",
    "\n",
    "                                                   # get all characters from the 30th onward\n",
    "                                                   # get the unique index of the character\n",
    "                                                   # make the location of the index from 0 to 1\n",
    "        sample_y[char_to_int[all_chars[i + seq_len]]] = 1\n",
    "        y.append(sample_y)                         # now, we transformed the y from character to one-hot-encoding\n",
    "\n",
    "\n",
    "                                                   # convert x, y from lists to arrays of arrays\n",
    "    x, y = np.array(x).astype('int32'), np.array(y).astype('int32') \n",
    "                                                   # x as 3-d (max_i, 30, 30), y as 2-d (max_i, 30)  \n",
    "                                                                                            # see **example2** #######\n",
    "\n",
    "\n",
    "                                                    # set starting index\n",
    "    start_i = 0     \n",
    "                                                    # for each section: train, validate, evaluate, test\n",
    "    for name, fraction in data_portions:\n",
    "                                                    # get ending index of each section\n",
    "        end_i = start_i + int(round(len(x) * fraction))    # len(x) == max_i\n",
    "        print(start_i, end_i)\n",
    "        x0 = x[start_i: end_i]\n",
    "        y0 = y[start_i: end_i]\n",
    "                                                    # print dim of each section's x and y\n",
    "        print('dims:')\n",
    "        print(x0.shape)\n",
    "        print(y0.shape)\n",
    "                                                    # set current section's ending index as starting index of next section\n",
    "        start_i = end_i\n",
    "                                                    # open an empty jsonl file and write to it\n",
    "        with open('data/%s.jsonl' % name, 'w') as outfile:\n",
    "            for sample_x, sample_y in zip(x0, y0):  # zip sample_x and sample_y together\n",
    "                outfile.write(json.dumps({\n",
    "                    'in_seq': sample_x.tolist(),    # write into file as list of list rather than numpy arrays\n",
    "                    'out_char': sample_y.tolist()   # QUESTION: it means kur accept lists from jsonl file by default\n",
    "                }))\n",
    "                outfile.write('\\n')\n",
    "\n",
    "        del x0, y0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## make data with seq_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m__pycache__\u001b[m\u001b[m/                      \u001b[34mdata\u001b[m\u001b[m/\r\n",
      "\u001b[34mbooks\u001b[m\u001b[m/                            \u001b[34mdata_supplier_char_rnn\u001b[m\u001b[m/\r\n",
      "char_rnn_demo.yaml                make_data.py\r\n",
      "char_rnn_demo_defaults.yaml       steps.sh\r\n",
      "char_rnn_demo_dlnd_defaults.yaml  \u001b[34mt1\u001b[m\u001b[m/\r\n",
      "char_rnn_demo_dp_defaults.yaml    \u001b[34mt2_dp\u001b[m\u001b[m/\r\n",
      "char_rnn_kur.ipynb                \u001b[34mt3_dlnd\u001b[m\u001b[m/\r\n",
      "char_rrn_demo_dlnd_fluid.yaml     view_data.py\r\n",
      "char_rrn_demo_dp_fluid.yaml       view_logs.py\r\n",
      "char_rrn_demo_fluid.yaml          view_outputs.py\r\n",
      "cleaned.txt                       vocab.py\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Natsume/Downloads/kur_road/character_rnn/data_supplier_char_rnn\n"
     ]
    }
   ],
   "source": [
    "%cd data_supplier_char_rnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from make_data_func import *\n",
    "# make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13299\n",
      "dims:\n",
      "(13299, 50, 30)\n",
      "(13299, 30)\n",
      "13299 14130\n",
      "dims:\n",
      "(831, 50, 30)\n",
      "(831, 30)\n",
      "14130 14961\n",
      "dims:\n",
      "(831, 50, 30)\n",
      "(831, 30)\n",
      "14961 15792\n",
      "dims:\n",
      "(831, 50, 30)\n",
      "(831, 30)\n"
     ]
    }
   ],
   "source": [
    "# %pycat make_data_func.py\n",
    "make_data(seq_len = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## View data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%pycat view_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing view_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile view_data.py\n",
    "\"\"\"\n",
    "Copyright 2016 Deepgram\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from vocab import *\n",
    "\n",
    "\n",
    "# this script shows sections of size `window` from the ends\n",
    "# of each data file. its output looks like this:\n",
    "# peek at train:\n",
    "# ------\n",
    "# \"the project gutenberg ebook of\" --> \" \"\n",
    "# \"he project gutenberg ebook of \" --> \"p\"\n",
    "# \"e project gutenberg ebook of p\" --> \"r\"\n",
    "# ------\n",
    "# \"o particular resentment by his\" --> \" \"\n",
    "# \" particular resentment by his \" --> \"h\"\n",
    "# \"particular resentment by his h\" --> \"a\"\n",
    "\n",
    "\n",
    "# peek at validate:\n",
    "# ------\n",
    "# \"articular resentment by his ha\" --> \"v\"\n",
    "# \"rticular resentment by his hav\" --> \"i\"\n",
    "# \"ticular resentment by his havi\" --> \"n\"\n",
    "# ------\n",
    "# \"ingley for a kingdom upon my h\" --> \"o\"\n",
    "# \"ngley for a kingdom upon my ho\" --> \"n\"\n",
    "# \"gley for a kingdom upon my hon\" --> \"o\"\n",
    "\n",
    "\n",
    "# peek at test:\n",
    "# ------\n",
    "# \"ley for a kingdom upon my hono\" --> \"u\"\n",
    "# \"ey for a kingdom upon my honou\" --> \"r\"\n",
    "# \"y for a kingdom upon my honour\" --> \" \"\n",
    "# ------\n",
    "# \"sting your time with me. mr. b\" --> \"i\"\n",
    "# \"ting your time with me. mr. bi\" --> \"n\"\n",
    "# \"ing your time with me. mr. bin\" --> \"g\"\n",
    "\n",
    "\n",
    "# peek at evaluate:\n",
    "# ------\n",
    "# \"ng your time with me. mr. bing\" --> \"l\"\n",
    "# \"g your time with me. mr. bingl\" --> \"e\"\n",
    "# \" your time with me. mr. bingle\" --> \"y\"\n",
    "# ------\n",
    "# \"they had yet learnt to care fo\" --> \"r\"\n",
    "# \"hey had yet learnt to care for\" --> \" \"\n",
    "# \"ey had yet learnt to care for \" --> \"a\"\n",
    "\n",
    "                                                        # set a window to print data\n",
    "window = 3\n",
    "\n",
    "                                                        # get data out of jsonl file\n",
    "def get_data(name):\n",
    "    with open('data/%s.jsonl' % name, 'r') as infile:   # read infile out of jsonl file\n",
    "                                                        # create an empty dictionary\n",
    "        data = {}                   \n",
    "\n",
    "        data = {                                        # use the first line to create key and first element of value\n",
    "            key: [value]\n",
    "            for key, value in json.loads(next(infile)).items()\n",
    "        }\n",
    "                                                        # loop through the rest lines of infile to fill data dict\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            for key, value in json.loads(line).items():\n",
    "                data[key].append(value)\n",
    "                                                        # turn data's values into np.arrays\n",
    "        for key in data:\n",
    "            data[key] = np.array(data[key])\n",
    "\n",
    "    return data\n",
    "\n",
    "                                                        # print out a window of data with a given name\n",
    "def view_data(name):\n",
    "    data = get_data(name)                               # get a sepcific section data\n",
    "\n",
    "    print('\\n\\npeek at %s:' % name)\n",
    "                                                        # first j_range == first 3 data points \n",
    "                                                        # second j_range == last 3 data points \n",
    "    for j_range in (range(window), range(len(data['in_seq']) - window, len(data['in_seq'])),):\n",
    "        print('------')\n",
    "        for j in j_range:\n",
    "            print(\n",
    "                '\"%s\" --> \"%s\"' % (                      # print one x data point which is a (30, 30) array\n",
    "                    ''.join([                            # join the elements of list below into a string\n",
    "                                                         # turn list of argmax into characters\n",
    "                        int_to_char[np.argmax(_)]        # take the 2-d array below, and flatten it to list of argmax\n",
    "                        for _ in data['in_seq'][j]       # take a 2-d (30,30) array at a time\n",
    "                    ]),\n",
    "                                                         # print one y data point which is (1, 30) array\n",
    "                    int_to_char[np.argmax(data['out_char'][j])]\n",
    "                )\n",
    "            )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for name in ('train', 'validate', 'test', 'evaluate',):\n",
    "        view_data(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 10)\n",
      "range(10, 20)\n"
     ]
    }
   ],
   "source": [
    "for i in (range(10), range(10,20)): \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "for i in range(10, 20):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### example1: use just the first set of in_sep and out_char for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "name = 'test'\n",
    "with open('data/%s.jsonl' % name, 'r') as infile:   # read infile out of jsonl file\n",
    "        data = {}\n",
    "\n",
    "        data = {                                        # turn infile to a dict {key: [value]}\n",
    "            key: [value]\n",
    "            for key, value in json.loads(next(infile)).items()\n",
    "        }\n",
    "        for key in data:\n",
    "            data[key] = np.array(data[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['out_char', 'in_seq'])\n",
      "[[[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "  [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "  [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "  [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 30, 30)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.keys())\n",
    "print(data['in_seq'])\n",
    "print(data['out_char'])\n",
    "data['in_seq'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### use the rest of infile lines to fill data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_char [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "in_seq [[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "out_char [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "name = 'test'\n",
    "with open('data/%s.jsonl' % name, 'r') as infile:   # read infile out of jsonl file\n",
    "    data = {}\n",
    "\n",
    "    data = {                                        # turn infile to a dict {key: [value]}\n",
    "        key: [value]\n",
    "        for key, value in json.loads(next(infile)).items()\n",
    "    }\n",
    "    \n",
    "    count = 0\n",
    "    for line in infile:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        for key, value in json.loads(line).items():\n",
    "            if count < 3:\n",
    "                print(key, value)\n",
    "            count += 1    \n",
    "            data[key].append(value)\n",
    "\n",
    "#     for key in data:\n",
    "#         data[key] = np.array(data[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "831"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['in_seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "peek at train:\n",
      "------\n",
      "\"the project gutenberg ebook of pride and prejudice\" --> \" \"\n",
      "\"he project gutenberg ebook of pride and prejudice \" --> \"b\"\n",
      "\"e project gutenberg ebook of pride and prejudice b\" --> \"y\"\n",
      "------\n",
      "\"to particular resentment by his having slighted on\" --> \"e\"\n",
      "\"o particular resentment by his having slighted one\" --> \" \"\n",
      "\" particular resentment by his having slighted one \" --> \"o\"\n",
      "\n",
      "\n",
      "peek at validate:\n",
      "------\n",
      "\"particular resentment by his having slighted one o\" --> \"f\"\n",
      "\"articular resentment by his having slighted one of\" --> \" \"\n",
      "\"rticular resentment by his having slighted one of \" --> \"h\"\n",
      "------\n",
      "\"bingley for a kingdom upon my honour i never met w\" --> \"i\"\n",
      "\"ingley for a kingdom upon my honour i never met wi\" --> \"t\"\n",
      "\"ngley for a kingdom upon my honour i never met wit\" --> \"h\"\n",
      "\n",
      "\n",
      "peek at test:\n",
      "------\n",
      "\"gley for a kingdom upon my honour i never met with\" --> \" \"\n",
      "\"ley for a kingdom upon my honour i never met with \" --> \"s\"\n",
      "\"ey for a kingdom upon my honour i never met with s\" --> \"o\"\n",
      "------\n",
      "\"asting your time with me. mr. bingley followed his\" --> \" \"\n",
      "\"sting your time with me. mr. bingley followed his \" --> \"a\"\n",
      "\"ting your time with me. mr. bingley followed his a\" --> \"d\"\n",
      "\n",
      "\n",
      "peek at evaluate:\n",
      "------\n",
      "\"ing your time with me. mr. bingley followed his ad\" --> \"v\"\n",
      "\"ng your time with me. mr. bingley followed his adv\" --> \"i\"\n",
      "\"g your time with me. mr. bingley followed his advi\" --> \"c\"\n",
      "------\n",
      "\" they had yet learnt to care for at a ball. they r\" --> \"e\"\n",
      "\"they had yet learnt to care for at a ball. they re\" --> \"t\"\n",
      "\"hey had yet learnt to care for at a ball. they ret\" --> \"u\"\n"
     ]
    }
   ],
   "source": [
    "!python view_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train in debug mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%pycat char_rrn_demo_dp_fluid.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-03-05 22:30:43,216 kur.kurfile:699]\u001b[0m Parsing source: char_rrn_demo_dp_fluid.yaml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:30:43,220 kur.kurfile:699]\u001b[0m Parsing source: char_rnn_demo_dp_defaults.yaml, included by char_rrn_demo_dp_fluid.yaml.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:30:43,231 kur.kurfile:82]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:43,231 kur.kurfile:784]\u001b[0m Parsing Kurfile section: settings\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:43,232 kur.kurfile:784]\u001b[0m Parsing Kurfile section: train\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:43,236 kur.kurfile:784]\u001b[0m Parsing Kurfile section: validate\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:43,237 kur.kurfile:784]\u001b[0m Parsing Kurfile section: test\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:43,238 kur.kurfile:784]\u001b[0m Parsing Kurfile section: evaluate\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:43,240 kur.containers.layers.placeholder:63]\u001b[0m Using short-hand name for placeholder: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:43,241 kur.containers.layers.placeholder:97]\u001b[0m Placeholder \"in_seq\" has a deferred shape.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:43,251 kur.containers.layers.output:50]\u001b[0m Using short-hand name for output: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:43,252 kur.kurfile:784]\u001b[0m Parsing Kurfile section: loss\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:30:43,253 kur.loggers.binary_logger:107]\u001b[0m Log does not exist. Creating path: t2_dp/log\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:49,009 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 32\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:49,403 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 32\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:49,403 kur.backend.backend:187]\u001b[0m Using backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:30:49,404 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:30:49,404 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:30:49,404 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:49,404 kur.backend.keras_backend:124]\u001b[0m Using the system-default Keras backend.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:49,404 kur.backend.keras_backend:189]\u001b[0m Overriding environmental variables: {'KERAS_BACKEND': None, 'THEANO_FLAGS': None, 'TF_CPP_MIN_LOG_LEVEL': '1'}\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:30:50,501 kur.backend.keras_backend:195]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:30:50,501 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:30:50,501 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,501 kur.model.model:272]\u001b[0m Assembled Node: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:274]\u001b[0m   Uses: \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:276]\u001b[0m   Used by: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:277]\u001b[0m   Aliases: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:272]\u001b[0m Assembled Node: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:274]\u001b[0m   Uses: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:276]\u001b[0m   Used by: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:277]\u001b[0m   Aliases: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:272]\u001b[0m Assembled Node: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:274]\u001b[0m   Uses: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:276]\u001b[0m   Used by: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:277]\u001b[0m   Aliases: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:272]\u001b[0m Assembled Node: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:274]\u001b[0m   Uses: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:276]\u001b[0m   Used by: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:277]\u001b[0m   Aliases: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:272]\u001b[0m Assembled Node: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:274]\u001b[0m   Uses: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:276]\u001b[0m   Used by: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:277]\u001b[0m   Aliases: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:272]\u001b[0m Assembled Node: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,502 kur.model.model:274]\u001b[0m   Uses: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:276]\u001b[0m   Used by: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:277]\u001b[0m   Aliases: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:272]\u001b[0m Assembled Node: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:274]\u001b[0m   Uses: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:276]\u001b[0m   Used by: ..recurrent.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:277]\u001b[0m   Aliases: ..dropout.1, ..for.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:272]\u001b[0m Assembled Node: ..recurrent.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:274]\u001b[0m   Uses: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:276]\u001b[0m   Used by: ..dropout.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:277]\u001b[0m   Aliases: ..recurrent.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:272]\u001b[0m Assembled Node: ..dropout.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:274]\u001b[0m   Uses: ..recurrent.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:276]\u001b[0m   Used by: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:277]\u001b[0m   Aliases: ..dropout.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:272]\u001b[0m Assembled Node: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:274]\u001b[0m   Uses: ..dropout.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:276]\u001b[0m   Used by: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:277]\u001b[0m   Aliases: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:272]\u001b[0m Assembled Node: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:274]\u001b[0m   Uses: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:276]\u001b[0m   Used by: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,503 kur.model.model:277]\u001b[0m   Aliases: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,504 kur.model.model:272]\u001b[0m Assembled Node: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,504 kur.model.model:274]\u001b[0m   Uses: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,504 kur.model.model:276]\u001b[0m   Used by: \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,504 kur.model.model:277]\u001b[0m   Aliases: out_char\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:30:50,504 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,504 kur.model.model:311]\u001b[0m Building node: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,504 kur.model.model:312]\u001b[0m   Aliases: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,504 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,504 kur.containers.layers.placeholder:117]\u001b[0m Creating placeholder for \"in_seq\" with data type \"float32\".\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,504 kur.model.model:125]\u001b[0m Trying to infer shape for input \"in_seq\"\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,504 kur.model.model:143]\u001b[0m Inferred shape for input \"in_seq\": (50, 30)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,504 kur.containers.layers.placeholder:127]\u001b[0m Inferred shape: (50, 30)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,508 kur.model.model:382]\u001b[0m   Value: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,509 kur.model.model:311]\u001b[0m Building node: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,509 kur.model.model:312]\u001b[0m   Aliases: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,509 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:50,509 kur.model.model:315]\u001b[0m   - in_seq: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:51,630 kur.model.model:382]\u001b[0m   Value: DimShuffle{1,0,2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:51,630 kur.model.model:311]\u001b[0m Building node: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:51,630 kur.model.model:312]\u001b[0m   Aliases: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:51,630 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:51,630 kur.model.model:315]\u001b[0m   - ..recurrent.0: DimShuffle{1,0,2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:51,650 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:51,650 kur.model.model:311]\u001b[0m Building node: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:51,650 kur.model.model:312]\u001b[0m   Aliases: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:51,650 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:51,651 kur.model.model:315]\u001b[0m   - ..batch_normalization.0: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,615 kur.model.model:382]\u001b[0m   Value: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,616 kur.model.model:311]\u001b[0m Building node: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,616 kur.model.model:312]\u001b[0m   Aliases: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,616 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,616 kur.model.model:315]\u001b[0m   - ..dropout.0: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,686 kur.model.model:382]\u001b[0m   Value: DimShuffle{1,0,2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,687 kur.model.model:311]\u001b[0m Building node: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,687 kur.model.model:312]\u001b[0m   Aliases: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,687 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,687 kur.model.model:315]\u001b[0m   - ..recurrent.1: DimShuffle{1,0,2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,709 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,709 kur.model.model:311]\u001b[0m Building node: ..dropout.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,709 kur.model.model:312]\u001b[0m   Aliases: ..dropout.1, ..for.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,709 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,710 kur.model.model:315]\u001b[0m   - ..batch_normalization.1: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,799 kur.model.model:382]\u001b[0m   Value: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,799 kur.model.model:311]\u001b[0m Building node: ..recurrent.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,800 kur.model.model:312]\u001b[0m   Aliases: ..recurrent.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,800 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,800 kur.model.model:315]\u001b[0m   - ..dropout.1: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,891 kur.model.model:382]\u001b[0m   Value: Subtensor{int64}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,892 kur.model.model:311]\u001b[0m Building node: ..dropout.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,892 kur.model.model:312]\u001b[0m   Aliases: ..dropout.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,892 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:52,892 kur.model.model:315]\u001b[0m   - ..recurrent.2: Subtensor{int64}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:53,004 kur.model.model:382]\u001b[0m   Value: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:53,005 kur.model.model:311]\u001b[0m Building node: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:53,005 kur.model.model:312]\u001b[0m   Aliases: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:53,005 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:53,005 kur.model.model:315]\u001b[0m   - ..dropout.2: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:53,006 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:53,006 kur.model.model:311]\u001b[0m Building node: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:53,006 kur.model.model:312]\u001b[0m   Aliases: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:53,006 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:53,006 kur.model.model:315]\u001b[0m   - ..dense.0: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:53,006 kur.model.model:382]\u001b[0m   Value: Softmax.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:53,006 kur.model.model:311]\u001b[0m Building node: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:53,007 kur.model.model:312]\u001b[0m   Aliases: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:53,007 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:53,007 kur.model.model:315]\u001b[0m   - ..activation.0: Softmax.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:53,007 kur.model.model:382]\u001b[0m   Value: Softmax.0\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:30:53,007 kur.model.model:284]\u001b[0m Model inputs:  in_seq\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:30:53,007 kur.model.model:285]\u001b[0m Model outputs: out_char\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:30:53,007 kur.kurfile:357]\u001b[0m Ignoring missing initial weights: t2_dp/best.w.kur. If this is undesireable, set \"must_exist\" to \"yes\" in the approriate \"weights\" section.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:30:53,007 kur.model.executor:315]\u001b[0m No historical training loss available from logs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:30:53,007 kur.model.executor:323]\u001b[0m No historical validation loss available from logs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:30:53,007 kur.model.executor:329]\u001b[0m No previous epochs.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:53,007 kur.model.executor:353]\u001b[0m Epoch handling mode: additional\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:53,008 kur.model.executor:101]\u001b[0m Recompiling the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:53,008 kur.backend.keras_backend:527]\u001b[0m Instantiating a Keras model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,138 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,138 kur.backend.keras_backend:538]\u001b[0m Layer (type)                     Output Shape          Param #     Connected to                     \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,138 kur.backend.keras_backend:538]\u001b[0m ====================================================================================================\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,139 kur.backend.keras_backend:538]\u001b[0m in_seq (InputLayer)              (None, 50, 30)        0                                            \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,139 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,139 kur.backend.keras_backend:538]\u001b[0m ..recurrent.0 (GRU)              (None, 50, 128)       61056       in_seq[0][0]                     \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,139 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,139 kur.backend.keras_backend:538]\u001b[0m ..batch_normalization.0 (BatchNo (None, 50, 128)       512         ..recurrent.0[0][0]              \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,139 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,139 kur.backend.keras_backend:538]\u001b[0m ..dropout.0 (Dropout)            (None, 50, 128)       0           ..batch_normalization.0[0][0]    \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,139 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,139 kur.backend.keras_backend:538]\u001b[0m ..recurrent.1 (GRU)              (None, 50, 128)       98688       ..dropout.0[0][0]                \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,139 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,139 kur.backend.keras_backend:538]\u001b[0m ..batch_normalization.1 (BatchNo (None, 50, 128)       512         ..recurrent.1[0][0]              \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,139 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,140 kur.backend.keras_backend:538]\u001b[0m ..dropout.1 (Dropout)            (None, 50, 128)       0           ..batch_normalization.1[0][0]    \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,140 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,140 kur.backend.keras_backend:538]\u001b[0m ..recurrent.2 (GRU)              (None, 128)           98688       ..dropout.1[0][0]                \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,140 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,140 kur.backend.keras_backend:538]\u001b[0m ..dropout.2 (Dropout)            (None, 128)           0           ..recurrent.2[0][0]              \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,140 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,140 kur.backend.keras_backend:538]\u001b[0m ..dense.0 (Dense)                (None, 30)            3870        ..dropout.2[0][0]                \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,140 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,140 kur.backend.keras_backend:538]\u001b[0m ..activation.0 (Activation)      (None, 30)            0           ..dense.0[0][0]                  \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,140 kur.backend.keras_backend:538]\u001b[0m ====================================================================================================\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,140 kur.backend.keras_backend:538]\u001b[0m Total params: 263,326\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,140 kur.backend.keras_backend:538]\u001b[0m Trainable params: 262,814\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,140 kur.backend.keras_backend:538]\u001b[0m Non-trainable params: 512\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,140 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,140 kur.backend.keras_backend:538]\u001b[0m \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,140 kur.backend.keras_backend:576]\u001b[0m Assembling a training function from the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:30:54,145 kur.backend.keras_backend:509]\u001b[0m Adding additional inputs: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:19,913 kur.backend.keras_backend:599]\u001b[0m Additional inputs for log functions: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:19,914 kur.backend.keras_backend:616]\u001b[0m Expected input shapes: in_seq=(None, 50, 30), out_char=(None, None)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:19,914 kur.backend.keras_backend:634]\u001b[0m Compiled model: {'shapes': {'input': [(None, 50, 30), (None, None)]}, 'names': {'output': ['..activation.0', 'out_char'], 'input': ['in_seq', 'out_char']}, 'func': <keras.backend.theano_backend.Function object at 0x11c421cc0>}\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:19,914 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:19,914 kur.providers.batch_provider:102]\u001b[0m Maximum number of batches set to: 1\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:31:19,934 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:19,934 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:19,934 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,040 kur.providers.provider:144]\u001b[0m Data source \"out_char\": entries=13299, shape=(30,)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,040 kur.providers.provider:144]\u001b[0m Data source \"in_seq\": entries=13299, shape=(50, 30)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,294 kur.model.hooks.plot_hook:73]\u001b[0m Plotting hook received training message.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,294 kur.model.hooks.plot_hook:80]\u001b[0m Plotting hook does not handle this status.\u001b[0m\n",
      "\n",
      "Epoch 1/1, loss=N/A:   0%|                       | 0/13299 [00:00<?, ?samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:20,300 kur.providers.shuffle_provider:184]\u001b[0m Shuffling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,610 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,611 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,611 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,611 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,617 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,798 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,798 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,799 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,799 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,799 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,800 kur.loggers.binary_logger:144]\u001b[0m Writing logger summary.\u001b[0m\n",
      "Epoch 1/1, loss=4.156:   0%|            | 32/13299 [00:00<03:27, 63.89samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:20,801 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,801 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,803 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,974 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.994:   0%|            | 64/13299 [00:00<02:46, 79.48samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:20,974 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,975 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:20,976 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:21,156 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.858:   1%|            | 96/13299 [00:00<02:18, 95.13samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:21,156 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:21,156 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:21,158 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:21,336 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.808:   1%|          | 128/13299 [00:01<01:59, 110.58samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:21,336 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:21,336 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:21,338 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:21,511 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.762:   1%|          | 160/13299 [00:01<01:44, 125.35samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:21,512 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:21,512 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:21,514 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:21,685 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.699:   1%|▏         | 192/13299 [00:01<01:34, 138.72samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:21,685 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:21,685 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:21,687 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:21,888 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.689:   2%|▏         | 224/13299 [00:01<01:30, 143.82samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:21,888 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:21,888 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:21,892 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:22,071 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.641:   2%|▏         | 256/13299 [00:01<01:25, 151.95samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:22,071 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:22,071 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:22,074 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:22,275 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.605:   2%|▏         | 288/13299 [00:01<01:24, 153.29samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:22,276 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:22,276 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:22,278 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:22,521 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.593:   2%|▏         | 320/13299 [00:02<01:29, 145.61samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:22,521 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:22,521 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:22,523 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:22,702 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.577:   3%|▎         | 352/13299 [00:02<01:24, 153.77samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:22,702 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:22,702 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:22,704 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:22,891 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.587:   3%|▎         | 384/13299 [00:02<01:21, 158.02samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:22,892 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:22,892 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:22,894 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:23,089 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.569:   3%|▎         | 416/13299 [00:02<01:20, 159.19samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:23,089 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:23,089 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:23,092 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:23,284 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.555:   3%|▎         | 448/13299 [00:02<01:20, 160.42samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:23,285 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:23,285 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:23,287 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:23,469 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.521:   4%|▎         | 480/13299 [00:03<01:18, 164.12samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:23,469 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:23,469 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:23,472 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:23,656 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.486:   4%|▍         | 512/13299 [00:03<01:16, 166.24samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:23,656 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:23,656 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:23,658 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:23,847 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.463:   4%|▍         | 544/13299 [00:03<01:16, 166.62samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:23,847 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:23,847 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:23,849 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:24,054 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.464:   4%|▍         | 576/13299 [00:03<01:18, 162.76samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:24,054 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:24,055 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:24,057 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:24,238 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.456:   5%|▍         | 608/13299 [00:03<01:16, 166.00samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:24,238 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:24,238 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:24,240 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:24,417 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.449:   5%|▍         | 640/13299 [00:04<01:14, 169.61samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:24,417 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:24,417 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:24,418 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:24,608 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.432:   5%|▌         | 672/13299 [00:04<01:14, 169.04samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:24,608 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:24,608 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:24,610 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:24,792 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.403:   5%|▌         | 704/13299 [00:04<01:13, 170.27samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:24,793 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:24,793 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:24,795 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:24,977 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.394:   6%|▌         | 736/13299 [00:04<01:13, 171.05samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:24,978 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:24,978 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:24,980 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:25,164 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.391:   6%|▌         | 768/13299 [00:04<01:13, 171.15samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:25,165 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:25,165 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:25,167 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:25,333 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.376:   6%|▌         | 800/13299 [00:05<01:10, 176.28samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:25,333 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:25,334 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:25,336 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:25,539 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.381:   6%|▋         | 832/13299 [00:05<01:13, 169.44samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:25,539 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:25,540 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:25,542 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:25,735 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.373:   6%|▋         | 864/13299 [00:05<01:14, 167.57samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:25,735 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:25,735 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:25,737 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:25,914 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.361:   7%|▋         | 896/13299 [00:05<01:12, 170.64samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:25,915 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:25,915 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:25,917 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:26,094 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.354:   7%|▋         | 928/13299 [00:05<01:11, 172.93samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:26,094 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:26,094 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:26,097 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:26,297 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.347:   7%|▋         | 960/13299 [00:05<01:13, 167.98samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:26,297 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:26,297 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:26,300 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:26,483 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.342:   7%|▋         | 992/13299 [00:06<01:12, 169.12samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:26,483 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:26,484 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:26,485 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:26,669 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.331:   8%|▋        | 1024/13299 [00:06<01:12, 169.99samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:26,670 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:26,670 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:26,672 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:26,858 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.316:   8%|▋        | 1056/13299 [00:06<01:12, 169.78samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:26,858 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:26,859 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:26,861 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:27,064 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.302:   8%|▋        | 1088/13299 [00:06<01:13, 165.30samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:27,064 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:27,064 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:27,066 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:27,269 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.292:   8%|▊        | 1120/13299 [00:06<01:15, 162.21samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:27,270 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:27,270 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:27,272 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:27,452 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.297:   9%|▊        | 1152/13299 [00:07<01:13, 165.90samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:27,452 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:27,453 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:27,454 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:27,638 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.295:   9%|▊        | 1184/13299 [00:07<01:12, 167.63samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:27,639 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:27,639 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:27,641 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:27,818 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.280:   9%|▊        | 1216/13299 [00:07<01:10, 170.55samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:27,819 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:27,819 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:27,822 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:28,003 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.279:   9%|▊        | 1248/13299 [00:07<01:10, 171.37samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:28,003 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:28,004 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:28,006 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:28,205 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.276:  10%|▊        | 1280/13299 [00:07<01:11, 167.14samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:28,206 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:28,206 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:28,208 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:28,402 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.265:  10%|▉        | 1312/13299 [00:08<01:12, 165.76samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:28,403 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:28,403 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:28,405 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:28,578 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.255:  10%|▉        | 1344/13299 [00:08<01:10, 170.38samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:28,578 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:28,578 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:28,581 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:28,770 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.253:  10%|▉        | 1376/13299 [00:08<01:10, 169.27samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:28,770 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:28,770 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:28,773 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:28,996 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.247:  11%|▉        | 1408/13299 [00:08<01:14, 159.90samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:28,996 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:28,996 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:28,999 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:29,194 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.237:  11%|▉        | 1440/13299 [00:08<01:13, 160.26samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:29,195 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:29,195 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:29,199 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:29,390 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.233:  11%|▉        | 1472/13299 [00:09<01:13, 161.32samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:29,390 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:29,390 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:29,392 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:29,603 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.232:  11%|█        | 1504/13299 [00:09<01:14, 157.69samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:29,604 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:29,604 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:29,606 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:29,808 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.225:  12%|█        | 1536/13299 [00:09<01:14, 157.22samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:29,809 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:29,809 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:29,812 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:30,003 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.224:  12%|█        | 1568/13299 [00:09<01:13, 159.26samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:30,003 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:30,004 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:30,006 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:30,194 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.214:  12%|█        | 1600/13299 [00:09<01:12, 161.55samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:30,195 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:30,195 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:30,197 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:30,378 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.206:  12%|█        | 1632/13299 [00:10<01:10, 165.24samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:30,378 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:30,378 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:30,381 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:30,571 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.202:  13%|█▏       | 1664/13299 [00:10<01:10, 165.47samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:30,571 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:30,571 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:30,573 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:30,746 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.200:  13%|█▏       | 1696/13299 [00:10<01:08, 170.13samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:30,747 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:30,747 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:30,749 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:30,955 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.195:  13%|█▏       | 1728/13299 [00:10<01:10, 164.72samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:30,955 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:30,956 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:30,958 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:31,142 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.193:  13%|█▏       | 1760/13299 [00:10<01:09, 166.57samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:31,142 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:31,143 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:31,145 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:31,350 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.190:  13%|█▏       | 1792/13299 [00:11<01:10, 162.56samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:31,350 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:31,351 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:31,352 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:31,533 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.186:  14%|█▏       | 1824/13299 [00:11<01:09, 166.10samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:31,533 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:31,533 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:31,535 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:31,720 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.180:  14%|█▎       | 1856/13299 [00:11<01:08, 167.56samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:31,720 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:31,721 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:31,723 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:31,900 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.182:  14%|█▎       | 1888/13299 [00:11<01:07, 170.19samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:31,902 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:31,902 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:31,905 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:32,121 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.180:  14%|█▎       | 1920/13299 [00:11<01:10, 161.98samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:32,121 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:32,121 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:32,123 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:32,308 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.175:  15%|█▎       | 1952/13299 [00:12<01:08, 164.56samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:32,308 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:32,308 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:32,310 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:32,501 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.175:  15%|█▎       | 1984/13299 [00:12<01:08, 164.96samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:32,501 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:32,502 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:32,503 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:32,689 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.168:  15%|█▎       | 2016/13299 [00:12<01:07, 166.55samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:32,689 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:32,689 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:32,692 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:32,888 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.170:  15%|█▍       | 2048/13299 [00:12<01:08, 164.74samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:32,888 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:32,889 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:32,891 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:33,098 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.171:  16%|█▍       | 2080/13299 [00:12<01:09, 160.90samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:33,098 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:33,098 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:33,100 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:33,284 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.169:  16%|█▍       | 2112/13299 [00:12<01:08, 164.09samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:33,284 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:33,284 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:33,287 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:33,489 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.165:  16%|█▍       | 2144/13299 [00:13<01:09, 161.63samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:33,489 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:33,489 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:33,491 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:33,675 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.162:  16%|█▍       | 2176/13299 [00:13<01:07, 164.42samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:33,676 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:33,676 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:33,679 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:33,889 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.153:  17%|█▍       | 2208/13299 [00:13<01:09, 159.78samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:33,889 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:33,889 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:33,892 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:34,105 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.148:  17%|█▌       | 2240/13299 [00:13<01:10, 156.02samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:34,106 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:34,106 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:34,108 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:34,317 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.143:  17%|█▌       | 2272/13299 [00:14<01:11, 154.52samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:34,317 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:34,318 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:34,321 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:34,531 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.139:  17%|█▌       | 2304/13299 [00:14<01:11, 152.93samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:34,532 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:34,532 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:34,534 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:34,749 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.136:  18%|█▌       | 2336/13299 [00:14<01:12, 150.92samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:34,750 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:34,750 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:34,755 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:34,960 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.133:  18%|█▌       | 2368/13299 [00:14<01:12, 151.36samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:34,960 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:34,960 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:34,961 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:35,162 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.132:  18%|█▌       | 2400/13299 [00:14<01:11, 153.42samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:35,162 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:35,162 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:35,164 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:35,347 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.129:  18%|█▋       | 2432/13299 [00:15<01:08, 158.73samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:35,347 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:35,348 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:35,349 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:35,561 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.123:  19%|█▋       | 2464/13299 [00:15<01:09, 155.75samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:35,562 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:35,562 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:35,564 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:35,749 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.119:  19%|█▋       | 2496/13299 [00:15<01:07, 159.86samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:35,750 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:35,750 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:35,752 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:35,930 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.113:  19%|█▋       | 2528/13299 [00:15<01:05, 164.51samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:35,931 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:35,931 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:35,934 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:36,156 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.108:  19%|█▋       | 2560/13299 [00:15<01:08, 157.02samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:36,156 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:36,157 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:36,158 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:36,344 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.105:  19%|█▊       | 2592/13299 [00:16<01:06, 160.72samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:36,344 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:36,345 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:36,347 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:36,537 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.100:  20%|█▊       | 2624/13299 [00:16<01:05, 162.29samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:36,537 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:36,537 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:36,539 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:36,741 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.100:  20%|█▊       | 2656/13299 [00:16<01:06, 160.64samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:36,741 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:36,741 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:36,743 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:36,931 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.098:  20%|█▊       | 2688/13299 [00:16<01:05, 162.71samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:36,932 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:36,933 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:36,936 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:37,116 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.095:  20%|█▊       | 2720/13299 [00:16<01:03, 165.79samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:37,116 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:37,117 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:37,119 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:37,333 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.088:  21%|█▊       | 2752/13299 [00:17<01:05, 159.92samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:37,333 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:37,333 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:37,335 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:37,521 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.086:  21%|█▉       | 2784/13299 [00:17<01:04, 162.71samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:37,522 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:37,522 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:37,525 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:37,724 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.083:  21%|█▉       | 2816/13299 [00:17<01:05, 161.24samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:37,724 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:37,724 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:37,726 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:37,921 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.084:  21%|█▉       | 2848/13299 [00:17<01:04, 161.43samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:37,922 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:37,922 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:37,925 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:38,107 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.083:  22%|█▉       | 2880/13299 [00:17<01:03, 164.55samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:38,108 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:38,108 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:38,110 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:38,315 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.083:  22%|█▉       | 2912/13299 [00:18<01:04, 161.23samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:38,315 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:38,316 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:38,318 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:38,513 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.077:  22%|█▉       | 2944/13299 [00:18<01:04, 161.33samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:38,514 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:38,514 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:38,516 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:38,697 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.073:  22%|██       | 2976/13299 [00:18<01:02, 165.03samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:38,697 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:38,697 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:38,700 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:38,901 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.070:  23%|██       | 3008/13299 [00:18<01:03, 162.30samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:38,902 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:38,902 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:38,905 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:39,111 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.067:  23%|██       | 3040/13299 [00:18<01:04, 159.30samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:39,111 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:39,112 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:39,115 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:39,316 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.063:  23%|██       | 3072/13299 [00:19<01:04, 158.28samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:39,317 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:39,317 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:39,317 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:39,525 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.062:  23%|██       | 3104/13299 [00:19<01:05, 156.69samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:39,526 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:39,526 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:39,528 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:39,724 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.063:  24%|██       | 3136/13299 [00:19<01:04, 157.99samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:39,724 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:39,724 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:39,727 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:39,919 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.062:  24%|██▏      | 3168/13299 [00:19<01:03, 159.70samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:39,920 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:39,922 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:39,923 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:40,130 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.056:  24%|██▏      | 3200/13299 [00:19<01:04, 157.30samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:40,130 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:40,130 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:40,132 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:40,347 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.053:  24%|██▏      | 3232/13299 [00:20<01:05, 154.15samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:40,347 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:40,348 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:40,351 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:40,543 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.049:  25%|██▏      | 3264/13299 [00:20<01:04, 156.79samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:40,544 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:40,544 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:40,546 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:40,761 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.046:  25%|██▏      | 3296/13299 [00:20<01:05, 153.59samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:40,762 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:40,762 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:40,764 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:40,968 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.044:  25%|██▎      | 3328/13299 [00:20<01:04, 154.03samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:40,968 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:40,968 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:40,970 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:41,154 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.045:  25%|██▎      | 3360/13299 [00:20<01:02, 158.81samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:41,155 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:41,158 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:41,159 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:41,368 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.044:  26%|██▎      | 3392/13299 [00:21<01:03, 156.07samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:41,368 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:41,369 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:41,371 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:41,566 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.040:  26%|██▎      | 3424/13299 [00:21<01:02, 157.60samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:41,567 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:41,567 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:41,569 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:41,774 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.039:  26%|██▎      | 3456/13299 [00:21<01:02, 156.44samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:41,775 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:41,775 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:41,777 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:41,985 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.039:  26%|██▎      | 3488/13299 [00:21<01:03, 155.09samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:41,985 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:41,985 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:41,988 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:42,196 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.037:  26%|██▍      | 3520/13299 [00:21<01:03, 154.08samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:42,196 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:42,196 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:42,199 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:42,398 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.033:  27%|██▍      | 3552/13299 [00:22<01:02, 155.16samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:42,399 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:42,399 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:42,402 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:42,609 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.032:  27%|██▍      | 3584/13299 [00:22<01:02, 154.27samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:42,609 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:42,609 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:42,612 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:42,813 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.031:  27%|██▍      | 3616/13299 [00:22<01:02, 155.06samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:42,813 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:42,813 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:42,816 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:43,022 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.030:  27%|██▍      | 3648/13299 [00:22<01:02, 154.33samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:43,023 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:43,023 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:43,026 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:43,230 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.024:  28%|██▍      | 3680/13299 [00:22<01:02, 154.17samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:43,231 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:43,231 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:43,233 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:43,429 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.021:  28%|██▌      | 3712/13299 [00:23<01:01, 156.23samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:43,429 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:43,429 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:43,432 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:43,646 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.020:  28%|██▌      | 3744/13299 [00:23<01:02, 153.46samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:43,646 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:43,646 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:43,650 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:43,851 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.018:  28%|██▌      | 3776/13299 [00:23<01:01, 154.25samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:43,851 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:43,852 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:43,854 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:44,053 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.017:  29%|██▌      | 3808/13299 [00:23<01:01, 155.37samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:44,054 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:44,054 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:44,056 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:44,263 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.018:  29%|██▌      | 3840/13299 [00:23<01:01, 154.48samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:44,264 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:44,264 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:44,266 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:44,460 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.016:  29%|██▌      | 3872/13299 [00:24<01:00, 156.83samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:44,460 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:44,461 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:44,463 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:44,666 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.015:  29%|██▋      | 3904/13299 [00:24<01:00, 156.25samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:44,667 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:44,667 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:44,670 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:44,886 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.014:  30%|██▋      | 3936/13299 [00:24<01:01, 152.99samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:44,886 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:44,886 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:44,889 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:45,082 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.015:  30%|██▋      | 3968/13299 [00:24<00:59, 155.93samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:45,082 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:45,083 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:45,085 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:45,294 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.015:  30%|██▋      | 4000/13299 [00:24<01:00, 154.34samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:45,295 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:45,295 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:45,298 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:45,486 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.014:  30%|██▋      | 4032/13299 [00:25<00:58, 157.89samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:45,486 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:45,487 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:45,489 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:45,760 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.014:  31%|██▊      | 4064/13299 [00:25<01:04, 142.83samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:45,760 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:45,763 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:45,764 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:45,963 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.014:  31%|██▊      | 4096/13299 [00:25<01:02, 146.91samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:45,964 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:45,964 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:45,966 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:46,173 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.012:  31%|██▊      | 4128/13299 [00:25<01:01, 148.46samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:46,174 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:46,174 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:46,177 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:46,380 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.011:  31%|██▊      | 4160/13299 [00:26<01:00, 150.35samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:46,380 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:46,381 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:46,383 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:46,586 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.011:  32%|██▊      | 4192/13299 [00:26<00:59, 151.83samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:46,586 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:46,587 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:46,589 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:46,788 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.009:  32%|██▊      | 4224/13299 [00:26<00:59, 153.69samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:46,789 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:46,789 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:46,791 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:46,975 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.006:  32%|██▉      | 4256/13299 [00:26<00:57, 158.58samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:46,975 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:46,976 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:46,978 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:47,179 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.003:  32%|██▉      | 4288/13299 [00:26<00:57, 158.06samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:47,179 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:47,180 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:47,182 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:47,386 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.000:  32%|██▉      | 4320/13299 [00:27<00:57, 156.93samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:47,387 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:47,387 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:47,390 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:47,597 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.998:  33%|██▉      | 4352/13299 [00:27<00:57, 155.46samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:47,597 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:47,597 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:47,600 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:47,804 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.998:  33%|██▉      | 4384/13299 [00:27<00:57, 155.21samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:47,804 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:47,804 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:47,807 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:48,013 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.995:  33%|██▉      | 4416/13299 [00:27<00:57, 154.37samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:48,014 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:48,014 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:48,017 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:48,214 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.992:  33%|███      | 4448/13299 [00:27<00:56, 155.92samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:48,214 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:48,214 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:48,217 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:48,416 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.992:  34%|███      | 4480/13299 [00:28<00:56, 156.61samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:48,417 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:48,417 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:48,421 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:48,617 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.994:  34%|███      | 4512/13299 [00:28<00:55, 157.43samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:48,617 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:48,618 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:48,620 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:48,818 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.992:  34%|███      | 4544/13299 [00:28<00:55, 157.96samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:48,818 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:48,819 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:48,821 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:49,016 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.991:  34%|███      | 4576/13299 [00:28<00:54, 158.90samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:49,018 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:49,018 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:49,022 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:49,210 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.990:  35%|███      | 4608/13299 [00:28<00:54, 160.89samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:49,210 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:49,210 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:49,213 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:49,420 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.990:  35%|███▏     | 4640/13299 [00:29<00:54, 158.15samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:49,420 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:49,421 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:49,423 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:49,628 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.987:  35%|███▏     | 4672/13299 [00:29<00:55, 156.85samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:49,628 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:49,629 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:49,632 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:49,842 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.982:  35%|███▏     | 4704/13299 [00:29<00:55, 154.47samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:49,843 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:49,843 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:49,845 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:50,049 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.979:  36%|███▏     | 4736/13299 [00:29<00:55, 154.66samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:50,049 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:50,049 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:50,052 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:50,258 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.977:  36%|███▏     | 4768/13299 [00:29<00:55, 154.20samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:50,258 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:50,258 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:50,261 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:50,466 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.975:  36%|███▏     | 4800/13299 [00:30<00:55, 153.96samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:50,467 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:50,467 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:50,469 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:50,675 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.972:  36%|███▎     | 4832/13299 [00:30<00:55, 153.69samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:50,676 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:50,676 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:50,678 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:50,890 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.970:  37%|███▎     | 4864/13299 [00:30<00:55, 152.23samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:50,891 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:50,891 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:50,893 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:51,098 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.968:  37%|███▎     | 4896/13299 [00:30<00:55, 152.64samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:51,099 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:51,099 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:51,102 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:51,314 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.966:  37%|███▎     | 4928/13299 [00:31<00:55, 151.36samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:51,315 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:51,315 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:51,317 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:51,529 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.964:  37%|███▎     | 4960/13299 [00:31<00:55, 150.67samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:51,529 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:51,529 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:51,532 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:51,728 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.962:  38%|███▍     | 4992/13299 [00:31<00:54, 153.47samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:51,729 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:51,729 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:51,732 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:51,941 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.960:  38%|███▍     | 5024/13299 [00:31<00:54, 152.58samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:51,941 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:51,941 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:51,944 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:52,143 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.958:  38%|███▍     | 5056/13299 [00:31<00:53, 154.31samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:52,143 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:52,143 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:52,146 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:52,344 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.956:  38%|███▍     | 5088/13299 [00:32<00:52, 155.54samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:52,345 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:52,348 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:52,349 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:52,553 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.955:  38%|███▍     | 5120/13299 [00:32<00:52, 154.80samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:52,555 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:52,555 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:52,558 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:52,747 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.954:  39%|███▍     | 5152/13299 [00:32<00:51, 157.88samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:52,747 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:52,747 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:52,750 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:52,944 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.955:  39%|███▌     | 5184/13299 [00:32<00:50, 159.20samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:52,944 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:52,945 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:52,947 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:53,144 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.954:  39%|███▌     | 5216/13299 [00:32<00:50, 159.40samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:53,145 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:53,145 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:53,148 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:53,366 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.951:  39%|███▌     | 5248/13299 [00:33<00:52, 154.57samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:53,366 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:53,366 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:53,369 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:53,573 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.948:  40%|███▌     | 5280/13299 [00:33<00:51, 154.62samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:53,573 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:53,573 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:53,576 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:53,780 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.946:  40%|███▌     | 5312/13299 [00:33<00:51, 154.43samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:53,781 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:53,781 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:53,784 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:53,987 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.943:  40%|███▌     | 5344/13299 [00:33<00:51, 154.50samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:53,988 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:53,988 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:53,990 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:54,202 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.941:  40%|███▋     | 5376/13299 [00:33<00:51, 152.83samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:54,202 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:54,203 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:54,205 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:54,403 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.940:  41%|███▋     | 5408/13299 [00:34<00:51, 154.68samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:54,403 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:54,404 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:54,407 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:54,609 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.937:  41%|███▋     | 5440/13299 [00:34<00:50, 154.79samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:54,610 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:54,610 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:54,613 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:54,820 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.936:  41%|███▋     | 5472/13299 [00:34<00:50, 153.85samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:54,821 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:54,821 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:54,823 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:55,025 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.934:  41%|███▋     | 5504/13299 [00:34<00:50, 154.65samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:55,025 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:55,026 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:55,026 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:55,236 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.934:  42%|███▋     | 5536/13299 [00:34<00:50, 153.61samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:55,237 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:55,237 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:55,239 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:55,439 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.934:  42%|███▊     | 5568/13299 [00:35<00:49, 154.80samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:55,440 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:55,440 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:55,442 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:55,655 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.932:  42%|███▊     | 5600/13299 [00:35<00:50, 152.69samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:55,656 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:55,656 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:55,660 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:55,864 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.931:  42%|███▊     | 5632/13299 [00:35<00:50, 153.00samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:55,864 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:55,864 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:55,867 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:56,082 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.929:  43%|███▊     | 5664/13299 [00:35<00:50, 151.03samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:56,082 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:56,082 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:56,085 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:56,300 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.929:  43%|███▊     | 5696/13299 [00:36<00:50, 149.69samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:56,301 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:56,301 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:56,303 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:56,507 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.926:  43%|███▉     | 5728/13299 [00:36<00:50, 151.11samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:56,508 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:56,508 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:56,511 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:56,714 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.924:  43%|███▉     | 5760/13299 [00:36<00:49, 152.09samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:56,715 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:56,715 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:56,717 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:56,919 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.923:  44%|███▉     | 5792/13299 [00:36<00:48, 153.43samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:56,919 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:56,919 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:56,922 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:57,114 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.922:  44%|███▉     | 5824/13299 [00:36<00:47, 156.39samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:57,115 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:57,115 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:57,118 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:57,316 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.921:  44%|███▉     | 5856/13299 [00:37<00:47, 156.97samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:57,317 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:57,317 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:57,319 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:57,526 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.921:  44%|███▉     | 5888/13299 [00:37<00:47, 155.53samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:57,527 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:57,527 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:57,530 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:57,735 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.919:  45%|████     | 5920/13299 [00:37<00:47, 154.97samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:57,735 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:57,735 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:57,738 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:57,939 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.919:  45%|████     | 5952/13299 [00:37<00:47, 155.42samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:57,940 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:57,940 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:57,942 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:58,130 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.918:  45%|████     | 5984/13299 [00:37<00:46, 158.76samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:58,131 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:58,131 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:58,134 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:58,352 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.916:  45%|████     | 6016/13299 [00:38<00:47, 154.13samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:58,353 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:58,353 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:58,355 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:58,557 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.913:  45%|████     | 6048/13299 [00:38<00:46, 154.73samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:58,558 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:58,558 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:58,561 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:58,773 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.914:  46%|████     | 6080/13299 [00:38<00:47, 152.67samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:58,774 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:58,774 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:58,777 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:58,970 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.912:  46%|████▏    | 6112/13299 [00:38<00:46, 155.61samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:58,970 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:58,970 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:58,973 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:59,172 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.908:  46%|████▏    | 6144/13299 [00:38<00:45, 156.38samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:59,173 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:59,173 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:59,175 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:59,375 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.906:  46%|████▏    | 6176/13299 [00:39<00:45, 156.81samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:59,375 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:59,375 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:59,378 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:59,591 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.905:  47%|████▏    | 6208/13299 [00:39<00:46, 153.99samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:59,592 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:59,592 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:59,594 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:59,800 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.903:  47%|████▏    | 6240/13299 [00:39<00:45, 153.87samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:31:59,800 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:59,800 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:31:59,803 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:00,012 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.904:  47%|████▏    | 6272/13299 [00:39<00:45, 152.80samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:00,013 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:00,013 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:00,015 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:00,219 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.903:  47%|████▎    | 6304/13299 [00:39<00:45, 153.44samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:00,219 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:00,219 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:00,222 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:00,420 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.902:  48%|████▎    | 6336/13299 [00:40<00:44, 155.02samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:00,421 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:00,421 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:00,424 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:00,623 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.900:  48%|████▎    | 6368/13299 [00:40<00:44, 155.92samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:00,623 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:00,623 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:00,626 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:00,838 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.900:  48%|████▎    | 6400/13299 [00:40<00:44, 153.60samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:00,839 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:00,839 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:00,842 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:01,040 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.898:  48%|████▎    | 6432/13299 [00:40<00:44, 155.02samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:01,041 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:01,041 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:01,044 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:01,234 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.898:  49%|████▎    | 6464/13299 [00:40<00:43, 158.08samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:01,234 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:01,234 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:01,236 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:01,446 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.895:  49%|████▍    | 6496/13299 [00:41<00:43, 155.86samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:01,446 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:01,446 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:01,449 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:01,647 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.894:  49%|████▍    | 6528/13299 [00:41<00:43, 156.83samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:01,647 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:01,647 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:01,649 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:01,850 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.893:  49%|████▍    | 6560/13299 [00:41<00:42, 156.94samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:01,851 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:01,851 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:01,854 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:02,062 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.892:  50%|████▍    | 6592/13299 [00:41<00:43, 155.13samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:02,063 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:02,063 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:02,065 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:02,257 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.891:  50%|████▍    | 6624/13299 [00:41<00:42, 157.80samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:02,257 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:02,257 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:02,260 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:02,448 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.889:  50%|████▌    | 6656/13299 [00:42<00:41, 160.51samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:02,449 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:02,449 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:02,451 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:02,652 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.887:  50%|████▌    | 6688/13299 [00:42<00:41, 159.37samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:02,653 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:02,653 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:02,655 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:02,853 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.886:  51%|████▌    | 6720/13299 [00:42<00:41, 159.40samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:02,853 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:02,854 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:02,856 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:03,059 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.885:  51%|████▌    | 6752/13299 [00:42<00:41, 158.10samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:03,060 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:03,060 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:03,060 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:03,263 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.883:  51%|████▌    | 6784/13299 [00:42<00:41, 157.86samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:03,263 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:03,263 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:03,266 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:03,453 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.881:  51%|████▌    | 6816/13299 [00:43<00:40, 160.71samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:03,454 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:03,454 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:03,456 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:03,668 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.881:  51%|████▋    | 6848/13299 [00:43<00:41, 156.99samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:03,669 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:03,669 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:03,671 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:03,875 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.878:  52%|████▋    | 6880/13299 [00:43<00:41, 156.31samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:03,875 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:03,876 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:03,878 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:04,077 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.877:  52%|████▋    | 6912/13299 [00:43<00:40, 156.94samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:04,077 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:04,078 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:04,080 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:04,279 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.878:  52%|████▋    | 6944/13299 [00:43<00:40, 157.36samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:04,279 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:04,280 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:04,282 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:04,481 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.876:  52%|████▋    | 6976/13299 [00:44<00:40, 157.72samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:04,481 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:04,482 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:04,483 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:04,706 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.874:  53%|████▋    | 7008/13299 [00:44<00:41, 152.58samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:04,707 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:04,707 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:04,709 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:04,899 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.874:  53%|████▊    | 7040/13299 [00:44<00:39, 156.53samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:04,899 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:04,899 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:04,902 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:05,086 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.874:  53%|████▊    | 7072/13299 [00:44<00:38, 160.61samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:05,086 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:05,086 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:05,089 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:05,283 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.873:  53%|████▊    | 7104/13299 [00:44<00:38, 161.02samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:05,284 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:05,284 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:05,286 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:05,483 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.872:  54%|████▊    | 7136/13299 [00:45<00:38, 160.67samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:05,484 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:05,484 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:05,486 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:05,676 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.870:  54%|████▊    | 7168/13299 [00:45<00:37, 162.35samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:05,676 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:05,676 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:05,683 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:05,899 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.869:  54%|████▊    | 7200/13299 [00:45<00:39, 156.18samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:05,899 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:05,899 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:05,902 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:06,089 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.867:  54%|████▉    | 7232/13299 [00:45<00:37, 159.70samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:06,089 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:06,089 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:06,091 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:06,288 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.867:  55%|████▉    | 7264/13299 [00:45<00:37, 160.02samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:06,288 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:06,288 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:06,291 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:06,497 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.865:  55%|████▉    | 7296/13299 [00:46<00:38, 157.88samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:06,497 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:06,497 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:06,500 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:06,695 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.863:  55%|████▉    | 7328/13299 [00:46<00:37, 158.93samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:06,695 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:06,696 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:06,698 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:06,894 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.861:  55%|████▉    | 7360/13299 [00:46<00:37, 159.44samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:06,895 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:06,895 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:06,897 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:07,117 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.860:  56%|█████    | 7392/13299 [00:46<00:38, 154.26samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:07,118 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:07,118 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:07,121 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:07,316 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.860:  56%|█████    | 7424/13299 [00:47<00:37, 156.21samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:07,317 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:07,317 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:07,319 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:07,513 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.859:  56%|█████    | 7456/13299 [00:47<00:36, 158.07samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:07,513 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:07,513 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:07,516 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:07,716 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.859:  56%|█████    | 7488/13299 [00:47<00:36, 157.88samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:07,717 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:07,717 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:07,719 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:07,908 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.857:  57%|█████    | 7520/13299 [00:47<00:36, 160.44samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:07,908 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:07,909 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:07,911 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:08,109 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.856:  57%|█████    | 7552/13299 [00:47<00:35, 160.08samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:08,110 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:08,110 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:08,113 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:08,309 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.854:  57%|█████▏   | 7584/13299 [00:48<00:35, 160.14samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:08,309 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:08,309 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:08,312 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:08,505 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.854:  57%|█████▏   | 7616/13299 [00:48<00:35, 161.03samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:08,505 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:08,505 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:08,508 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:08,685 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.853:  58%|█████▏   | 7648/13299 [00:48<00:34, 165.54samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:08,686 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:08,686 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:08,687 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:08,892 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.852:  58%|█████▏   | 7680/13299 [00:48<00:34, 162.28samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:08,892 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:08,892 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:08,894 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:09,077 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.851:  58%|█████▏   | 7712/13299 [00:48<00:33, 165.13samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:09,078 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:09,078 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:09,081 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:09,279 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.850:  58%|█████▏   | 7744/13299 [00:48<00:34, 163.31samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:09,279 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:09,279 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:09,282 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:09,495 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.849:  58%|█████▎   | 7776/13299 [00:49<00:34, 158.38samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:09,495 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:09,496 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:09,499 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:09,702 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.846:  59%|█████▎   | 7808/13299 [00:49<00:34, 157.22samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:09,702 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:09,703 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:09,705 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:09,899 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.845:  59%|█████▎   | 7840/13299 [00:49<00:34, 158.79samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:09,899 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:09,899 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:09,901 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:10,099 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.844:  59%|█████▎   | 7872/13299 [00:49<00:34, 159.04samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:10,100 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:10,100 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:10,100 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:10,295 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.843:  59%|█████▎   | 7904/13299 [00:49<00:33, 160.42samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:10,295 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:10,295 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:10,298 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:10,484 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.842:  60%|█████▎   | 7936/13299 [00:50<00:32, 162.82samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:10,485 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:10,485 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:10,488 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:10,686 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.842:  60%|█████▍   | 7968/13299 [00:50<00:33, 161.48samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:10,687 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:10,687 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:10,689 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:10,899 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.840:  60%|█████▍   | 8000/13299 [00:50<00:33, 158.04samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:10,899 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:10,899 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:10,902 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:11,099 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.838:  60%|█████▍   | 8032/13299 [00:50<00:33, 158.47samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:11,100 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:11,100 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:11,102 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:11,296 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.837:  61%|█████▍   | 8064/13299 [00:50<00:32, 159.77samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:11,296 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:11,297 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:11,299 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:11,506 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.835:  61%|█████▍   | 8096/13299 [00:51<00:33, 157.52samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:11,506 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:11,506 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:11,509 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:11,708 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.835:  61%|█████▌   | 8128/13299 [00:51<00:32, 157.78samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:11,708 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:11,708 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:11,711 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:11,902 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.835:  61%|█████▌   | 8160/13299 [00:51<00:32, 159.76samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:11,903 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:11,903 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:11,906 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:12,112 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.835:  62%|█████▌   | 8192/13299 [00:51<00:32, 157.44samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:12,113 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:12,113 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:12,115 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:12,309 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.834:  62%|█████▌   | 8224/13299 [00:52<00:31, 159.08samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:12,309 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:12,309 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:12,311 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:12,501 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.831:  62%|█████▌   | 8256/13299 [00:52<00:31, 161.23samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:12,501 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:12,501 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:12,504 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:12,703 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.829:  62%|█████▌   | 8288/13299 [00:52<00:31, 160.43samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:12,703 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:12,703 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:12,706 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:12,901 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.827:  63%|█████▋   | 8320/13299 [00:52<00:30, 160.70samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:12,901 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:12,902 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:12,904 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:13,107 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.825:  63%|█████▋   | 8352/13299 [00:52<00:31, 158.94samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:13,108 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:13,108 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:13,110 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:13,311 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.823:  63%|█████▋   | 8384/13299 [00:53<00:31, 158.33samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:13,312 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:13,312 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:13,314 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:13,522 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.822:  63%|█████▋   | 8416/13299 [00:53<00:31, 156.41samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:13,522 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:13,522 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:13,525 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:13,722 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.820:  64%|█████▋   | 8448/13299 [00:53<00:30, 157.33samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:13,723 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:13,723 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:13,725 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:13,923 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.819:  64%|█████▋   | 8480/13299 [00:53<00:30, 157.95samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:13,923 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:13,924 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:13,928 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:14,129 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.819:  64%|█████▊   | 8512/13299 [00:53<00:30, 157.23samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:14,129 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:14,129 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:14,132 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:14,332 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.818:  64%|█████▊   | 8544/13299 [00:54<00:30, 157.35samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:14,332 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:14,332 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:14,336 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:14,536 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.818:  64%|█████▊   | 8576/13299 [00:54<00:30, 157.08samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:14,537 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:14,537 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:14,538 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:14,736 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.815:  65%|█████▊   | 8608/13299 [00:54<00:29, 158.10samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:14,736 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:14,736 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:14,739 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:14,936 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.813:  65%|█████▊   | 8640/13299 [00:54<00:29, 158.57samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:14,936 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:14,937 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:14,939 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:15,136 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.811:  65%|█████▊   | 8672/13299 [00:54<00:29, 158.95samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:15,137 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:15,140 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:15,141 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:15,336 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.810:  65%|█████▉   | 8704/13299 [00:55<00:28, 159.32samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:15,336 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:15,337 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:15,339 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:15,557 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.810:  66%|█████▉   | 8736/13299 [00:55<00:29, 154.60samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:15,558 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:15,558 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:15,561 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:15,754 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.808:  66%|█████▉   | 8768/13299 [00:55<00:28, 156.93samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:15,755 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:15,755 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:15,757 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:15,947 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.806:  66%|█████▉   | 8800/13299 [00:55<00:28, 159.51samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:15,947 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:15,948 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:15,950 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:16,151 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.805:  66%|█████▉   | 8832/13299 [00:55<00:28, 158.69samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:16,152 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:16,152 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:16,154 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:16,361 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.804:  67%|█████▉   | 8864/13299 [00:56<00:28, 156.63samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:16,362 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:16,362 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:16,365 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:16,590 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.804:  67%|██████   | 8896/13299 [00:56<00:29, 151.22samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:16,591 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:16,591 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:16,593 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:16,791 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.802:  67%|██████   | 8928/13299 [00:56<00:28, 153.49samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:16,792 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:16,792 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:16,794 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:16,989 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.801:  67%|██████   | 8960/13299 [00:56<00:27, 155.91samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:16,990 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:16,990 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:16,993 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:17,182 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.799:  68%|██████   | 8992/13299 [00:56<00:27, 158.74samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:17,182 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:17,183 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:17,185 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:17,380 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.798:  68%|██████   | 9024/13299 [00:57<00:26, 159.53samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:17,381 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:17,381 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:17,383 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:17,568 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.798:  68%|██████▏  | 9056/13299 [00:57<00:26, 162.63samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:17,569 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:17,569 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:17,572 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:17,786 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.796:  68%|██████▏  | 9088/13299 [00:57<00:26, 157.52samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:17,787 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:17,787 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:17,789 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:17,978 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.794:  69%|██████▏  | 9120/13299 [00:57<00:26, 160.32samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:17,978 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:17,978 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:17,981 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:18,171 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.792:  69%|██████▏  | 9152/13299 [00:57<00:25, 161.91samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:18,171 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:18,171 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:18,174 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:18,368 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.791:  69%|██████▏  | 9184/13299 [00:58<00:25, 161.86samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:18,369 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:18,369 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:18,372 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:18,577 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.790:  69%|██████▏  | 9216/13299 [00:58<00:25, 159.20samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:18,578 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:18,578 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:18,581 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:18,805 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.788:  70%|██████▎  | 9248/13299 [00:58<00:26, 153.00samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:18,806 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:18,806 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:18,809 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:19,006 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.786:  70%|██████▎  | 9280/13299 [00:58<00:25, 154.98samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:19,006 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:19,007 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:19,008 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:19,215 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.787:  70%|██████▎  | 9312/13299 [00:58<00:25, 154.28samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:19,216 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:19,216 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:19,218 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:19,414 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.787:  70%|██████▎  | 9344/13299 [00:59<00:25, 156.33samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:19,414 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:19,414 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:19,417 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:19,602 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.786:  71%|██████▎  | 9376/13299 [00:59<00:24, 160.13samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:19,602 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:19,603 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:19,605 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:19,815 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.785:  71%|██████▎  | 9408/13299 [00:59<00:24, 157.02samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:19,816 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:19,816 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:19,818 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:20,045 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.783:  71%|██████▍  | 9440/13299 [00:59<00:25, 151.30samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:20,045 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:20,045 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:20,048 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:20,235 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.781:  71%|██████▍  | 9472/13299 [00:59<00:24, 155.91samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:20,236 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:20,236 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:20,238 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:20,424 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.781:  71%|██████▍  | 9504/13299 [01:00<00:23, 159.82samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:20,424 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:20,424 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:20,427 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:20,622 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.780:  72%|██████▍  | 9536/13299 [01:00<00:23, 160.25samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:20,623 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:20,623 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:20,625 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:20,819 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.779:  72%|██████▍  | 9568/13299 [01:00<00:23, 161.02samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:20,819 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:20,819 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:20,822 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:21,021 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.778:  72%|██████▍  | 9600/13299 [01:00<00:23, 160.10samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:21,022 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:21,022 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:21,027 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:21,232 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.778:  72%|██████▌  | 9632/13299 [01:00<00:23, 157.53samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:21,232 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:21,233 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:21,235 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:21,431 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.776:  73%|██████▌  | 9664/13299 [01:01<00:22, 158.49samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:21,432 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:21,432 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:21,435 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:21,629 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.774:  73%|██████▌  | 9696/13299 [01:01<00:22, 159.37samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:21,630 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:21,630 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:21,632 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:21,833 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.772:  73%|██████▌  | 9728/13299 [01:01<00:22, 158.54samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:21,834 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:21,834 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:21,837 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:22,035 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.771:  73%|██████▌  | 9760/13299 [01:01<00:22, 158.51samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:22,036 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:22,036 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:22,040 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:22,259 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.771:  74%|██████▋  | 9792/13299 [01:01<00:22, 153.53samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:22,260 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:22,260 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:22,262 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:22,495 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.771:  74%|██████▋  | 9824/13299 [01:02<00:23, 147.73samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:22,495 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:22,495 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:22,496 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:22,710 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.770:  74%|██████▋  | 9856/13299 [01:02<00:23, 147.99samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:22,711 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:22,711 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:22,713 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:22,902 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.770:  74%|██████▋  | 9888/13299 [01:02<00:22, 153.14samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:22,903 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:22,903 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:22,905 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:23,097 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.768:  75%|██████▋  | 9920/13299 [01:02<00:21, 156.32samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:23,098 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:23,098 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:23,100 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:23,336 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.768:  75%|██████▋  | 9952/13299 [01:03<00:22, 148.80samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:23,337 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:23,337 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:23,339 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:23,543 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.766:  75%|██████▊  | 9984/13299 [01:03<00:22, 150.58samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:23,543 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:23,543 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:23,546 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:23,744 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.766:  75%|██████  | 10016/13299 [01:03<00:21, 153.05samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:23,744 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:23,745 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:23,747 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:23,943 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.765:  76%|██████  | 10048/13299 [01:03<00:20, 155.18samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:23,944 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:23,944 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:23,946 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:24,143 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.763:  76%|██████  | 10080/13299 [01:03<00:20, 156.73samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:24,143 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:24,143 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:24,146 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:24,341 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.763:  76%|██████  | 10112/13299 [01:04<00:20, 158.20samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:24,341 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:24,341 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:24,344 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:24,536 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.762:  76%|██████  | 10144/13299 [01:04<00:19, 159.73samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:24,537 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:24,537 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:24,539 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:24,752 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.761:  77%|██████  | 10176/13299 [01:04<00:20, 156.08samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:24,753 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:24,753 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:24,756 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:24,952 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.760:  77%|██████▏ | 10208/13299 [01:04<00:19, 157.32samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:24,952 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:24,953 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:24,955 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:25,149 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.758:  77%|██████▏ | 10240/13299 [01:04<00:19, 158.90samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:25,149 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:25,149 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:25,151 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:25,331 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.757:  77%|██████▏ | 10272/13299 [01:05<00:18, 163.47samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:25,332 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:25,332 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:25,335 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:25,527 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.757:  77%|██████▏ | 10304/13299 [01:05<00:18, 163.38samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:25,528 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:25,528 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:25,530 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:25,721 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.756:  78%|██████▏ | 10336/13299 [01:05<00:18, 163.96samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:25,721 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:25,722 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:25,724 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:25,939 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.755:  78%|██████▏ | 10368/13299 [01:05<00:18, 158.29samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:25,940 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:25,940 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:25,942 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:26,127 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.755:  78%|██████▎ | 10400/13299 [01:05<00:17, 161.72samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:26,128 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:26,128 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:26,131 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:26,333 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.755:  78%|██████▎ | 10432/13299 [01:06<00:17, 159.82samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:26,334 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:26,334 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:26,336 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:26,530 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.754:  79%|██████▎ | 10464/13299 [01:06<00:17, 160.69samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:26,530 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:26,530 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:26,534 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:26,734 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.753:  79%|██████▎ | 10496/13299 [01:06<00:17, 159.34samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:26,735 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:26,735 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:26,738 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:26,937 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.753:  79%|██████▎ | 10528/13299 [01:06<00:17, 159.01samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:26,937 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:26,937 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:26,940 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:27,158 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.751:  79%|██████▎ | 10560/13299 [01:06<00:17, 154.37samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:27,159 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:27,159 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:27,161 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:27,353 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.749:  80%|██████▎ | 10592/13299 [01:07<00:17, 157.20samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:27,353 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:27,353 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:27,356 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:27,553 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.748:  80%|██████▍ | 10624/13299 [01:07<00:16, 158.05samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:27,553 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:27,554 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:27,556 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:27,791 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.747:  80%|██████▍ | 10656/13299 [01:07<00:17, 150.21samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:27,791 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:27,791 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:27,794 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:27,982 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.746:  80%|██████▍ | 10688/13299 [01:07<00:16, 154.98samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:27,982 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:27,982 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:27,984 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:28,190 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.746:  81%|██████▍ | 10720/13299 [01:07<00:16, 154.54samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:28,191 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:28,191 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:28,193 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:28,406 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.746:  81%|██████▍ | 10752/13299 [01:08<00:16, 152.52samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:28,407 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:28,407 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:28,409 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:28,602 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.745:  81%|██████▍ | 10784/13299 [01:08<00:16, 155.77samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:28,602 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:28,602 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:28,605 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:28,798 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.743:  81%|██████▌ | 10816/13299 [01:08<00:15, 157.88samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:28,798 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:28,799 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:28,801 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:28,997 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.742:  82%|██████▌ | 10848/13299 [01:08<00:15, 158.75samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:28,997 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:28,997 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:29,000 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:29,196 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.741:  82%|██████▌ | 10880/13299 [01:08<00:15, 159.25samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:29,197 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:29,197 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:29,199 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:29,390 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.739:  82%|██████▌ | 10912/13299 [01:09<00:14, 161.08samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:29,390 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:29,390 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:29,393 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:29,623 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.739:  82%|██████▌ | 10944/13299 [01:09<00:15, 153.03samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:29,624 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:29,624 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:29,627 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:29,838 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.738:  83%|██████▌ | 10976/13299 [01:09<00:15, 151.86samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:29,838 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:29,838 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:29,841 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:30,032 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.737:  83%|██████▌ | 11008/13299 [01:09<00:14, 155.57samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:30,032 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:30,032 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:30,034 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:30,234 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.736:  83%|██████▋ | 11040/13299 [01:09<00:14, 156.36samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:30,234 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:30,234 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:30,237 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:30,428 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.736:  83%|██████▋ | 11072/13299 [01:10<00:14, 158.75samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:30,429 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:30,429 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:30,432 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:30,629 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.735:  83%|██████▋ | 11104/13299 [01:10<00:13, 159.01samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:30,629 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:30,629 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:30,632 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:30,851 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.734:  84%|██████▋ | 11136/13299 [01:10<00:14, 154.10samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:30,852 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:30,852 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:30,854 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:31,056 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.734:  84%|██████▋ | 11168/13299 [01:10<00:13, 154.81samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:31,056 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:31,056 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:31,059 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:31,246 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.733:  84%|██████▋ | 11200/13299 [01:10<00:13, 158.66samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:31,246 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:31,246 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:31,249 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:31,457 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.732:  84%|██████▊ | 11232/13299 [01:11<00:13, 156.50samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:31,457 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:31,457 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:31,460 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:31,650 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.732:  85%|██████▊ | 11264/13299 [01:11<00:12, 159.14samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:31,650 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:31,651 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:31,655 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:31,848 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.731:  85%|██████▊ | 11296/13299 [01:11<00:12, 159.96samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:31,848 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:31,848 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:31,851 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:32,051 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.729:  85%|██████▊ | 11328/13299 [01:11<00:12, 159.24samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:32,051 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:32,051 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:32,054 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:32,251 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.729:  85%|██████▊ | 11360/13299 [01:11<00:12, 159.45samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:32,251 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:32,251 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:32,254 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:32,472 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.728:  86%|██████▊ | 11392/13299 [01:12<00:12, 154.59samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:32,473 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:32,473 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:32,475 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:32,677 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.728:  86%|██████▊ | 11424/13299 [01:12<00:12, 155.17samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:32,677 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:32,677 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:32,680 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:32,874 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.728:  86%|██████▉ | 11456/13299 [01:12<00:11, 157.25samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:32,874 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:32,875 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:32,877 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:33,075 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.727:  86%|██████▉ | 11488/13299 [01:12<00:11, 157.81samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:33,075 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:33,076 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:33,078 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:33,279 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.726:  87%|██████▉ | 11520/13299 [01:12<00:11, 157.46samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:33,280 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:33,280 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:33,282 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:33,485 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.724:  87%|██████▉ | 11552/13299 [01:13<00:11, 156.95samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:33,485 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:33,485 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:33,488 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:33,681 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.723:  87%|██████▉ | 11584/13299 [01:13<00:10, 158.65samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:33,682 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:33,682 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:33,685 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:33,899 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.722:  87%|██████▉ | 11616/13299 [01:13<00:10, 155.06samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:33,899 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:33,902 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:33,902 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:34,093 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.722:  88%|███████ | 11648/13299 [01:13<00:10, 157.84samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:34,093 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:34,094 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:34,096 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:34,287 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.720:  88%|███████ | 11680/13299 [01:13<00:10, 159.82samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:34,288 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:34,288 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:34,290 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:34,492 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.718:  88%|███████ | 11712/13299 [01:14<00:09, 158.82samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:34,492 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:34,492 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:34,495 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:34,690 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.717:  88%|███████ | 11744/13299 [01:14<00:09, 159.50samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:34,691 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:34,691 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:34,694 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:34,884 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.717:  89%|███████ | 11776/13299 [01:14<00:09, 161.25samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:34,884 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:34,884 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:34,887 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:35,078 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.715:  89%|███████ | 11808/13299 [01:14<00:09, 162.28samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:35,078 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:35,079 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:35,081 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:35,312 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.713:  89%|███████ | 11840/13299 [01:15<00:09, 153.57samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:35,313 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:35,313 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:35,315 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:35,507 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.712:  89%|███████▏| 11872/13299 [01:15<00:09, 156.62samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:35,508 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:35,508 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:35,510 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:35,729 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.711:  90%|███████▏| 11904/13299 [01:15<00:09, 152.77samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:35,729 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:35,729 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:35,732 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:35,946 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.711:  90%|███████▏| 11936/13299 [01:15<00:09, 151.09samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:35,946 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:35,947 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:35,950 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:36,152 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.710:  90%|███████▏| 11968/13299 [01:15<00:08, 152.35samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:36,152 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:36,153 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:36,156 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:36,345 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.709:  90%|███████▏| 12000/13299 [01:16<00:08, 156.02samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:36,346 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:36,346 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:36,349 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:36,546 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.708:  90%|███████▏| 12032/13299 [01:16<00:08, 157.17samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:36,546 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:36,546 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:36,549 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:36,770 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.707:  91%|███████▎| 12064/13299 [01:16<00:08, 152.38samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:36,771 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:36,771 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:36,773 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:36,968 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.706:  91%|███████▎| 12096/13299 [01:16<00:07, 155.19samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:36,968 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:36,969 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:36,971 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:37,161 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.706:  91%|███████▎| 12128/13299 [01:16<00:07, 158.18samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:37,161 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:37,162 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:37,164 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:37,360 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.705:  91%|███████▎| 12160/13299 [01:17<00:07, 158.89samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:37,361 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:37,361 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:37,364 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:37,546 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.704:  92%|███████▎| 12192/13299 [01:17<00:06, 162.63samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:37,547 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:37,547 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:37,550 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:37,737 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.702:  92%|███████▎| 12224/13299 [01:17<00:06, 164.18samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:37,737 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:37,738 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:37,740 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:37,933 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.701:  92%|███████▎| 12256/13299 [01:17<00:06, 163.91samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:37,933 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:37,933 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:37,936 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:38,154 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.700:  92%|███████▍| 12288/13299 [01:17<00:06, 157.68samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:38,154 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:38,157 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:38,157 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:38,356 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.699:  93%|███████▍| 12320/13299 [01:18<00:06, 157.94samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:38,356 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:38,356 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:38,358 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:38,546 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.698:  93%|███████▍| 12352/13299 [01:18<00:05, 160.76samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:38,547 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:38,547 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:38,550 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:38,746 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.697:  93%|███████▍| 12384/13299 [01:18<00:05, 160.70samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:38,746 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:38,746 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:38,749 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:38,954 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.696:  93%|███████▍| 12416/13299 [01:18<00:05, 158.59samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:38,954 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:38,954 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:38,957 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:39,163 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.696:  94%|███████▍| 12448/13299 [01:18<00:05, 156.72samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:39,164 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:39,164 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:39,167 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:39,363 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.696:  94%|███████▌| 12480/13299 [01:19<00:05, 157.78samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:39,364 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:39,364 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:39,366 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:39,573 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.696:  94%|███████▌| 12512/13299 [01:19<00:05, 156.25samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:39,573 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:39,573 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:39,575 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:39,814 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.695:  94%|███████▌| 12544/13299 [01:19<00:05, 148.18samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:39,815 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:39,815 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:39,817 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:40,010 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.694:  95%|███████▌| 12576/13299 [01:19<00:04, 152.59samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:40,010 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:40,010 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:40,013 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:40,211 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.693:  95%|███████▌| 12608/13299 [01:19<00:04, 154.36samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:40,212 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:40,212 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:40,214 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:40,418 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.692:  95%|███████▌| 12640/13299 [01:20<00:04, 154.55samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:40,418 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:40,421 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:40,422 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:40,618 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.692:  95%|███████▌| 12672/13299 [01:20<00:04, 156.21samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:40,618 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:40,618 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:40,621 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:40,827 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.691:  96%|███████▋| 12704/13299 [01:20<00:03, 155.11samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:40,828 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:40,828 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:40,837 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:41,034 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.690:  96%|███████▋| 12736/13299 [01:20<00:03, 155.01samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:41,034 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:41,035 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:41,037 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:41,236 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.690:  96%|███████▋| 12768/13299 [01:20<00:03, 155.88samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:41,237 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:41,237 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:41,241 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:41,457 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.688:  96%|███████▋| 12800/13299 [01:21<00:03, 152.41samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:41,458 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:41,458 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:41,460 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:41,661 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.687:  96%|███████▋| 12832/13299 [01:21<00:03, 153.91samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:41,661 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:41,661 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:41,664 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:41,861 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.685:  97%|███████▋| 12864/13299 [01:21<00:02, 155.54samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:41,862 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:41,862 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:41,864 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:42,063 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.684:  97%|███████▊| 12896/13299 [01:21<00:02, 156.52samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:42,063 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:42,063 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:42,066 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:42,273 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.683:  97%|███████▊| 12928/13299 [01:21<00:02, 155.13samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:42,274 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:42,274 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:42,276 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:42,471 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.681:  97%|███████▊| 12960/13299 [01:22<00:02, 156.98samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:42,472 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:42,472 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:42,475 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:42,654 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.680:  98%|███████▊| 12992/13299 [01:22<00:01, 162.09samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:42,654 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:42,654 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:42,657 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:42,869 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.679:  98%|███████▊| 13024/13299 [01:22<00:01, 157.83samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:42,869 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:42,870 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:42,872 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:43,069 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.677:  98%|███████▊| 13056/13299 [01:22<00:01, 158.47samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:43,069 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:43,070 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:43,072 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:43,256 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.677:  98%|███████▊| 13088/13299 [01:22<00:01, 162.17samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:43,256 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:43,256 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:43,259 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:43,446 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.677:  99%|███████▉| 13120/13299 [01:23<00:01, 163.93samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:43,446 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:43,447 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:43,449 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:43,724 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.676:  99%|███████▉| 13152/13299 [01:23<00:01, 145.47samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:43,724 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:43,725 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:43,726 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,013 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.675:  99%|███████▉| 13184/13299 [01:23<00:00, 132.86samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:44,014 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,014 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,017 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,220 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.674:  99%|███████▉| 13216/13299 [01:23<00:00, 138.65samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:44,221 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,221 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,224 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,423 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.673: 100%|███████▉| 13248/13299 [01:24<00:00, 143.92samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:44,424 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,424 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,426 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,639 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.672: 100%|███████▉| 13280/13299 [01:24<00:00, 145.23samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:44,639 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,774 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.672: 100%|████████| 13299/13299 [01:24<00:00, 143.65samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-05 22:32:44,775 kur.model.executor:464]\u001b[0m Training loss: 2.672\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:32:44,776 kur.model.executor:471]\u001b[0m Saving best historical training weights: t2_dp/best.w.kur\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,776 kur.model.executor:430]\u001b[0m Saving weights to: t2_dp/best.w.kur\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,776 kur.model.model:213]\u001b[0m Saving model weights to: t2_dp/best.w.kur\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,793 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: training_loss_out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,793 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: training_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,794 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: training_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,794 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: training_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,795 kur.loggers.binary_logger:144]\u001b[0m Writing logger summary.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,797 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,798 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,798 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,799 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,799 kur.loggers.binary_logger:144]\u001b[0m Writing logger summary.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,801 kur.model.executor:101]\u001b[0m Recompiling the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,801 kur.backend.keras_backend:543]\u001b[0m Reusing an existing model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,801 kur.backend.keras_backend:560]\u001b[0m Assembling a testing function from the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:44,805 kur.backend.keras_backend:509]\u001b[0m Adding additional inputs: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,347 kur.backend.keras_backend:599]\u001b[0m Additional inputs for log functions: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,347 kur.backend.keras_backend:616]\u001b[0m Expected input shapes: in_seq=(None, 50, 30), out_char=(None, None)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,348 kur.backend.keras_backend:634]\u001b[0m Compiled model: {'shapes': {'input': [(None, 50, 30), (None, None)]}, 'names': {'output': ['..activation.0', 'out_char'], 'input': ['in_seq', 'out_char']}, 'func': <keras.backend.theano_backend.Function object at 0x122e678d0>}\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,348 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,348 kur.providers.batch_provider:102]\u001b[0m Maximum number of batches set to: 1\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:32:47,376 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,377 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,377 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=N/A:   0%|                        | 0/831 [00:00<?, ?samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:47,407 kur.providers.shuffle_provider:184]\u001b[0m Shuffling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,440 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,440 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,440 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,442 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,488 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,490 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=2.122:   8%|▉           | 64/831 [00:00<00:01, 482.76samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:47,540 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,540 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,594 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,596 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=2.109:  15%|█▋         | 128/831 [00:00<00:01, 514.10samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:47,645 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,647 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,697 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,699 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=2.115:  23%|██▌        | 192/831 [00:00<00:01, 543.15samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:47,747 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,748 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,803 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,805 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=2.182:  31%|███▍       | 256/831 [00:00<00:01, 551.15samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:47,859 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,861 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,907 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,910 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,956 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:47,958 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=2.155:  42%|████▋      | 352/831 [00:00<00:00, 579.74samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:48,005 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,007 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,059 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,061 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,104 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,106 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=2.157:  54%|█████▉     | 448/831 [00:00<00:00, 597.84samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:48,154 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,156 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,198 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,200 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,242 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,244 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=2.159:  65%|███████▏   | 544/831 [00:00<00:00, 629.95samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:48,289 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,289 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,334 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,336 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,380 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,382 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=2.157:  77%|████████▍  | 640/831 [00:01<00:00, 653.88samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:48,421 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,423 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,464 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,466 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,510 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,512 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=2.179:  89%|█████████▋ | 736/831 [00:01<00:00, 667.08samples/s]\u001b[1;34m[DEBUG 2017-03-05 22:32:48,558 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,560 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,601 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,603 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=2.166: 100%|███████████| 831/831 [00:01<00:00, 681.67samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-05 22:32:48,690 kur.model.executor:197]\u001b[0m Validation loss: 2.166\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-05 22:32:48,691 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t2_dp/best.w.kur\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,691 kur.model.executor:442]\u001b[0m Recent weight file seems the same as the soon-to-be-saved file. Skipping: t2_dp/best.w.kur\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,691 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: validation_loss_out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,693 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: validation_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,693 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: validation_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,694 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: validation_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,694 kur.loggers.binary_logger:144]\u001b[0m Writing logger summary.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,696 kur.model.hooks.plot_hook:73]\u001b[0m Plotting hook received training message.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,696 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: batch_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,696 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: batch_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,697 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: batch_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,697 kur.model.hooks.plot_hook:107]\u001b[0m Using per-batch training statistics for plotting.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,697 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,697 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:48,698 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:49,091 kur.model.hooks.plot_hook:130]\u001b[0m Loss-per-batch plot saved to: t2_dp/loss.png\u001b[0m\n",
      "Completed 1 epochs.\n",
      "\u001b[1;37m[INFO 2017-03-05 22:32:49,092 kur.model.executor:235]\u001b[0m Saving most recent weights: t2_dp/last.w.kur\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:49,092 kur.model.model:213]\u001b[0m Saving model weights to: t2_dp/last.w.kur\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:49,110 kur.model.hooks.plot_hook:73]\u001b[0m Plotting hook received training message.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:49,110 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: batch_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:49,110 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: batch_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:49,111 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: batch_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:49,111 kur.model.hooks.plot_hook:107]\u001b[0m Using per-batch training statistics for plotting.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:49,111 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:49,112 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:49,112 kur.loggers.binary_logger:184]\u001b[0m Loading binary column: validation_loss_time\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-05 22:32:49,485 kur.model.hooks.plot_hook:130]\u001b[0m Loss-per-batch plot saved to: t2_dp/loss.png\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!kur -vv train char_rrn_demo_dp_fluid.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
